{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73ccb70d",
   "metadata": {},
   "source": [
    "# SMART SHOES MLP TRAINING MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013aa57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d59b714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv('ssnew.csv')\n",
    "\n",
    "# Shift the 't' column by 32.534 seconds\n",
    "df['t'] = df['t'] + 32.534\n",
    "\n",
    "# Create empty lists to store the matched values\n",
    "timestamps = []\n",
    "w_matched = []\n",
    "s_matched = []\n",
    "ax_matched = []\n",
    "ay_matched = []\n",
    "az_matched = []\n",
    "gx_matched = []\n",
    "gy_matched = []\n",
    "gz_matched = []\n",
    "\n",
    "# Iterate over the unique timestamps\n",
    "for timestamp in df['t'].unique():\n",
    "    # Find the matching values for weight\n",
    "    w_match = df.loc[df['tw'] == timestamp, 'w']\n",
    "    if not w_match.empty:\n",
    "        w_matched.append(w_match.iloc[0])\n",
    "    \n",
    "    # Find the matching values for sensor\n",
    "    s_match = df.loc[df['ts'] == timestamp, 's']\n",
    "    if not s_match.empty:\n",
    "        s_matched.append(s_match.iloc[0])\n",
    "    \n",
    "    # Find the matching values for IMU\n",
    "    imu_match = df.loc[df['ti'] == timestamp, ['ax', 'ay', 'az', 'gx', 'gy', 'gz']]\n",
    "    if not imu_match.empty:\n",
    "        ax_matched.append(imu_match['ax'].iloc[0])\n",
    "        ay_matched.append(imu_match['ay'].iloc[0])\n",
    "        az_matched.append(imu_match['az'].iloc[0])\n",
    "        gx_matched.append(imu_match['gx'].iloc[0])\n",
    "        gy_matched.append(imu_match['gy'].iloc[0])\n",
    "        gz_matched.append(imu_match['gz'].iloc[0])\n",
    "    \n",
    "    # Append the timestamp\n",
    "    timestamps.append(timestamp)\n",
    "\n",
    "# Create a dataframe with the matched data\n",
    "matched_data = pd.DataFrame({\n",
    "    'timestamps': timestamps,\n",
    "    'w_matched': w_matched,\n",
    "    's_matched': s_matched,\n",
    "    'ax_matched': ax_matched,\n",
    "    'ay_matched': ay_matched,\n",
    "    'az_matched': az_matched,\n",
    "    'gx_matched': gx_matched,\n",
    "    'gy_matched': gy_matched,\n",
    "    'gz_matched': gz_matched\n",
    "})\n",
    "\n",
    "# Save the matched data to a CSV file\n",
    "matched_data.to_csv('ssnew1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e575c6b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db5ad2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c855f3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221d5239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e70908d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03d2ac4b",
   "metadata": {},
   "source": [
    "## Importing Libraries & Loading Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f726b91d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "33b40b88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>matching_values</th>\n",
       "      <th>ax</th>\n",
       "      <th>ay</th>\n",
       "      <th>az</th>\n",
       "      <th>gx</th>\n",
       "      <th>gy</th>\n",
       "      <th>gz</th>\n",
       "      <th>s</th>\n",
       "      <th>w</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59.12</td>\n",
       "      <td>415.04</td>\n",
       "      <td>1533.69</td>\n",
       "      <td>-279.30</td>\n",
       "      <td>-54.51</td>\n",
       "      <td>16.04</td>\n",
       "      <td>-18.60</td>\n",
       "      <td>580000.0</td>\n",
       "      <td>0.602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59.15</td>\n",
       "      <td>96.68</td>\n",
       "      <td>575.68</td>\n",
       "      <td>399.41</td>\n",
       "      <td>-10.06</td>\n",
       "      <td>-9.51</td>\n",
       "      <td>5.12</td>\n",
       "      <td>580000.0</td>\n",
       "      <td>1.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59.18</td>\n",
       "      <td>15.14</td>\n",
       "      <td>981.45</td>\n",
       "      <td>185.06</td>\n",
       "      <td>-7.07</td>\n",
       "      <td>-4.88</td>\n",
       "      <td>1.34</td>\n",
       "      <td>580000.0</td>\n",
       "      <td>2.906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.46</td>\n",
       "      <td>30.27</td>\n",
       "      <td>962.89</td>\n",
       "      <td>188.96</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.16</td>\n",
       "      <td>3.05</td>\n",
       "      <td>607998.5</td>\n",
       "      <td>50.824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59.59</td>\n",
       "      <td>23.44</td>\n",
       "      <td>950.20</td>\n",
       "      <td>191.41</td>\n",
       "      <td>5.18</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.52</td>\n",
       "      <td>646791.0</td>\n",
       "      <td>57.660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   matching_values      ax       ay      az     gx     gy     gz         s  \\\n",
       "0            59.12  415.04  1533.69 -279.30 -54.51  16.04 -18.60  580000.0   \n",
       "1            59.15   96.68   575.68  399.41 -10.06  -9.51   5.12  580000.0   \n",
       "2            59.18   15.14   981.45  185.06  -7.07  -4.88   1.34  580000.0   \n",
       "3            59.46   30.27   962.89  188.96   0.43   1.16   3.05  607998.5   \n",
       "4            59.59   23.44   950.20  191.41   5.18   1.71   1.52  646791.0   \n",
       "\n",
       "        w  \n",
       "0   0.602  \n",
       "1   1.112  \n",
       "2   2.906  \n",
       "3  50.824  \n",
       "4  57.660  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"nmain.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "63488e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(667, 9)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488e43a4",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "aec0af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"matching_values\", axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a4243f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction features\n",
    "df['s_ax'] = df['s'] * df['ax']\n",
    "df['s_ay'] = df['s'] * df['ay']\n",
    "df['s_az'] = df['s'] * df['az']\n",
    "\n",
    "df['s_gx'] = df['s'] * df['gx']\n",
    "df['s_gy'] = df['s'] * df['gy']\n",
    "df['s_gz'] = df['s'] * df['gz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e49ac8a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ax</th>\n",
       "      <th>ay</th>\n",
       "      <th>az</th>\n",
       "      <th>gx</th>\n",
       "      <th>gy</th>\n",
       "      <th>gz</th>\n",
       "      <th>s</th>\n",
       "      <th>w</th>\n",
       "      <th>s_ax</th>\n",
       "      <th>s_ay</th>\n",
       "      <th>s_az</th>\n",
       "      <th>s_gx</th>\n",
       "      <th>s_gy</th>\n",
       "      <th>s_gz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>415.04</td>\n",
       "      <td>1533.69</td>\n",
       "      <td>-279.30</td>\n",
       "      <td>-54.51</td>\n",
       "      <td>16.04</td>\n",
       "      <td>-18.60</td>\n",
       "      <td>580000.0</td>\n",
       "      <td>0.602</td>\n",
       "      <td>2.407232e+08</td>\n",
       "      <td>8.895402e+08</td>\n",
       "      <td>-1.619940e+08</td>\n",
       "      <td>-3.161580e+07</td>\n",
       "      <td>9303200.00</td>\n",
       "      <td>-1.078800e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96.68</td>\n",
       "      <td>575.68</td>\n",
       "      <td>399.41</td>\n",
       "      <td>-10.06</td>\n",
       "      <td>-9.51</td>\n",
       "      <td>5.12</td>\n",
       "      <td>580000.0</td>\n",
       "      <td>1.112</td>\n",
       "      <td>5.607440e+07</td>\n",
       "      <td>3.338944e+08</td>\n",
       "      <td>2.316578e+08</td>\n",
       "      <td>-5.834800e+06</td>\n",
       "      <td>-5515800.00</td>\n",
       "      <td>2.969600e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.14</td>\n",
       "      <td>981.45</td>\n",
       "      <td>185.06</td>\n",
       "      <td>-7.07</td>\n",
       "      <td>-4.88</td>\n",
       "      <td>1.34</td>\n",
       "      <td>580000.0</td>\n",
       "      <td>2.906</td>\n",
       "      <td>8.781200e+06</td>\n",
       "      <td>5.692410e+08</td>\n",
       "      <td>1.073348e+08</td>\n",
       "      <td>-4.100600e+06</td>\n",
       "      <td>-2830400.00</td>\n",
       "      <td>7.772000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.27</td>\n",
       "      <td>962.89</td>\n",
       "      <td>188.96</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.16</td>\n",
       "      <td>3.05</td>\n",
       "      <td>607998.5</td>\n",
       "      <td>50.824</td>\n",
       "      <td>1.840411e+07</td>\n",
       "      <td>5.854357e+08</td>\n",
       "      <td>1.148874e+08</td>\n",
       "      <td>2.614394e+05</td>\n",
       "      <td>705278.26</td>\n",
       "      <td>1.854395e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.44</td>\n",
       "      <td>950.20</td>\n",
       "      <td>191.41</td>\n",
       "      <td>5.18</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.52</td>\n",
       "      <td>646791.0</td>\n",
       "      <td>57.660</td>\n",
       "      <td>1.516078e+07</td>\n",
       "      <td>6.145808e+08</td>\n",
       "      <td>1.238023e+08</td>\n",
       "      <td>3.350377e+06</td>\n",
       "      <td>1106012.61</td>\n",
       "      <td>9.831223e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ax       ay      az     gx     gy     gz         s       w  \\\n",
       "0  415.04  1533.69 -279.30 -54.51  16.04 -18.60  580000.0   0.602   \n",
       "1   96.68   575.68  399.41 -10.06  -9.51   5.12  580000.0   1.112   \n",
       "2   15.14   981.45  185.06  -7.07  -4.88   1.34  580000.0   2.906   \n",
       "3   30.27   962.89  188.96   0.43   1.16   3.05  607998.5  50.824   \n",
       "4   23.44   950.20  191.41   5.18   1.71   1.52  646791.0  57.660   \n",
       "\n",
       "           s_ax          s_ay          s_az          s_gx        s_gy  \\\n",
       "0  2.407232e+08  8.895402e+08 -1.619940e+08 -3.161580e+07  9303200.00   \n",
       "1  5.607440e+07  3.338944e+08  2.316578e+08 -5.834800e+06 -5515800.00   \n",
       "2  8.781200e+06  5.692410e+08  1.073348e+08 -4.100600e+06 -2830400.00   \n",
       "3  1.840411e+07  5.854357e+08  1.148874e+08  2.614394e+05   705278.26   \n",
       "4  1.516078e+07  6.145808e+08  1.238023e+08  3.350377e+06  1106012.61   \n",
       "\n",
       "           s_gz  \n",
       "0 -1.078800e+07  \n",
       "1  2.969600e+06  \n",
       "2  7.772000e+05  \n",
       "3  1.854395e+06  \n",
       "4  9.831223e+05  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "423a2d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc X Outlier Count: 79\n",
      "Acc Y Outlier Count: 165\n",
      "Acc Z Outlier Count: 113\n",
      "Gyro X Outlier Count: 87\n",
      "Gyro Y Outlier Count: 78\n",
      "Gyro Z Outlier Count: 369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5141/223150818.py:18: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data1 = df['ax']\n",
    "data2 = df['ay']\n",
    "data3 = df['az']\n",
    "data4 = df['gx']\n",
    "data5 = df['gy']\n",
    "data6 = df['gz']\n",
    "\n",
    "fig, ax = plt.subplots(2, 3, figsize=(15, 5))\n",
    "\n",
    "bp1 = ax[0, 0].boxplot(data1, vert=True)\n",
    "ax[0, 0].set_title(\"Acc X\")\n",
    "lower_whisker = bp1['whiskers'][0].get_ydata()[1]\n",
    "upper_whisker = bp1['whiskers'][1].get_ydata()[1]\n",
    "outlier_count = sum((data1 < lower_whisker) | (data1 > upper_whisker))\n",
    "print(\"Acc X Outlier Count:\", outlier_count)\n",
    "fig.show()\n",
    "\n",
    "bp2 = ax[0, 1].boxplot(data2, vert=True)\n",
    "ax[0, 1].set_title(\"Acc Y\")\n",
    "lower_whisker = bp2['whiskers'][0].get_ydata()[1]\n",
    "upper_whisker = bp2['whiskers'][1].get_ydata()[1]\n",
    "outlier_count = sum((data2 < lower_whisker) | (data2 > upper_whisker))\n",
    "print(\"Acc Y Outlier Count:\", outlier_count)\n",
    "\n",
    "bp3 = ax[0, 2].boxplot(data3, vert=True)\n",
    "ax[0, 2].set_title(\"Acc Z\")\n",
    "lower_whisker = bp3['whiskers'][0].get_ydata()[1]\n",
    "upper_whisker = bp3['whiskers'][1].get_ydata()[1]\n",
    "outlier_count = sum((data3 < lower_whisker) | (data3 > upper_whisker))\n",
    "print(\"Acc Z Outlier Count:\", outlier_count)\n",
    "\n",
    "bp4 = ax[1, 0].boxplot(data4, vert=True)\n",
    "ax[1, 0].set_title(\"Gyro X\")\n",
    "lower_whisker = bp4['whiskers'][0].get_ydata()[1]\n",
    "upper_whisker = bp4['whiskers'][1].get_ydata()[1]\n",
    "outlier_count = sum((data4 < lower_whisker) | (data4 > upper_whisker))\n",
    "print(\"Gyro X Outlier Count:\", outlier_count)\n",
    "\n",
    "bp5 = ax[1, 1].boxplot(data5, vert=True)\n",
    "ax[1, 1].set_title(\"Gyro Y\")\n",
    "lower_whisker = bp4['whiskers'][0].get_ydata()[1]\n",
    "upper_whisker = bp4['whiskers'][1].get_ydata()[1]\n",
    "outlier_count = sum((data5 < lower_whisker) | (data5 > upper_whisker))\n",
    "print(\"Gyro Y Outlier Count:\", outlier_count)\n",
    "\n",
    "bp5 = ax[1, 2].boxplot(data6, vert=True)\n",
    "ax[1, 2].set_title(\"Gyro Z\")\n",
    "lower_whisker = bp4['whiskers'][0].get_ydata()[1]\n",
    "upper_whisker = bp4['whiskers'][1].get_ydata()[1]\n",
    "outlier_count = sum((data6 < lower_whisker) | (data6 > upper_whisker))\n",
    "print(\"Gyro Z Outlier Count:\", outlier_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "3dcfb553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x7f164a8cda00>,\n",
       "  <matplotlib.lines.Line2D at 0x7f164a8cd460>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x7f164a8cd070>,\n",
       "  <matplotlib.lines.Line2D at 0x7f164a8cda30>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x7f164a8cd970>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x7f164a8ee220>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x7f164a8cdac0>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data1 = df['w']\n",
    "plt.boxplot(data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69114ad",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "376cb6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ax</th>\n",
       "      <th>ay</th>\n",
       "      <th>az</th>\n",
       "      <th>gx</th>\n",
       "      <th>gy</th>\n",
       "      <th>gz</th>\n",
       "      <th>s</th>\n",
       "      <th>w</th>\n",
       "      <th>s_ax</th>\n",
       "      <th>s_ay</th>\n",
       "      <th>s_az</th>\n",
       "      <th>s_gx</th>\n",
       "      <th>s_gy</th>\n",
       "      <th>s_gz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>415.04</td>\n",
       "      <td>1533.69</td>\n",
       "      <td>-279.30</td>\n",
       "      <td>-54.51</td>\n",
       "      <td>16.04</td>\n",
       "      <td>-18.60</td>\n",
       "      <td>580000.00</td>\n",
       "      <td>0.602</td>\n",
       "      <td>2.407232e+08</td>\n",
       "      <td>8.895402e+08</td>\n",
       "      <td>-1.619940e+08</td>\n",
       "      <td>-3.161580e+07</td>\n",
       "      <td>9.303200e+06</td>\n",
       "      <td>-1.078800e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96.68</td>\n",
       "      <td>575.68</td>\n",
       "      <td>399.41</td>\n",
       "      <td>-10.06</td>\n",
       "      <td>-9.51</td>\n",
       "      <td>5.12</td>\n",
       "      <td>580000.00</td>\n",
       "      <td>1.112</td>\n",
       "      <td>5.607440e+07</td>\n",
       "      <td>3.338944e+08</td>\n",
       "      <td>2.316578e+08</td>\n",
       "      <td>-5.834800e+06</td>\n",
       "      <td>-5.515800e+06</td>\n",
       "      <td>2.969600e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.14</td>\n",
       "      <td>981.45</td>\n",
       "      <td>185.06</td>\n",
       "      <td>-7.07</td>\n",
       "      <td>-4.88</td>\n",
       "      <td>1.34</td>\n",
       "      <td>580000.00</td>\n",
       "      <td>2.906</td>\n",
       "      <td>8.781200e+06</td>\n",
       "      <td>5.692410e+08</td>\n",
       "      <td>1.073348e+08</td>\n",
       "      <td>-4.100600e+06</td>\n",
       "      <td>-2.830400e+06</td>\n",
       "      <td>7.772000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.27</td>\n",
       "      <td>962.89</td>\n",
       "      <td>188.96</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.16</td>\n",
       "      <td>3.05</td>\n",
       "      <td>607998.50</td>\n",
       "      <td>50.824</td>\n",
       "      <td>1.840411e+07</td>\n",
       "      <td>5.854357e+08</td>\n",
       "      <td>1.148874e+08</td>\n",
       "      <td>2.614394e+05</td>\n",
       "      <td>7.052783e+05</td>\n",
       "      <td>1.854395e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.44</td>\n",
       "      <td>950.20</td>\n",
       "      <td>191.41</td>\n",
       "      <td>5.18</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.52</td>\n",
       "      <td>646791.00</td>\n",
       "      <td>57.660</td>\n",
       "      <td>1.516078e+07</td>\n",
       "      <td>6.145808e+08</td>\n",
       "      <td>1.238023e+08</td>\n",
       "      <td>3.350377e+06</td>\n",
       "      <td>1.106013e+06</td>\n",
       "      <td>9.831223e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>199.22</td>\n",
       "      <td>888.67</td>\n",
       "      <td>93.26</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-15.73</td>\n",
       "      <td>86.71</td>\n",
       "      <td>710290.50</td>\n",
       "      <td>55.184</td>\n",
       "      <td>1.415041e+08</td>\n",
       "      <td>6.312139e+08</td>\n",
       "      <td>6.624169e+07</td>\n",
       "      <td>4.758946e+05</td>\n",
       "      <td>-1.117287e+07</td>\n",
       "      <td>6.158929e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>333.01</td>\n",
       "      <td>869.63</td>\n",
       "      <td>77.64</td>\n",
       "      <td>-24.94</td>\n",
       "      <td>-5.79</td>\n",
       "      <td>57.93</td>\n",
       "      <td>694045.50</td>\n",
       "      <td>56.256</td>\n",
       "      <td>2.311241e+08</td>\n",
       "      <td>6.035628e+08</td>\n",
       "      <td>5.388569e+07</td>\n",
       "      <td>-1.730949e+07</td>\n",
       "      <td>-4.018523e+06</td>\n",
       "      <td>4.020606e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>339.84</td>\n",
       "      <td>919.43</td>\n",
       "      <td>155.27</td>\n",
       "      <td>-30.98</td>\n",
       "      <td>1.16</td>\n",
       "      <td>51.34</td>\n",
       "      <td>697636.00</td>\n",
       "      <td>55.926</td>\n",
       "      <td>2.370846e+08</td>\n",
       "      <td>6.414275e+08</td>\n",
       "      <td>1.083219e+08</td>\n",
       "      <td>-2.161276e+07</td>\n",
       "      <td>8.092578e+05</td>\n",
       "      <td>3.581663e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>315.43</td>\n",
       "      <td>845.21</td>\n",
       "      <td>315.92</td>\n",
       "      <td>-26.52</td>\n",
       "      <td>-5.24</td>\n",
       "      <td>66.40</td>\n",
       "      <td>703798.50</td>\n",
       "      <td>54.058</td>\n",
       "      <td>2.219992e+08</td>\n",
       "      <td>5.948575e+08</td>\n",
       "      <td>2.223440e+08</td>\n",
       "      <td>-1.866474e+07</td>\n",
       "      <td>-3.687904e+06</td>\n",
       "      <td>4.673222e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>434.57</td>\n",
       "      <td>1640.63</td>\n",
       "      <td>734.37</td>\n",
       "      <td>22.20</td>\n",
       "      <td>-26.89</td>\n",
       "      <td>154.02</td>\n",
       "      <td>674534.25</td>\n",
       "      <td>52.108</td>\n",
       "      <td>2.931323e+08</td>\n",
       "      <td>1.106661e+09</td>\n",
       "      <td>4.953577e+08</td>\n",
       "      <td>1.497466e+07</td>\n",
       "      <td>-1.813823e+07</td>\n",
       "      <td>1.038918e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>667 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ax       ay      az     gx     gy      gz          s       w  \\\n",
       "0    415.04  1533.69 -279.30 -54.51  16.04  -18.60  580000.00   0.602   \n",
       "1     96.68   575.68  399.41 -10.06  -9.51    5.12  580000.00   1.112   \n",
       "2     15.14   981.45  185.06  -7.07  -4.88    1.34  580000.00   2.906   \n",
       "3     30.27   962.89  188.96   0.43   1.16    3.05  607998.50  50.824   \n",
       "4     23.44   950.20  191.41   5.18   1.71    1.52  646791.00  57.660   \n",
       "..      ...      ...     ...    ...    ...     ...        ...     ...   \n",
       "662  199.22   888.67   93.26   0.67 -15.73   86.71  710290.50  55.184   \n",
       "663  333.01   869.63   77.64 -24.94  -5.79   57.93  694045.50  56.256   \n",
       "664  339.84   919.43  155.27 -30.98   1.16   51.34  697636.00  55.926   \n",
       "665  315.43   845.21  315.92 -26.52  -5.24   66.40  703798.50  54.058   \n",
       "666  434.57  1640.63  734.37  22.20 -26.89  154.02  674534.25  52.108   \n",
       "\n",
       "             s_ax          s_ay          s_az          s_gx          s_gy  \\\n",
       "0    2.407232e+08  8.895402e+08 -1.619940e+08 -3.161580e+07  9.303200e+06   \n",
       "1    5.607440e+07  3.338944e+08  2.316578e+08 -5.834800e+06 -5.515800e+06   \n",
       "2    8.781200e+06  5.692410e+08  1.073348e+08 -4.100600e+06 -2.830400e+06   \n",
       "3    1.840411e+07  5.854357e+08  1.148874e+08  2.614394e+05  7.052783e+05   \n",
       "4    1.516078e+07  6.145808e+08  1.238023e+08  3.350377e+06  1.106013e+06   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "662  1.415041e+08  6.312139e+08  6.624169e+07  4.758946e+05 -1.117287e+07   \n",
       "663  2.311241e+08  6.035628e+08  5.388569e+07 -1.730949e+07 -4.018523e+06   \n",
       "664  2.370846e+08  6.414275e+08  1.083219e+08 -2.161276e+07  8.092578e+05   \n",
       "665  2.219992e+08  5.948575e+08  2.223440e+08 -1.866474e+07 -3.687904e+06   \n",
       "666  2.931323e+08  1.106661e+09  4.953577e+08  1.497466e+07 -1.813823e+07   \n",
       "\n",
       "             s_gz  \n",
       "0   -1.078800e+07  \n",
       "1    2.969600e+06  \n",
       "2    7.772000e+05  \n",
       "3    1.854395e+06  \n",
       "4    9.831223e+05  \n",
       "..            ...  \n",
       "662  6.158929e+07  \n",
       "663  4.020606e+07  \n",
       "664  3.581663e+07  \n",
       "665  4.673222e+07  \n",
       "666  1.038918e+08  \n",
       "\n",
       "[667 rows x 14 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5dc927d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"s\", \"ax\", \"ay\", \"az\", \"gx\", \"gy\", \"gz\", \"s_ax\", \"s_ay\", \"s_az\", \"s_gx\", \"s_gy\", \"s_gz\"]]\n",
    "\n",
    "min_values = X.min().values\n",
    "max_values = X.max().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "955e2b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.8000000e+05, -1.1762700e+03, -1.8154300e+03, -1.2465800e+03,\n",
       "       -2.7177000e+02, -1.1701000e+02, -3.1159000e+02, -6.8223660e+08,\n",
       "       -1.0529494e+09, -7.2301640e+08, -1.5762660e+08, -6.7865800e+07,\n",
       "       -1.8072220e+08])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "35d445ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.0790025e+05, 2.3178700e+03, 3.9998800e+03, 2.9209000e+03,\n",
       "       1.9994000e+02, 1.9110000e+02, 3.6909000e+02, 1.3443646e+09,\n",
       "       2.3199304e+09, 1.6941220e+09, 1.1596520e+08, 1.1083800e+08,\n",
       "       2.1407220e+08])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ac8c5d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = (s-s_min)/(s_max-s_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65afc680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1f7ab3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ax</th>\n",
       "      <th>ay</th>\n",
       "      <th>az</th>\n",
       "      <th>gx</th>\n",
       "      <th>gy</th>\n",
       "      <th>gz</th>\n",
       "      <th>s</th>\n",
       "      <th>w</th>\n",
       "      <th>s_ax</th>\n",
       "      <th>s_ay</th>\n",
       "      <th>s_az</th>\n",
       "      <th>s_gx</th>\n",
       "      <th>s_gy</th>\n",
       "      <th>s_gz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>415.04</td>\n",
       "      <td>1533.69</td>\n",
       "      <td>-279.30</td>\n",
       "      <td>-54.51</td>\n",
       "      <td>16.04</td>\n",
       "      <td>-18.60</td>\n",
       "      <td>580000.0</td>\n",
       "      <td>0.602</td>\n",
       "      <td>2.407232e+08</td>\n",
       "      <td>8.895402e+08</td>\n",
       "      <td>-1.619940e+08</td>\n",
       "      <td>-3.161580e+07</td>\n",
       "      <td>9303200.00</td>\n",
       "      <td>-1.078800e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96.68</td>\n",
       "      <td>575.68</td>\n",
       "      <td>399.41</td>\n",
       "      <td>-10.06</td>\n",
       "      <td>-9.51</td>\n",
       "      <td>5.12</td>\n",
       "      <td>580000.0</td>\n",
       "      <td>1.112</td>\n",
       "      <td>5.607440e+07</td>\n",
       "      <td>3.338944e+08</td>\n",
       "      <td>2.316578e+08</td>\n",
       "      <td>-5.834800e+06</td>\n",
       "      <td>-5515800.00</td>\n",
       "      <td>2.969600e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.14</td>\n",
       "      <td>981.45</td>\n",
       "      <td>185.06</td>\n",
       "      <td>-7.07</td>\n",
       "      <td>-4.88</td>\n",
       "      <td>1.34</td>\n",
       "      <td>580000.0</td>\n",
       "      <td>2.906</td>\n",
       "      <td>8.781200e+06</td>\n",
       "      <td>5.692410e+08</td>\n",
       "      <td>1.073348e+08</td>\n",
       "      <td>-4.100600e+06</td>\n",
       "      <td>-2830400.00</td>\n",
       "      <td>7.772000e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.27</td>\n",
       "      <td>962.89</td>\n",
       "      <td>188.96</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.16</td>\n",
       "      <td>3.05</td>\n",
       "      <td>607998.5</td>\n",
       "      <td>50.824</td>\n",
       "      <td>1.840411e+07</td>\n",
       "      <td>5.854357e+08</td>\n",
       "      <td>1.148874e+08</td>\n",
       "      <td>2.614394e+05</td>\n",
       "      <td>705278.26</td>\n",
       "      <td>1.854395e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23.44</td>\n",
       "      <td>950.20</td>\n",
       "      <td>191.41</td>\n",
       "      <td>5.18</td>\n",
       "      <td>1.71</td>\n",
       "      <td>1.52</td>\n",
       "      <td>646791.0</td>\n",
       "      <td>57.660</td>\n",
       "      <td>1.516078e+07</td>\n",
       "      <td>6.145808e+08</td>\n",
       "      <td>1.238023e+08</td>\n",
       "      <td>3.350377e+06</td>\n",
       "      <td>1106012.61</td>\n",
       "      <td>9.831223e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ax       ay      az     gx     gy     gz         s       w  \\\n",
       "0  415.04  1533.69 -279.30 -54.51  16.04 -18.60  580000.0   0.602   \n",
       "1   96.68   575.68  399.41 -10.06  -9.51   5.12  580000.0   1.112   \n",
       "2   15.14   981.45  185.06  -7.07  -4.88   1.34  580000.0   2.906   \n",
       "3   30.27   962.89  188.96   0.43   1.16   3.05  607998.5  50.824   \n",
       "4   23.44   950.20  191.41   5.18   1.71   1.52  646791.0  57.660   \n",
       "\n",
       "           s_ax          s_ay          s_az          s_gx        s_gy  \\\n",
       "0  2.407232e+08  8.895402e+08 -1.619940e+08 -3.161580e+07  9303200.00   \n",
       "1  5.607440e+07  3.338944e+08  2.316578e+08 -5.834800e+06 -5515800.00   \n",
       "2  8.781200e+06  5.692410e+08  1.073348e+08 -4.100600e+06 -2830400.00   \n",
       "3  1.840411e+07  5.854357e+08  1.148874e+08  2.614394e+05   705278.26   \n",
       "4  1.516078e+07  6.145808e+08  1.238023e+08  3.350377e+06  1106012.61   \n",
       "\n",
       "           s_gz  \n",
       "0 -1.078800e+07  \n",
       "1  2.969600e+06  \n",
       "2  7.772000e+05  \n",
       "3  1.854395e+06  \n",
       "4  9.831223e+05  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "937b5a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.504]\n",
      "[80.98]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalize the input features\n",
    "scaler = MinMaxScaler()\n",
    "df[['s', 'ax', 'ay', 'az', 'gx', 'gy', 'gz', 's_ax', 's_ay', 's_az', 's_gx', 's_gy', 's_gz']] = scaler.fit_transform(df[['s', 'ax', 'ay', 'az', 'gx', 'gy', 'gz', 's_ax', 's_ay', 's_az', 's_gx', 's_gy', 's_gz']])\n",
    "\n",
    "# Normalize the output features\n",
    "df[['w']] = scaler.fit_transform(df[['w']])\n",
    "\n",
    "\n",
    "# access the mean of the normalization\n",
    "print(scaler.data_min_)\n",
    "\n",
    "# access the standard deviation of the normalization\n",
    "print(scaler.data_range_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "25b0e70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5362238367109357"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "90ba07e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ax</th>\n",
       "      <th>ay</th>\n",
       "      <th>az</th>\n",
       "      <th>gx</th>\n",
       "      <th>gy</th>\n",
       "      <th>gz</th>\n",
       "      <th>s</th>\n",
       "      <th>w</th>\n",
       "      <th>s_ax</th>\n",
       "      <th>s_ay</th>\n",
       "      <th>s_az</th>\n",
       "      <th>s_gx</th>\n",
       "      <th>s_gy</th>\n",
       "      <th>s_gz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455423</td>\n",
       "      <td>0.575914</td>\n",
       "      <td>0.232102</td>\n",
       "      <td>0.460580</td>\n",
       "      <td>0.431826</td>\n",
       "      <td>0.430437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.455423</td>\n",
       "      <td>0.575914</td>\n",
       "      <td>0.232102</td>\n",
       "      <td>0.460580</td>\n",
       "      <td>0.431826</td>\n",
       "      <td>0.430437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.364310</td>\n",
       "      <td>0.411175</td>\n",
       "      <td>0.394961</td>\n",
       "      <td>0.554811</td>\n",
       "      <td>0.348901</td>\n",
       "      <td>0.465285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007508</td>\n",
       "      <td>0.364310</td>\n",
       "      <td>0.411175</td>\n",
       "      <td>0.394961</td>\n",
       "      <td>0.554811</td>\n",
       "      <td>0.348901</td>\n",
       "      <td>0.465285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.340974</td>\n",
       "      <td>0.480951</td>\n",
       "      <td>0.343527</td>\n",
       "      <td>0.561150</td>\n",
       "      <td>0.363928</td>\n",
       "      <td>0.459731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029662</td>\n",
       "      <td>0.340974</td>\n",
       "      <td>0.480951</td>\n",
       "      <td>0.343527</td>\n",
       "      <td>0.561150</td>\n",
       "      <td>0.363928</td>\n",
       "      <td>0.459731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.345304</td>\n",
       "      <td>0.477760</td>\n",
       "      <td>0.344462</td>\n",
       "      <td>0.577049</td>\n",
       "      <td>0.383532</td>\n",
       "      <td>0.462244</td>\n",
       "      <td>0.085387</td>\n",
       "      <td>0.621388</td>\n",
       "      <td>0.345722</td>\n",
       "      <td>0.485753</td>\n",
       "      <td>0.346651</td>\n",
       "      <td>0.577093</td>\n",
       "      <td>0.383714</td>\n",
       "      <td>0.462460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.343349</td>\n",
       "      <td>0.475577</td>\n",
       "      <td>0.345050</td>\n",
       "      <td>0.587119</td>\n",
       "      <td>0.385317</td>\n",
       "      <td>0.459996</td>\n",
       "      <td>0.203693</td>\n",
       "      <td>0.705804</td>\n",
       "      <td>0.344122</td>\n",
       "      <td>0.494394</td>\n",
       "      <td>0.350339</td>\n",
       "      <td>0.588384</td>\n",
       "      <td>0.385956</td>\n",
       "      <td>0.460253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ax        ay        az        gx        gy        gz         s  \\\n",
       "0  0.455423  0.575914  0.232102  0.460580  0.431826  0.430437  0.000000   \n",
       "1  0.364310  0.411175  0.394961  0.554811  0.348901  0.465285  0.000000   \n",
       "2  0.340974  0.480951  0.343527  0.561150  0.363928  0.459731  0.000000   \n",
       "3  0.345304  0.477760  0.344462  0.577049  0.383532  0.462244  0.085387   \n",
       "4  0.343349  0.475577  0.345050  0.587119  0.385317  0.459996  0.203693   \n",
       "\n",
       "          w      s_ax      s_ay      s_az      s_gx      s_gy      s_gz  \n",
       "0  0.001210  0.455423  0.575914  0.232102  0.460580  0.431826  0.430437  \n",
       "1  0.007508  0.364310  0.411175  0.394961  0.554811  0.348901  0.465285  \n",
       "2  0.029662  0.340974  0.480951  0.343527  0.561150  0.363928  0.459731  \n",
       "3  0.621388  0.345722  0.485753  0.346651  0.577093  0.383714  0.462460  \n",
       "4  0.705804  0.344122  0.494394  0.350339  0.588384  0.385956  0.460253  "
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "470fa81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"s\", \"ax\", \"ay\", \"az\", \"gx\", \"gy\", \"gz\", \"s_ax\", \"s_ay\", \"s_az\", \"s_gx\", \"s_gy\", \"s_gz\"]]\n",
    "\n",
    "min_values_n = X.min().values\n",
    "max_values_n = X.max().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9a8067ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_values_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8b429ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_values_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57078159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a5b49a6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.001210\n",
       "1      0.007508\n",
       "2      0.029662\n",
       "3      0.621388\n",
       "4      0.705804\n",
       "         ...   \n",
       "662    0.675228\n",
       "663    0.688466\n",
       "664    0.684391\n",
       "665    0.661324\n",
       "666    0.637244\n",
       "Name: w, Length: 667, dtype: float64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0ebb0bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 8.53872481e-02, 2.03693044e-01, 2.58955277e-01,\n",
       "       2.95348204e-01, 4.47951778e-01, 4.76185212e-01, 7.22563493e-01,\n",
       "       7.32980533e-01, 7.42691565e-01, 7.52160909e-01, 7.55569720e-01,\n",
       "       7.26523539e-01, 7.01990926e-01, 4.24626087e-02, 6.21492359e-02,\n",
       "       2.21979093e-01, 3.16536050e-01, 5.53990886e-01, 7.38295411e-01,\n",
       "       7.03110931e-01, 6.57476626e-01, 5.97549865e-01, 4.24712241e-01,\n",
       "       3.51944684e-01, 3.47190952e-01, 5.14313423e-01, 5.16500064e-01,\n",
       "       5.01800471e-01, 1.58744923e-02, 2.46119056e-02, 4.72751241e-01,\n",
       "       3.83342190e-01, 2.40204910e-01, 5.58385515e-01, 8.24368844e-01,\n",
       "       8.71037000e-01, 8.19239235e-01, 7.65186974e-01, 7.08532549e-01,\n",
       "       6.33117084e-01, 5.47376832e-01, 5.40195532e-01, 5.15548555e-01,\n",
       "       4.78194207e-01, 2.37571487e-01, 1.36848783e-01, 2.67704126e-01,\n",
       "       4.97845763e-01, 5.46814923e-01, 5.96326932e-01, 7.28240524e-01,\n",
       "       8.07913382e-01, 7.01252896e-01, 7.40403522e-01, 6.37543735e-01,\n",
       "       6.25752039e-01, 6.07064191e-01, 4.26345360e-01, 2.76884510e-01,\n",
       "       1.85277382e-02, 2.11980625e-01, 4.90553148e-01, 5.63894050e-01,\n",
       "       6.10012496e-01, 7.40586505e-01, 7.55669598e-01, 7.69663945e-01,\n",
       "       7.08603455e-01, 5.78958083e-01, 5.10172682e-01, 4.90696485e-01,\n",
       "       4.81849282e-01, 4.58550276e-01, 4.27354051e-01, 3.28782915e-01,\n",
       "       1.81459910e-01, 2.95896389e-01, 3.59520159e-01, 3.86864603e-01,\n",
       "       4.41514607e-01, 6.51951318e-01, 7.40188518e-01, 7.28916035e-01,\n",
       "       5.39391934e-01, 5.23465597e-01, 4.98050855e-01, 4.45445681e-01,\n",
       "       3.81515415e-01, 3.10064570e-01, 2.19381504e-01, 8.20615416e-02,\n",
       "       3.98018910e-01, 4.35874172e-01, 5.11988021e-01, 6.49623628e-01,\n",
       "       6.69170731e-01, 5.83857438e-01, 5.52349381e-01, 5.25627077e-01,\n",
       "       5.30938906e-01, 5.38858235e-01, 5.47229683e-01, 5.45638498e-01,\n",
       "       4.34940199e-01, 4.21485650e-01, 4.02442511e-01, 3.85963414e-01,\n",
       "       3.46142615e-01, 3.05376406e-01, 1.47662284e-01, 5.80925907e-01,\n",
       "       5.75827557e-01, 5.78053844e-01, 6.46224728e-01, 5.27214450e-01,\n",
       "       3.86237125e-01, 3.83044844e-01, 6.17931063e-01, 5.86616662e-01,\n",
       "       4.50433478e-01, 4.28054721e-01, 2.79470662e-01, 1.79828317e-01,\n",
       "       2.95024173e-01, 2.92473854e-01, 3.01622978e-01, 5.95333489e-01,\n",
       "       6.56692089e-01, 7.06870458e-01, 7.22853215e-01, 6.92817404e-01,\n",
       "       6.58471593e-01, 6.42280694e-01, 6.30813792e-01, 5.25809297e-01,\n",
       "       1.94848891e-01, 2.58420053e-01, 4.01252363e-01, 4.49494930e-01,\n",
       "       4.79156390e-01, 5.79741095e-01, 5.41776806e-01, 5.14545963e-01,\n",
       "       5.03911632e-01, 4.99940149e-01, 4.89716766e-01, 4.29828126e-01,\n",
       "       3.00251372e-02, 1.15140961e-01, 1.23017595e-01, 7.21111070e-02,\n",
       "       1.22531929e-01, 1.72272665e-01, 3.19502654e-01, 4.46780690e-01,\n",
       "       4.55449485e-01, 4.45586730e-01, 4.06921770e-01, 3.78166073e-01,\n",
       "       3.29366934e-01, 9.84750698e-02, 4.53575439e-02, 1.14441053e-01,\n",
       "       1.64880936e-01, 2.24164971e-01, 2.89379926e-01, 4.81499328e-01,\n",
       "       4.43846109e-01, 2.72363318e-01, 3.50400770e-01, 3.14563652e-01,\n",
       "       2.99953263e-01, 2.88900359e-01, 2.74565969e-01, 2.73502384e-01,\n",
       "       2.54727619e-01, 1.91496499e-01, 1.99294603e-01, 2.06432444e-01,\n",
       "       3.82501996e-01, 2.75685212e-01, 7.78087238e-02, 2.87991546e-02,\n",
       "       1.17695092e-01, 1.73908834e-01, 1.78256192e-01, 1.62290209e-01,\n",
       "       1.45927001e-01, 1.32380198e-01, 1.28038939e-01, 2.26355424e-01,\n",
       "       4.17505781e-01, 2.81820462e-01, 1.62454893e-01, 1.31828963e-01,\n",
       "       9.68183159e-02, 3.36109076e-01, 3.71500937e-01, 3.87800101e-01,\n",
       "       3.21433881e-01, 2.78524490e-01, 2.69339532e-01, 2.51157936e-01,\n",
       "       3.01434659e-01, 3.42676622e-01, 4.19700046e-01, 3.77537071e-01,\n",
       "       3.29397431e-01, 1.50477165e-01, 1.24554647e-01, 8.77820313e-02,\n",
       "       3.25015001e-02, 5.41651005e-02, 6.61169060e-02, 6.25228252e-02,\n",
       "       9.30298162e-02, 2.28814251e-01, 2.26581102e-01, 2.27617240e-01,\n",
       "       2.36175483e-01, 2.40130954e-01, 2.37672127e-01, 2.83792861e-01,\n",
       "       4.75730805e-01, 7.95282254e-01, 7.38816912e-01, 6.44298838e-01,\n",
       "       4.97146617e-01, 2.88911033e-01, 8.85185357e-02, 9.66162728e-02,\n",
       "       1.29331252e-01, 1.44984641e-01, 3.10956610e-01, 4.26761645e-01,\n",
       "       4.28709646e-01, 4.38595274e-01, 4.51619052e-01, 4.68753836e-01,\n",
       "       4.86028907e-01, 4.99950823e-01, 5.10358714e-01, 4.91269067e-01,\n",
       "       4.17284677e-01, 1.01520966e-01, 1.08943192e-02, 1.11556792e-01,\n",
       "       1.91955480e-01, 2.85238422e-01, 4.59953904e-01, 4.90569922e-01,\n",
       "       5.47305926e-01, 5.79259241e-01, 6.11118015e-01, 2.45578495e-01,\n",
       "       3.02343472e-01, 2.73859962e-01, 3.14686402e-01, 3.70315363e-01,\n",
       "       4.77020832e-01, 5.38391630e-01, 5.94088446e-01, 5.04773174e-01,\n",
       "       1.98272188e-01, 4.79259317e-01, 7.52819646e-01, 9.85240938e-01,\n",
       "       9.58560568e-01, 6.37829645e-01, 1.94433368e-01, 1.40553415e-01,\n",
       "       7.16971091e-02, 2.50586116e-01, 2.95827008e-01, 8.99441217e-01,\n",
       "       1.29358700e-01, 4.82151203e-02, 9.20401860e-03, 2.31440812e-01,\n",
       "       2.94734450e-01, 3.93483232e-01, 5.66275110e-01, 6.13970255e-01,\n",
       "       6.77578013e-01, 7.53578260e-01, 8.09752356e-01, 9.21338883e-01,\n",
       "       7.08503577e-01, 3.06901260e-01, 2.33650325e-01, 2.05429853e-01,\n",
       "       4.82021590e-02, 8.16101848e-03, 3.98566332e-02, 1.03391199e-01,\n",
       "       1.98742605e-01, 2.66045085e-01, 3.30292520e-01, 1.00000000e+00,\n",
       "       9.98283777e-01, 3.86207391e-02, 4.50042353e-01, 3.79067262e-01,\n",
       "       3.61709087e-01, 3.51337793e-01, 3.17344985e-01, 2.04042998e-01,\n",
       "       1.26552968e-01, 2.56405721e-01, 3.86354539e-01, 5.15999149e-01,\n",
       "       4.05256629e-01, 3.96407901e-01, 3.75624904e-01, 3.64339460e-01,\n",
       "       2.35570116e-01, 2.79810705e-04, 7.37670984e-02, 2.21172445e-01,\n",
       "       2.54328870e-01, 2.80532723e-01, 4.35868835e-01, 4.70569175e-01,\n",
       "       4.46236317e-01, 3.16434647e-01, 3.05363445e-01, 2.98435271e-01,\n",
       "       2.84235068e-01, 1.83869180e-01, 3.72379253e-01, 3.82136031e-01,\n",
       "       3.84331820e-01, 3.86860791e-01, 3.87284700e-01, 4.65929806e-01,\n",
       "       4.33185092e-01, 4.02334246e-01, 3.86855454e-01, 3.87418125e-01,\n",
       "       3.83562532e-01, 3.70306214e-01, 3.47344200e-01, 3.29586513e-01,\n",
       "       2.52174251e-01, 9.55145658e-02, 4.37894604e-01, 4.49160225e-01,\n",
       "       3.58968162e-01, 3.75732406e-01, 3.55632544e-01, 3.37659090e-01,\n",
       "       4.66666311e-02, 2.01411100e-01, 2.27258137e-01, 3.45897876e-01,\n",
       "       3.22889354e-01, 3.07070519e-01, 2.63190559e-01, 2.65448867e-01,\n",
       "       2.67526481e-01, 2.67916081e-01, 2.67212361e-01, 2.66383603e-01,\n",
       "       2.63114316e-01, 2.53713591e-01, 1.36931887e-02, 1.43921818e-01,\n",
       "       3.73707400e-01, 2.96283702e-01, 2.83012898e-01, 2.69375366e-01,\n",
       "       2.16689374e-02, 2.12731616e-01, 3.43127979e-01, 3.92208454e-01,\n",
       "       3.74344789e-01, 3.13316321e-01, 3.03718890e-01, 2.97607275e-01,\n",
       "       2.92772726e-01, 2.85503747e-01, 3.60907014e-01, 3.72641527e-01,\n",
       "       3.83567869e-01, 3.95130074e-01, 3.90956549e-01, 3.88255270e-01,\n",
       "       2.16361531e-01, 1.65809572e-01, 7.80275404e-02, 3.18700580e-01,\n",
       "       3.15478564e-01, 3.09562893e-01, 2.71131998e-01, 1.30535887e-01,\n",
       "       3.56795245e-01, 3.52280152e-01, 3.09818306e-01, 3.07579058e-01,\n",
       "       4.05191060e-01, 3.44766434e-01, 3.53150844e-01, 3.56375910e-01,\n",
       "       3.00222400e-01, 1.14975515e-01, 4.59081687e-01, 3.97347974e-01,\n",
       "       3.47805468e-01, 3.58755445e-01, 3.77549270e-01, 2.88301854e-01])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.s.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "10648c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ax</th>\n",
       "      <th>ay</th>\n",
       "      <th>az</th>\n",
       "      <th>gx</th>\n",
       "      <th>gy</th>\n",
       "      <th>gz</th>\n",
       "      <th>s</th>\n",
       "      <th>w</th>\n",
       "      <th>s_ax</th>\n",
       "      <th>s_ay</th>\n",
       "      <th>s_az</th>\n",
       "      <th>s_gx</th>\n",
       "      <th>s_gy</th>\n",
       "      <th>s_gz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455423</td>\n",
       "      <td>0.575914</td>\n",
       "      <td>0.232102</td>\n",
       "      <td>0.460580</td>\n",
       "      <td>0.431826</td>\n",
       "      <td>0.430437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.455423</td>\n",
       "      <td>0.575914</td>\n",
       "      <td>0.232102</td>\n",
       "      <td>0.460580</td>\n",
       "      <td>0.431826</td>\n",
       "      <td>0.430437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.364310</td>\n",
       "      <td>0.411175</td>\n",
       "      <td>0.394961</td>\n",
       "      <td>0.554811</td>\n",
       "      <td>0.348901</td>\n",
       "      <td>0.465285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007508</td>\n",
       "      <td>0.364310</td>\n",
       "      <td>0.411175</td>\n",
       "      <td>0.394961</td>\n",
       "      <td>0.554811</td>\n",
       "      <td>0.348901</td>\n",
       "      <td>0.465285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.340974</td>\n",
       "      <td>0.480951</td>\n",
       "      <td>0.343527</td>\n",
       "      <td>0.561150</td>\n",
       "      <td>0.363928</td>\n",
       "      <td>0.459731</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029662</td>\n",
       "      <td>0.340974</td>\n",
       "      <td>0.480951</td>\n",
       "      <td>0.343527</td>\n",
       "      <td>0.561150</td>\n",
       "      <td>0.363928</td>\n",
       "      <td>0.459731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.345304</td>\n",
       "      <td>0.477760</td>\n",
       "      <td>0.344462</td>\n",
       "      <td>0.577049</td>\n",
       "      <td>0.383532</td>\n",
       "      <td>0.462244</td>\n",
       "      <td>0.085387</td>\n",
       "      <td>0.621388</td>\n",
       "      <td>0.345722</td>\n",
       "      <td>0.485753</td>\n",
       "      <td>0.346651</td>\n",
       "      <td>0.577093</td>\n",
       "      <td>0.383714</td>\n",
       "      <td>0.462460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.343349</td>\n",
       "      <td>0.475577</td>\n",
       "      <td>0.345050</td>\n",
       "      <td>0.587119</td>\n",
       "      <td>0.385317</td>\n",
       "      <td>0.459996</td>\n",
       "      <td>0.203693</td>\n",
       "      <td>0.705804</td>\n",
       "      <td>0.344122</td>\n",
       "      <td>0.494394</td>\n",
       "      <td>0.350339</td>\n",
       "      <td>0.588384</td>\n",
       "      <td>0.385956</td>\n",
       "      <td>0.460253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ax        ay        az        gx        gy        gz         s  \\\n",
       "0  0.455423  0.575914  0.232102  0.460580  0.431826  0.430437  0.000000   \n",
       "1  0.364310  0.411175  0.394961  0.554811  0.348901  0.465285  0.000000   \n",
       "2  0.340974  0.480951  0.343527  0.561150  0.363928  0.459731  0.000000   \n",
       "3  0.345304  0.477760  0.344462  0.577049  0.383532  0.462244  0.085387   \n",
       "4  0.343349  0.475577  0.345050  0.587119  0.385317  0.459996  0.203693   \n",
       "\n",
       "          w      s_ax      s_ay      s_az      s_gx      s_gy      s_gz  \n",
       "0  0.001210  0.455423  0.575914  0.232102  0.460580  0.431826  0.430437  \n",
       "1  0.007508  0.364310  0.411175  0.394961  0.554811  0.348901  0.465285  \n",
       "2  0.029662  0.340974  0.480951  0.343527  0.561150  0.363928  0.459731  \n",
       "3  0.621388  0.345722  0.485753  0.346651  0.577093  0.383714  0.462460  \n",
       "4  0.705804  0.344122  0.494394  0.350339  0.588384  0.385956  0.460253  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6043ca54",
   "metadata": {},
   "source": [
    "### Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2edb12b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into inputs and outputs\n",
    "X = df[[\"s\", \"ax\", \"ay\", \"az\", \"gx\", \"gy\", \"gz\", \"s_ax\", \"s_ay\", \"s_az\", \"s_gx\", \"s_gy\", \"s_gz\"]]\n",
    "y = df[\"w\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "7cd3761f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>ax</th>\n",
       "      <th>ay</th>\n",
       "      <th>az</th>\n",
       "      <th>gx</th>\n",
       "      <th>gy</th>\n",
       "      <th>gz</th>\n",
       "      <th>s_ax</th>\n",
       "      <th>s_ay</th>\n",
       "      <th>s_az</th>\n",
       "      <th>s_gx</th>\n",
       "      <th>s_gy</th>\n",
       "      <th>s_gz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.455423</td>\n",
       "      <td>0.575914</td>\n",
       "      <td>0.232102</td>\n",
       "      <td>0.460580</td>\n",
       "      <td>0.431826</td>\n",
       "      <td>0.430437</td>\n",
       "      <td>0.455423</td>\n",
       "      <td>0.575914</td>\n",
       "      <td>0.232102</td>\n",
       "      <td>0.460580</td>\n",
       "      <td>0.431826</td>\n",
       "      <td>0.430437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364310</td>\n",
       "      <td>0.411175</td>\n",
       "      <td>0.394961</td>\n",
       "      <td>0.554811</td>\n",
       "      <td>0.348901</td>\n",
       "      <td>0.465285</td>\n",
       "      <td>0.364310</td>\n",
       "      <td>0.411175</td>\n",
       "      <td>0.394961</td>\n",
       "      <td>0.554811</td>\n",
       "      <td>0.348901</td>\n",
       "      <td>0.465285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340974</td>\n",
       "      <td>0.480951</td>\n",
       "      <td>0.343527</td>\n",
       "      <td>0.561150</td>\n",
       "      <td>0.363928</td>\n",
       "      <td>0.459731</td>\n",
       "      <td>0.340974</td>\n",
       "      <td>0.480951</td>\n",
       "      <td>0.343527</td>\n",
       "      <td>0.561150</td>\n",
       "      <td>0.363928</td>\n",
       "      <td>0.459731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.085387</td>\n",
       "      <td>0.345304</td>\n",
       "      <td>0.477760</td>\n",
       "      <td>0.344462</td>\n",
       "      <td>0.577049</td>\n",
       "      <td>0.383532</td>\n",
       "      <td>0.462244</td>\n",
       "      <td>0.345722</td>\n",
       "      <td>0.485753</td>\n",
       "      <td>0.346651</td>\n",
       "      <td>0.577093</td>\n",
       "      <td>0.383714</td>\n",
       "      <td>0.462460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.203693</td>\n",
       "      <td>0.343349</td>\n",
       "      <td>0.475577</td>\n",
       "      <td>0.345050</td>\n",
       "      <td>0.587119</td>\n",
       "      <td>0.385317</td>\n",
       "      <td>0.459996</td>\n",
       "      <td>0.344122</td>\n",
       "      <td>0.494394</td>\n",
       "      <td>0.350339</td>\n",
       "      <td>0.588384</td>\n",
       "      <td>0.385956</td>\n",
       "      <td>0.460253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          s        ax        ay        az        gx        gy        gz  \\\n",
       "0  0.000000  0.455423  0.575914  0.232102  0.460580  0.431826  0.430437   \n",
       "1  0.000000  0.364310  0.411175  0.394961  0.554811  0.348901  0.465285   \n",
       "2  0.000000  0.340974  0.480951  0.343527  0.561150  0.363928  0.459731   \n",
       "3  0.085387  0.345304  0.477760  0.344462  0.577049  0.383532  0.462244   \n",
       "4  0.203693  0.343349  0.475577  0.345050  0.587119  0.385317  0.459996   \n",
       "\n",
       "       s_ax      s_ay      s_az      s_gx      s_gy      s_gz  \n",
       "0  0.455423  0.575914  0.232102  0.460580  0.431826  0.430437  \n",
       "1  0.364310  0.411175  0.394961  0.554811  0.348901  0.465285  \n",
       "2  0.340974  0.480951  0.343527  0.561150  0.363928  0.459731  \n",
       "3  0.345722  0.485753  0.346651  0.577093  0.383714  0.462460  \n",
       "4  0.344122  0.494394  0.350339  0.588384  0.385956  0.460253  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "caa16c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>ax</th>\n",
       "      <th>ay</th>\n",
       "      <th>az</th>\n",
       "      <th>gx</th>\n",
       "      <th>gy</th>\n",
       "      <th>gz</th>\n",
       "      <th>s_ax</th>\n",
       "      <th>s_ay</th>\n",
       "      <th>s_az</th>\n",
       "      <th>s_gx</th>\n",
       "      <th>s_gy</th>\n",
       "      <th>s_gz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.455423</td>\n",
       "      <td>0.575914</td>\n",
       "      <td>0.232102</td>\n",
       "      <td>0.460580</td>\n",
       "      <td>0.431826</td>\n",
       "      <td>0.430437</td>\n",
       "      <td>0.455423</td>\n",
       "      <td>0.575914</td>\n",
       "      <td>0.232102</td>\n",
       "      <td>0.460580</td>\n",
       "      <td>0.431826</td>\n",
       "      <td>0.430437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364310</td>\n",
       "      <td>0.411175</td>\n",
       "      <td>0.394961</td>\n",
       "      <td>0.554811</td>\n",
       "      <td>0.348901</td>\n",
       "      <td>0.465285</td>\n",
       "      <td>0.364310</td>\n",
       "      <td>0.411175</td>\n",
       "      <td>0.394961</td>\n",
       "      <td>0.554811</td>\n",
       "      <td>0.348901</td>\n",
       "      <td>0.465285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340974</td>\n",
       "      <td>0.480951</td>\n",
       "      <td>0.343527</td>\n",
       "      <td>0.561150</td>\n",
       "      <td>0.363928</td>\n",
       "      <td>0.459731</td>\n",
       "      <td>0.340974</td>\n",
       "      <td>0.480951</td>\n",
       "      <td>0.343527</td>\n",
       "      <td>0.561150</td>\n",
       "      <td>0.363928</td>\n",
       "      <td>0.459731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.085387</td>\n",
       "      <td>0.345304</td>\n",
       "      <td>0.477760</td>\n",
       "      <td>0.344462</td>\n",
       "      <td>0.577049</td>\n",
       "      <td>0.383532</td>\n",
       "      <td>0.462244</td>\n",
       "      <td>0.345722</td>\n",
       "      <td>0.485753</td>\n",
       "      <td>0.346651</td>\n",
       "      <td>0.577093</td>\n",
       "      <td>0.383714</td>\n",
       "      <td>0.462460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.203693</td>\n",
       "      <td>0.343349</td>\n",
       "      <td>0.475577</td>\n",
       "      <td>0.345050</td>\n",
       "      <td>0.587119</td>\n",
       "      <td>0.385317</td>\n",
       "      <td>0.459996</td>\n",
       "      <td>0.344122</td>\n",
       "      <td>0.494394</td>\n",
       "      <td>0.350339</td>\n",
       "      <td>0.588384</td>\n",
       "      <td>0.385956</td>\n",
       "      <td>0.460253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>0.397348</td>\n",
       "      <td>0.393656</td>\n",
       "      <td>0.464997</td>\n",
       "      <td>0.321499</td>\n",
       "      <td>0.577558</td>\n",
       "      <td>0.328714</td>\n",
       "      <td>0.585150</td>\n",
       "      <td>0.406464</td>\n",
       "      <td>0.499325</td>\n",
       "      <td>0.326526</td>\n",
       "      <td>0.577877</td>\n",
       "      <td>0.317245</td>\n",
       "      <td>0.613766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>0.347805</td>\n",
       "      <td>0.431946</td>\n",
       "      <td>0.461723</td>\n",
       "      <td>0.317751</td>\n",
       "      <td>0.523266</td>\n",
       "      <td>0.360975</td>\n",
       "      <td>0.542869</td>\n",
       "      <td>0.450686</td>\n",
       "      <td>0.491127</td>\n",
       "      <td>0.321414</td>\n",
       "      <td>0.512870</td>\n",
       "      <td>0.357280</td>\n",
       "      <td>0.559603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>0.358755</td>\n",
       "      <td>0.433901</td>\n",
       "      <td>0.470286</td>\n",
       "      <td>0.336378</td>\n",
       "      <td>0.510462</td>\n",
       "      <td>0.383532</td>\n",
       "      <td>0.533187</td>\n",
       "      <td>0.453627</td>\n",
       "      <td>0.502353</td>\n",
       "      <td>0.343935</td>\n",
       "      <td>0.497141</td>\n",
       "      <td>0.384295</td>\n",
       "      <td>0.548485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>0.377549</td>\n",
       "      <td>0.426915</td>\n",
       "      <td>0.457523</td>\n",
       "      <td>0.374927</td>\n",
       "      <td>0.519917</td>\n",
       "      <td>0.362760</td>\n",
       "      <td>0.555312</td>\n",
       "      <td>0.446183</td>\n",
       "      <td>0.488546</td>\n",
       "      <td>0.391107</td>\n",
       "      <td>0.507917</td>\n",
       "      <td>0.359130</td>\n",
       "      <td>0.576134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>0.288302</td>\n",
       "      <td>0.461012</td>\n",
       "      <td>0.594304</td>\n",
       "      <td>0.475335</td>\n",
       "      <td>0.623201</td>\n",
       "      <td>0.292493</td>\n",
       "      <td>0.684037</td>\n",
       "      <td>0.481283</td>\n",
       "      <td>0.640287</td>\n",
       "      <td>0.504056</td>\n",
       "      <td>0.630871</td>\n",
       "      <td>0.278268</td>\n",
       "      <td>0.720917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>667 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            s        ax        ay        az        gx        gy        gz  \\\n",
       "0    0.000000  0.455423  0.575914  0.232102  0.460580  0.431826  0.430437   \n",
       "1    0.000000  0.364310  0.411175  0.394961  0.554811  0.348901  0.465285   \n",
       "2    0.000000  0.340974  0.480951  0.343527  0.561150  0.363928  0.459731   \n",
       "3    0.085387  0.345304  0.477760  0.344462  0.577049  0.383532  0.462244   \n",
       "4    0.203693  0.343349  0.475577  0.345050  0.587119  0.385317  0.459996   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "662  0.397348  0.393656  0.464997  0.321499  0.577558  0.328714  0.585150   \n",
       "663  0.347805  0.431946  0.461723  0.317751  0.523266  0.360975  0.542869   \n",
       "664  0.358755  0.433901  0.470286  0.336378  0.510462  0.383532  0.533187   \n",
       "665  0.377549  0.426915  0.457523  0.374927  0.519917  0.362760  0.555312   \n",
       "666  0.288302  0.461012  0.594304  0.475335  0.623201  0.292493  0.684037   \n",
       "\n",
       "         s_ax      s_ay      s_az      s_gx      s_gy      s_gz  \n",
       "0    0.455423  0.575914  0.232102  0.460580  0.431826  0.430437  \n",
       "1    0.364310  0.411175  0.394961  0.554811  0.348901  0.465285  \n",
       "2    0.340974  0.480951  0.343527  0.561150  0.363928  0.459731  \n",
       "3    0.345722  0.485753  0.346651  0.577093  0.383714  0.462460  \n",
       "4    0.344122  0.494394  0.350339  0.588384  0.385956  0.460253  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "662  0.406464  0.499325  0.326526  0.577877  0.317245  0.613766  \n",
       "663  0.450686  0.491127  0.321414  0.512870  0.357280  0.559603  \n",
       "664  0.453627  0.502353  0.343935  0.497141  0.384295  0.548485  \n",
       "665  0.446183  0.488546  0.391107  0.507917  0.359130  0.576134  \n",
       "666  0.481283  0.640287  0.504056  0.630871  0.278268  0.720917  \n",
       "\n",
       "[667 rows x 13 columns]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6cdbebb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.001210\n",
       "1    0.007508\n",
       "2    0.029662\n",
       "3    0.621388\n",
       "4    0.705804\n",
       "Name: w, dtype: float64"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "59d894b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00000000e+00, 8.53872481e-02, 2.03693044e-01, 2.58955277e-01,\n",
       "       2.95348204e-01, 4.47951778e-01, 4.76185212e-01, 7.22563493e-01,\n",
       "       7.32980533e-01, 7.42691565e-01, 7.52160909e-01, 7.55569720e-01,\n",
       "       7.26523539e-01, 7.01990926e-01, 4.24626087e-02, 6.21492359e-02,\n",
       "       2.21979093e-01, 3.16536050e-01, 5.53990886e-01, 7.38295411e-01,\n",
       "       7.03110931e-01, 6.57476626e-01, 5.97549865e-01, 4.24712241e-01,\n",
       "       3.51944684e-01, 3.47190952e-01, 5.14313423e-01, 5.16500064e-01,\n",
       "       5.01800471e-01, 1.58744923e-02, 2.46119056e-02, 4.72751241e-01,\n",
       "       3.83342190e-01, 2.40204910e-01, 5.58385515e-01, 8.24368844e-01,\n",
       "       8.71037000e-01, 8.19239235e-01, 7.65186974e-01, 7.08532549e-01,\n",
       "       6.33117084e-01, 5.47376832e-01, 5.40195532e-01, 5.15548555e-01,\n",
       "       4.78194207e-01, 2.37571487e-01, 1.36848783e-01, 2.67704126e-01,\n",
       "       4.97845763e-01, 5.46814923e-01, 5.96326932e-01, 7.28240524e-01,\n",
       "       8.07913382e-01, 7.01252896e-01, 7.40403522e-01, 6.37543735e-01,\n",
       "       6.25752039e-01, 6.07064191e-01, 4.26345360e-01, 2.76884510e-01,\n",
       "       1.85277382e-02, 2.11980625e-01, 4.90553148e-01, 5.63894050e-01,\n",
       "       6.10012496e-01, 7.40586505e-01, 7.55669598e-01, 7.69663945e-01,\n",
       "       7.08603455e-01, 5.78958083e-01, 5.10172682e-01, 4.90696485e-01,\n",
       "       4.81849282e-01, 4.58550276e-01, 4.27354051e-01, 3.28782915e-01,\n",
       "       1.81459910e-01, 2.95896389e-01, 3.59520159e-01, 3.86864603e-01,\n",
       "       4.41514607e-01, 6.51951318e-01, 7.40188518e-01, 7.28916035e-01,\n",
       "       5.39391934e-01, 5.23465597e-01, 4.98050855e-01, 4.45445681e-01,\n",
       "       3.81515415e-01, 3.10064570e-01, 2.19381504e-01, 8.20615416e-02,\n",
       "       3.98018910e-01, 4.35874172e-01, 5.11988021e-01, 6.49623628e-01,\n",
       "       6.69170731e-01, 5.83857438e-01, 5.52349381e-01, 5.25627077e-01,\n",
       "       5.30938906e-01, 5.38858235e-01, 5.47229683e-01, 5.45638498e-01,\n",
       "       4.34940199e-01, 4.21485650e-01, 4.02442511e-01, 3.85963414e-01,\n",
       "       3.46142615e-01, 3.05376406e-01, 1.47662284e-01, 5.80925907e-01,\n",
       "       5.75827557e-01, 5.78053844e-01, 6.46224728e-01, 5.27214450e-01,\n",
       "       3.86237125e-01, 3.83044844e-01, 6.17931063e-01, 5.86616662e-01,\n",
       "       4.50433478e-01, 4.28054721e-01, 2.79470662e-01, 1.79828317e-01,\n",
       "       2.95024173e-01, 2.92473854e-01, 3.01622978e-01, 5.95333489e-01,\n",
       "       6.56692089e-01, 7.06870458e-01, 7.22853215e-01, 6.92817404e-01,\n",
       "       6.58471593e-01, 6.42280694e-01, 6.30813792e-01, 5.25809297e-01,\n",
       "       1.94848891e-01, 2.58420053e-01, 4.01252363e-01, 4.49494930e-01,\n",
       "       4.79156390e-01, 5.79741095e-01, 5.41776806e-01, 5.14545963e-01,\n",
       "       5.03911632e-01, 4.99940149e-01, 4.89716766e-01, 4.29828126e-01,\n",
       "       3.00251372e-02, 1.15140961e-01, 1.23017595e-01, 7.21111070e-02,\n",
       "       1.22531929e-01, 1.72272665e-01, 3.19502654e-01, 4.46780690e-01,\n",
       "       4.55449485e-01, 4.45586730e-01, 4.06921770e-01, 3.78166073e-01,\n",
       "       3.29366934e-01, 9.84750698e-02, 4.53575439e-02, 1.14441053e-01,\n",
       "       1.64880936e-01, 2.24164971e-01, 2.89379926e-01, 4.81499328e-01,\n",
       "       4.43846109e-01, 2.72363318e-01, 3.50400770e-01, 3.14563652e-01,\n",
       "       2.99953263e-01, 2.88900359e-01, 2.74565969e-01, 2.73502384e-01,\n",
       "       2.54727619e-01, 1.91496499e-01, 1.99294603e-01, 2.06432444e-01,\n",
       "       3.82501996e-01, 2.75685212e-01, 7.78087238e-02, 2.87991546e-02,\n",
       "       1.17695092e-01, 1.73908834e-01, 1.78256192e-01, 1.62290209e-01,\n",
       "       1.45927001e-01, 1.32380198e-01, 1.28038939e-01, 2.26355424e-01,\n",
       "       4.17505781e-01, 2.81820462e-01, 1.62454893e-01, 1.31828963e-01,\n",
       "       9.68183159e-02, 3.36109076e-01, 3.71500937e-01, 3.87800101e-01,\n",
       "       3.21433881e-01, 2.78524490e-01, 2.69339532e-01, 2.51157936e-01,\n",
       "       3.01434659e-01, 3.42676622e-01, 4.19700046e-01, 3.77537071e-01,\n",
       "       3.29397431e-01, 1.50477165e-01, 1.24554647e-01, 8.77820313e-02,\n",
       "       3.25015001e-02, 5.41651005e-02, 6.61169060e-02, 6.25228252e-02,\n",
       "       9.30298162e-02, 2.28814251e-01, 2.26581102e-01, 2.27617240e-01,\n",
       "       2.36175483e-01, 2.40130954e-01, 2.37672127e-01, 2.83792861e-01,\n",
       "       4.75730805e-01, 7.95282254e-01, 7.38816912e-01, 6.44298838e-01,\n",
       "       4.97146617e-01, 2.88911033e-01, 8.85185357e-02, 9.66162728e-02,\n",
       "       1.29331252e-01, 1.44984641e-01, 3.10956610e-01, 4.26761645e-01,\n",
       "       4.28709646e-01, 4.38595274e-01, 4.51619052e-01, 4.68753836e-01,\n",
       "       4.86028907e-01, 4.99950823e-01, 5.10358714e-01, 4.91269067e-01,\n",
       "       4.17284677e-01, 1.01520966e-01, 1.08943192e-02, 1.11556792e-01,\n",
       "       1.91955480e-01, 2.85238422e-01, 4.59953904e-01, 4.90569922e-01,\n",
       "       5.47305926e-01, 5.79259241e-01, 6.11118015e-01, 2.45578495e-01,\n",
       "       3.02343472e-01, 2.73859962e-01, 3.14686402e-01, 3.70315363e-01,\n",
       "       4.77020832e-01, 5.38391630e-01, 5.94088446e-01, 5.04773174e-01,\n",
       "       1.98272188e-01, 4.79259317e-01, 7.52819646e-01, 9.85240938e-01,\n",
       "       9.58560568e-01, 6.37829645e-01, 1.94433368e-01, 1.40553415e-01,\n",
       "       7.16971091e-02, 2.50586116e-01, 2.95827008e-01, 8.99441217e-01,\n",
       "       1.29358700e-01, 4.82151203e-02, 9.20401860e-03, 2.31440812e-01,\n",
       "       2.94734450e-01, 3.93483232e-01, 5.66275110e-01, 6.13970255e-01,\n",
       "       6.77578013e-01, 7.53578260e-01, 8.09752356e-01, 9.21338883e-01,\n",
       "       7.08503577e-01, 3.06901260e-01, 2.33650325e-01, 2.05429853e-01,\n",
       "       4.82021590e-02, 8.16101848e-03, 3.98566332e-02, 1.03391199e-01,\n",
       "       1.98742605e-01, 2.66045085e-01, 3.30292520e-01, 1.00000000e+00,\n",
       "       9.98283777e-01, 3.86207391e-02, 4.50042353e-01, 3.79067262e-01,\n",
       "       3.61709087e-01, 3.51337793e-01, 3.17344985e-01, 2.04042998e-01,\n",
       "       1.26552968e-01, 2.56405721e-01, 3.86354539e-01, 5.15999149e-01,\n",
       "       4.05256629e-01, 3.96407901e-01, 3.75624904e-01, 3.64339460e-01,\n",
       "       2.35570116e-01, 2.79810705e-04, 7.37670984e-02, 2.21172445e-01,\n",
       "       2.54328870e-01, 2.80532723e-01, 4.35868835e-01, 4.70569175e-01,\n",
       "       4.46236317e-01, 3.16434647e-01, 3.05363445e-01, 2.98435271e-01,\n",
       "       2.84235068e-01, 1.83869180e-01, 3.72379253e-01, 3.82136031e-01,\n",
       "       3.84331820e-01, 3.86860791e-01, 3.87284700e-01, 4.65929806e-01,\n",
       "       4.33185092e-01, 4.02334246e-01, 3.86855454e-01, 3.87418125e-01,\n",
       "       3.83562532e-01, 3.70306214e-01, 3.47344200e-01, 3.29586513e-01,\n",
       "       2.52174251e-01, 9.55145658e-02, 4.37894604e-01, 4.49160225e-01,\n",
       "       3.58968162e-01, 3.75732406e-01, 3.55632544e-01, 3.37659090e-01,\n",
       "       4.66666311e-02, 2.01411100e-01, 2.27258137e-01, 3.45897876e-01,\n",
       "       3.22889354e-01, 3.07070519e-01, 2.63190559e-01, 2.65448867e-01,\n",
       "       2.67526481e-01, 2.67916081e-01, 2.67212361e-01, 2.66383603e-01,\n",
       "       2.63114316e-01, 2.53713591e-01, 1.36931887e-02, 1.43921818e-01,\n",
       "       3.73707400e-01, 2.96283702e-01, 2.83012898e-01, 2.69375366e-01,\n",
       "       2.16689374e-02, 2.12731616e-01, 3.43127979e-01, 3.92208454e-01,\n",
       "       3.74344789e-01, 3.13316321e-01, 3.03718890e-01, 2.97607275e-01,\n",
       "       2.92772726e-01, 2.85503747e-01, 3.60907014e-01, 3.72641527e-01,\n",
       "       3.83567869e-01, 3.95130074e-01, 3.90956549e-01, 3.88255270e-01,\n",
       "       2.16361531e-01, 1.65809572e-01, 7.80275404e-02, 3.18700580e-01,\n",
       "       3.15478564e-01, 3.09562893e-01, 2.71131998e-01, 1.30535887e-01,\n",
       "       3.56795245e-01, 3.52280152e-01, 3.09818306e-01, 3.07579058e-01,\n",
       "       4.05191060e-01, 3.44766434e-01, 3.53150844e-01, 3.56375910e-01,\n",
       "       3.00222400e-01, 1.14975515e-01, 4.59081687e-01, 3.97347974e-01,\n",
       "       3.47805468e-01, 3.58755445e-01, 3.77549270e-01, 2.88301854e-01])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.s.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "08af986d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>ax</th>\n",
       "      <th>ay</th>\n",
       "      <th>az</th>\n",
       "      <th>gx</th>\n",
       "      <th>gy</th>\n",
       "      <th>gz</th>\n",
       "      <th>s_ax</th>\n",
       "      <th>s_ay</th>\n",
       "      <th>s_az</th>\n",
       "      <th>s_gx</th>\n",
       "      <th>s_gy</th>\n",
       "      <th>s_gz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.455423</td>\n",
       "      <td>0.575914</td>\n",
       "      <td>0.232102</td>\n",
       "      <td>0.460580</td>\n",
       "      <td>0.431826</td>\n",
       "      <td>0.430437</td>\n",
       "      <td>0.455423</td>\n",
       "      <td>0.575914</td>\n",
       "      <td>0.232102</td>\n",
       "      <td>0.460580</td>\n",
       "      <td>0.431826</td>\n",
       "      <td>0.430437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364310</td>\n",
       "      <td>0.411175</td>\n",
       "      <td>0.394961</td>\n",
       "      <td>0.554811</td>\n",
       "      <td>0.348901</td>\n",
       "      <td>0.465285</td>\n",
       "      <td>0.364310</td>\n",
       "      <td>0.411175</td>\n",
       "      <td>0.394961</td>\n",
       "      <td>0.554811</td>\n",
       "      <td>0.348901</td>\n",
       "      <td>0.465285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340974</td>\n",
       "      <td>0.480951</td>\n",
       "      <td>0.343527</td>\n",
       "      <td>0.561150</td>\n",
       "      <td>0.363928</td>\n",
       "      <td>0.459731</td>\n",
       "      <td>0.340974</td>\n",
       "      <td>0.480951</td>\n",
       "      <td>0.343527</td>\n",
       "      <td>0.561150</td>\n",
       "      <td>0.363928</td>\n",
       "      <td>0.459731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.085387</td>\n",
       "      <td>0.345304</td>\n",
       "      <td>0.477760</td>\n",
       "      <td>0.344462</td>\n",
       "      <td>0.577049</td>\n",
       "      <td>0.383532</td>\n",
       "      <td>0.462244</td>\n",
       "      <td>0.345722</td>\n",
       "      <td>0.485753</td>\n",
       "      <td>0.346651</td>\n",
       "      <td>0.577093</td>\n",
       "      <td>0.383714</td>\n",
       "      <td>0.462460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.203693</td>\n",
       "      <td>0.343349</td>\n",
       "      <td>0.475577</td>\n",
       "      <td>0.345050</td>\n",
       "      <td>0.587119</td>\n",
       "      <td>0.385317</td>\n",
       "      <td>0.459996</td>\n",
       "      <td>0.344122</td>\n",
       "      <td>0.494394</td>\n",
       "      <td>0.350339</td>\n",
       "      <td>0.588384</td>\n",
       "      <td>0.385956</td>\n",
       "      <td>0.460253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>0.397348</td>\n",
       "      <td>0.393656</td>\n",
       "      <td>0.464997</td>\n",
       "      <td>0.321499</td>\n",
       "      <td>0.577558</td>\n",
       "      <td>0.328714</td>\n",
       "      <td>0.585150</td>\n",
       "      <td>0.406464</td>\n",
       "      <td>0.499325</td>\n",
       "      <td>0.326526</td>\n",
       "      <td>0.577877</td>\n",
       "      <td>0.317245</td>\n",
       "      <td>0.613766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>0.347805</td>\n",
       "      <td>0.431946</td>\n",
       "      <td>0.461723</td>\n",
       "      <td>0.317751</td>\n",
       "      <td>0.523266</td>\n",
       "      <td>0.360975</td>\n",
       "      <td>0.542869</td>\n",
       "      <td>0.450686</td>\n",
       "      <td>0.491127</td>\n",
       "      <td>0.321414</td>\n",
       "      <td>0.512870</td>\n",
       "      <td>0.357280</td>\n",
       "      <td>0.559603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>0.358755</td>\n",
       "      <td>0.433901</td>\n",
       "      <td>0.470286</td>\n",
       "      <td>0.336378</td>\n",
       "      <td>0.510462</td>\n",
       "      <td>0.383532</td>\n",
       "      <td>0.533187</td>\n",
       "      <td>0.453627</td>\n",
       "      <td>0.502353</td>\n",
       "      <td>0.343935</td>\n",
       "      <td>0.497141</td>\n",
       "      <td>0.384295</td>\n",
       "      <td>0.548485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>0.377549</td>\n",
       "      <td>0.426915</td>\n",
       "      <td>0.457523</td>\n",
       "      <td>0.374927</td>\n",
       "      <td>0.519917</td>\n",
       "      <td>0.362760</td>\n",
       "      <td>0.555312</td>\n",
       "      <td>0.446183</td>\n",
       "      <td>0.488546</td>\n",
       "      <td>0.391107</td>\n",
       "      <td>0.507917</td>\n",
       "      <td>0.359130</td>\n",
       "      <td>0.576134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>0.288302</td>\n",
       "      <td>0.461012</td>\n",
       "      <td>0.594304</td>\n",
       "      <td>0.475335</td>\n",
       "      <td>0.623201</td>\n",
       "      <td>0.292493</td>\n",
       "      <td>0.684037</td>\n",
       "      <td>0.481283</td>\n",
       "      <td>0.640287</td>\n",
       "      <td>0.504056</td>\n",
       "      <td>0.630871</td>\n",
       "      <td>0.278268</td>\n",
       "      <td>0.720917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>667 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            s        ax        ay        az        gx        gy        gz  \\\n",
       "0    0.000000  0.455423  0.575914  0.232102  0.460580  0.431826  0.430437   \n",
       "1    0.000000  0.364310  0.411175  0.394961  0.554811  0.348901  0.465285   \n",
       "2    0.000000  0.340974  0.480951  0.343527  0.561150  0.363928  0.459731   \n",
       "3    0.085387  0.345304  0.477760  0.344462  0.577049  0.383532  0.462244   \n",
       "4    0.203693  0.343349  0.475577  0.345050  0.587119  0.385317  0.459996   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "662  0.397348  0.393656  0.464997  0.321499  0.577558  0.328714  0.585150   \n",
       "663  0.347805  0.431946  0.461723  0.317751  0.523266  0.360975  0.542869   \n",
       "664  0.358755  0.433901  0.470286  0.336378  0.510462  0.383532  0.533187   \n",
       "665  0.377549  0.426915  0.457523  0.374927  0.519917  0.362760  0.555312   \n",
       "666  0.288302  0.461012  0.594304  0.475335  0.623201  0.292493  0.684037   \n",
       "\n",
       "         s_ax      s_ay      s_az      s_gx      s_gy      s_gz  \n",
       "0    0.455423  0.575914  0.232102  0.460580  0.431826  0.430437  \n",
       "1    0.364310  0.411175  0.394961  0.554811  0.348901  0.465285  \n",
       "2    0.340974  0.480951  0.343527  0.561150  0.363928  0.459731  \n",
       "3    0.345722  0.485753  0.346651  0.577093  0.383714  0.462460  \n",
       "4    0.344122  0.494394  0.350339  0.588384  0.385956  0.460253  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "662  0.406464  0.499325  0.326526  0.577877  0.317245  0.613766  \n",
       "663  0.450686  0.491127  0.321414  0.512870  0.357280  0.559603  \n",
       "664  0.453627  0.502353  0.343935  0.497141  0.384295  0.548485  \n",
       "665  0.446183  0.488546  0.391107  0.507917  0.359130  0.576134  \n",
       "666  0.481283  0.640287  0.504056  0.630871  0.278268  0.720917  \n",
       "\n",
       "[667 rows x 13 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "19abe09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: Adding interaction features\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# # Create polynomial features up to degree 2\n",
    "# poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# X = poly_features.fit_transform(X)\n",
    "\n",
    "# # Continue with model training using X_poly as the input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "682f220c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>ax</th>\n",
       "      <th>ay</th>\n",
       "      <th>az</th>\n",
       "      <th>gx</th>\n",
       "      <th>gy</th>\n",
       "      <th>gz</th>\n",
       "      <th>s_ax</th>\n",
       "      <th>s_ay</th>\n",
       "      <th>s_az</th>\n",
       "      <th>s_gx</th>\n",
       "      <th>s_gy</th>\n",
       "      <th>s_gz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.455423</td>\n",
       "      <td>0.575914</td>\n",
       "      <td>0.232102</td>\n",
       "      <td>0.460580</td>\n",
       "      <td>0.431826</td>\n",
       "      <td>0.430437</td>\n",
       "      <td>0.455423</td>\n",
       "      <td>0.575914</td>\n",
       "      <td>0.232102</td>\n",
       "      <td>0.460580</td>\n",
       "      <td>0.431826</td>\n",
       "      <td>0.430437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364310</td>\n",
       "      <td>0.411175</td>\n",
       "      <td>0.394961</td>\n",
       "      <td>0.554811</td>\n",
       "      <td>0.348901</td>\n",
       "      <td>0.465285</td>\n",
       "      <td>0.364310</td>\n",
       "      <td>0.411175</td>\n",
       "      <td>0.394961</td>\n",
       "      <td>0.554811</td>\n",
       "      <td>0.348901</td>\n",
       "      <td>0.465285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340974</td>\n",
       "      <td>0.480951</td>\n",
       "      <td>0.343527</td>\n",
       "      <td>0.561150</td>\n",
       "      <td>0.363928</td>\n",
       "      <td>0.459731</td>\n",
       "      <td>0.340974</td>\n",
       "      <td>0.480951</td>\n",
       "      <td>0.343527</td>\n",
       "      <td>0.561150</td>\n",
       "      <td>0.363928</td>\n",
       "      <td>0.459731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.085387</td>\n",
       "      <td>0.345304</td>\n",
       "      <td>0.477760</td>\n",
       "      <td>0.344462</td>\n",
       "      <td>0.577049</td>\n",
       "      <td>0.383532</td>\n",
       "      <td>0.462244</td>\n",
       "      <td>0.345722</td>\n",
       "      <td>0.485753</td>\n",
       "      <td>0.346651</td>\n",
       "      <td>0.577093</td>\n",
       "      <td>0.383714</td>\n",
       "      <td>0.462460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.203693</td>\n",
       "      <td>0.343349</td>\n",
       "      <td>0.475577</td>\n",
       "      <td>0.345050</td>\n",
       "      <td>0.587119</td>\n",
       "      <td>0.385317</td>\n",
       "      <td>0.459996</td>\n",
       "      <td>0.344122</td>\n",
       "      <td>0.494394</td>\n",
       "      <td>0.350339</td>\n",
       "      <td>0.588384</td>\n",
       "      <td>0.385956</td>\n",
       "      <td>0.460253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>0.397348</td>\n",
       "      <td>0.393656</td>\n",
       "      <td>0.464997</td>\n",
       "      <td>0.321499</td>\n",
       "      <td>0.577558</td>\n",
       "      <td>0.328714</td>\n",
       "      <td>0.585150</td>\n",
       "      <td>0.406464</td>\n",
       "      <td>0.499325</td>\n",
       "      <td>0.326526</td>\n",
       "      <td>0.577877</td>\n",
       "      <td>0.317245</td>\n",
       "      <td>0.613766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>0.347805</td>\n",
       "      <td>0.431946</td>\n",
       "      <td>0.461723</td>\n",
       "      <td>0.317751</td>\n",
       "      <td>0.523266</td>\n",
       "      <td>0.360975</td>\n",
       "      <td>0.542869</td>\n",
       "      <td>0.450686</td>\n",
       "      <td>0.491127</td>\n",
       "      <td>0.321414</td>\n",
       "      <td>0.512870</td>\n",
       "      <td>0.357280</td>\n",
       "      <td>0.559603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>0.358755</td>\n",
       "      <td>0.433901</td>\n",
       "      <td>0.470286</td>\n",
       "      <td>0.336378</td>\n",
       "      <td>0.510462</td>\n",
       "      <td>0.383532</td>\n",
       "      <td>0.533187</td>\n",
       "      <td>0.453627</td>\n",
       "      <td>0.502353</td>\n",
       "      <td>0.343935</td>\n",
       "      <td>0.497141</td>\n",
       "      <td>0.384295</td>\n",
       "      <td>0.548485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>0.377549</td>\n",
       "      <td>0.426915</td>\n",
       "      <td>0.457523</td>\n",
       "      <td>0.374927</td>\n",
       "      <td>0.519917</td>\n",
       "      <td>0.362760</td>\n",
       "      <td>0.555312</td>\n",
       "      <td>0.446183</td>\n",
       "      <td>0.488546</td>\n",
       "      <td>0.391107</td>\n",
       "      <td>0.507917</td>\n",
       "      <td>0.359130</td>\n",
       "      <td>0.576134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>0.288302</td>\n",
       "      <td>0.461012</td>\n",
       "      <td>0.594304</td>\n",
       "      <td>0.475335</td>\n",
       "      <td>0.623201</td>\n",
       "      <td>0.292493</td>\n",
       "      <td>0.684037</td>\n",
       "      <td>0.481283</td>\n",
       "      <td>0.640287</td>\n",
       "      <td>0.504056</td>\n",
       "      <td>0.630871</td>\n",
       "      <td>0.278268</td>\n",
       "      <td>0.720917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>667 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            s        ax        ay        az        gx        gy        gz  \\\n",
       "0    0.000000  0.455423  0.575914  0.232102  0.460580  0.431826  0.430437   \n",
       "1    0.000000  0.364310  0.411175  0.394961  0.554811  0.348901  0.465285   \n",
       "2    0.000000  0.340974  0.480951  0.343527  0.561150  0.363928  0.459731   \n",
       "3    0.085387  0.345304  0.477760  0.344462  0.577049  0.383532  0.462244   \n",
       "4    0.203693  0.343349  0.475577  0.345050  0.587119  0.385317  0.459996   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "662  0.397348  0.393656  0.464997  0.321499  0.577558  0.328714  0.585150   \n",
       "663  0.347805  0.431946  0.461723  0.317751  0.523266  0.360975  0.542869   \n",
       "664  0.358755  0.433901  0.470286  0.336378  0.510462  0.383532  0.533187   \n",
       "665  0.377549  0.426915  0.457523  0.374927  0.519917  0.362760  0.555312   \n",
       "666  0.288302  0.461012  0.594304  0.475335  0.623201  0.292493  0.684037   \n",
       "\n",
       "         s_ax      s_ay      s_az      s_gx      s_gy      s_gz  \n",
       "0    0.455423  0.575914  0.232102  0.460580  0.431826  0.430437  \n",
       "1    0.364310  0.411175  0.394961  0.554811  0.348901  0.465285  \n",
       "2    0.340974  0.480951  0.343527  0.561150  0.363928  0.459731  \n",
       "3    0.345722  0.485753  0.346651  0.577093  0.383714  0.462460  \n",
       "4    0.344122  0.494394  0.350339  0.588384  0.385956  0.460253  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "662  0.406464  0.499325  0.326526  0.577877  0.317245  0.613766  \n",
       "663  0.450686  0.491127  0.321414  0.512870  0.357280  0.559603  \n",
       "664  0.453627  0.502353  0.343935  0.497141  0.384295  0.548485  \n",
       "665  0.446183  0.488546  0.391107  0.507917  0.359130  0.576134  \n",
       "666  0.481283  0.640287  0.504056  0.630871  0.278268  0.720917  \n",
       "\n",
       "[667 rows x 13 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "03b591e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(667, 13)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f49a83d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: Applying dimensionality reduction using Principal Component Analysis (PCA)\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # Specify the desired number of principal components\n",
    "# n_components = 10\n",
    "\n",
    "# # Create PCA object and fit-transform the data\n",
    "# pca = PCA(n_components=n_components)\n",
    "# X = pca.fit_transform(X)\n",
    "\n",
    "# # Continue with model training using X_pca as the input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "60e459cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>ax</th>\n",
       "      <th>ay</th>\n",
       "      <th>az</th>\n",
       "      <th>gx</th>\n",
       "      <th>gy</th>\n",
       "      <th>gz</th>\n",
       "      <th>s_ax</th>\n",
       "      <th>s_ay</th>\n",
       "      <th>s_az</th>\n",
       "      <th>s_gx</th>\n",
       "      <th>s_gy</th>\n",
       "      <th>s_gz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.455423</td>\n",
       "      <td>0.575914</td>\n",
       "      <td>0.232102</td>\n",
       "      <td>0.460580</td>\n",
       "      <td>0.431826</td>\n",
       "      <td>0.430437</td>\n",
       "      <td>0.455423</td>\n",
       "      <td>0.575914</td>\n",
       "      <td>0.232102</td>\n",
       "      <td>0.460580</td>\n",
       "      <td>0.431826</td>\n",
       "      <td>0.430437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364310</td>\n",
       "      <td>0.411175</td>\n",
       "      <td>0.394961</td>\n",
       "      <td>0.554811</td>\n",
       "      <td>0.348901</td>\n",
       "      <td>0.465285</td>\n",
       "      <td>0.364310</td>\n",
       "      <td>0.411175</td>\n",
       "      <td>0.394961</td>\n",
       "      <td>0.554811</td>\n",
       "      <td>0.348901</td>\n",
       "      <td>0.465285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340974</td>\n",
       "      <td>0.480951</td>\n",
       "      <td>0.343527</td>\n",
       "      <td>0.561150</td>\n",
       "      <td>0.363928</td>\n",
       "      <td>0.459731</td>\n",
       "      <td>0.340974</td>\n",
       "      <td>0.480951</td>\n",
       "      <td>0.343527</td>\n",
       "      <td>0.561150</td>\n",
       "      <td>0.363928</td>\n",
       "      <td>0.459731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.085387</td>\n",
       "      <td>0.345304</td>\n",
       "      <td>0.477760</td>\n",
       "      <td>0.344462</td>\n",
       "      <td>0.577049</td>\n",
       "      <td>0.383532</td>\n",
       "      <td>0.462244</td>\n",
       "      <td>0.345722</td>\n",
       "      <td>0.485753</td>\n",
       "      <td>0.346651</td>\n",
       "      <td>0.577093</td>\n",
       "      <td>0.383714</td>\n",
       "      <td>0.462460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.203693</td>\n",
       "      <td>0.343349</td>\n",
       "      <td>0.475577</td>\n",
       "      <td>0.345050</td>\n",
       "      <td>0.587119</td>\n",
       "      <td>0.385317</td>\n",
       "      <td>0.459996</td>\n",
       "      <td>0.344122</td>\n",
       "      <td>0.494394</td>\n",
       "      <td>0.350339</td>\n",
       "      <td>0.588384</td>\n",
       "      <td>0.385956</td>\n",
       "      <td>0.460253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>0.397348</td>\n",
       "      <td>0.393656</td>\n",
       "      <td>0.464997</td>\n",
       "      <td>0.321499</td>\n",
       "      <td>0.577558</td>\n",
       "      <td>0.328714</td>\n",
       "      <td>0.585150</td>\n",
       "      <td>0.406464</td>\n",
       "      <td>0.499325</td>\n",
       "      <td>0.326526</td>\n",
       "      <td>0.577877</td>\n",
       "      <td>0.317245</td>\n",
       "      <td>0.613766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>663</th>\n",
       "      <td>0.347805</td>\n",
       "      <td>0.431946</td>\n",
       "      <td>0.461723</td>\n",
       "      <td>0.317751</td>\n",
       "      <td>0.523266</td>\n",
       "      <td>0.360975</td>\n",
       "      <td>0.542869</td>\n",
       "      <td>0.450686</td>\n",
       "      <td>0.491127</td>\n",
       "      <td>0.321414</td>\n",
       "      <td>0.512870</td>\n",
       "      <td>0.357280</td>\n",
       "      <td>0.559603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>0.358755</td>\n",
       "      <td>0.433901</td>\n",
       "      <td>0.470286</td>\n",
       "      <td>0.336378</td>\n",
       "      <td>0.510462</td>\n",
       "      <td>0.383532</td>\n",
       "      <td>0.533187</td>\n",
       "      <td>0.453627</td>\n",
       "      <td>0.502353</td>\n",
       "      <td>0.343935</td>\n",
       "      <td>0.497141</td>\n",
       "      <td>0.384295</td>\n",
       "      <td>0.548485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>665</th>\n",
       "      <td>0.377549</td>\n",
       "      <td>0.426915</td>\n",
       "      <td>0.457523</td>\n",
       "      <td>0.374927</td>\n",
       "      <td>0.519917</td>\n",
       "      <td>0.362760</td>\n",
       "      <td>0.555312</td>\n",
       "      <td>0.446183</td>\n",
       "      <td>0.488546</td>\n",
       "      <td>0.391107</td>\n",
       "      <td>0.507917</td>\n",
       "      <td>0.359130</td>\n",
       "      <td>0.576134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>666</th>\n",
       "      <td>0.288302</td>\n",
       "      <td>0.461012</td>\n",
       "      <td>0.594304</td>\n",
       "      <td>0.475335</td>\n",
       "      <td>0.623201</td>\n",
       "      <td>0.292493</td>\n",
       "      <td>0.684037</td>\n",
       "      <td>0.481283</td>\n",
       "      <td>0.640287</td>\n",
       "      <td>0.504056</td>\n",
       "      <td>0.630871</td>\n",
       "      <td>0.278268</td>\n",
       "      <td>0.720917</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>667 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            s        ax        ay        az        gx        gy        gz  \\\n",
       "0    0.000000  0.455423  0.575914  0.232102  0.460580  0.431826  0.430437   \n",
       "1    0.000000  0.364310  0.411175  0.394961  0.554811  0.348901  0.465285   \n",
       "2    0.000000  0.340974  0.480951  0.343527  0.561150  0.363928  0.459731   \n",
       "3    0.085387  0.345304  0.477760  0.344462  0.577049  0.383532  0.462244   \n",
       "4    0.203693  0.343349  0.475577  0.345050  0.587119  0.385317  0.459996   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "662  0.397348  0.393656  0.464997  0.321499  0.577558  0.328714  0.585150   \n",
       "663  0.347805  0.431946  0.461723  0.317751  0.523266  0.360975  0.542869   \n",
       "664  0.358755  0.433901  0.470286  0.336378  0.510462  0.383532  0.533187   \n",
       "665  0.377549  0.426915  0.457523  0.374927  0.519917  0.362760  0.555312   \n",
       "666  0.288302  0.461012  0.594304  0.475335  0.623201  0.292493  0.684037   \n",
       "\n",
       "         s_ax      s_ay      s_az      s_gx      s_gy      s_gz  \n",
       "0    0.455423  0.575914  0.232102  0.460580  0.431826  0.430437  \n",
       "1    0.364310  0.411175  0.394961  0.554811  0.348901  0.465285  \n",
       "2    0.340974  0.480951  0.343527  0.561150  0.363928  0.459731  \n",
       "3    0.345722  0.485753  0.346651  0.577093  0.383714  0.462460  \n",
       "4    0.344122  0.494394  0.350339  0.588384  0.385956  0.460253  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "662  0.406464  0.499325  0.326526  0.577877  0.317245  0.613766  \n",
       "663  0.450686  0.491127  0.321414  0.512870  0.357280  0.559603  \n",
       "664  0.453627  0.502353  0.343935  0.497141  0.384295  0.548485  \n",
       "665  0.446183  0.488546  0.391107  0.507917  0.359130  0.576134  \n",
       "666  0.481283  0.640287  0.504056  0.630871  0.278268  0.720917  \n",
       "\n",
       "[667 rows x 13 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57226a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06775b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc49ad41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "684adea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "72626174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "86bde783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            ax        ay        az        gx        gy        gz         s  \\\n",
      "ax    1.000000  0.386610  0.383225 -0.143187 -0.446267  0.366076  0.169691   \n",
      "ay    0.386610  1.000000  0.277912 -0.050286 -0.569654  0.416820  0.101573   \n",
      "az    0.383225  0.277912  1.000000  0.254681 -0.387110  0.053436  0.134119   \n",
      "gx   -0.143187 -0.050286  0.254681  1.000000  0.171349 -0.317107 -0.133104   \n",
      "gy   -0.446267 -0.569654 -0.387110  0.171349  1.000000 -0.473614 -0.290286   \n",
      "gz    0.366076  0.416820  0.053436 -0.317107 -0.473614  1.000000  0.094540   \n",
      "s     0.169691  0.101573  0.134119 -0.133104 -0.290286  0.094540  1.000000   \n",
      "w     0.154014  0.123725  0.044574 -0.134006 -0.281166  0.241553  0.663280   \n",
      "s_ax  0.996628  0.380362  0.378735 -0.154082 -0.450124  0.378081  0.207241   \n",
      "s_ay  0.401094  0.958435  0.297988 -0.082727 -0.604412  0.425095  0.373624   \n",
      "s_az  0.386853  0.284934  0.991789  0.227420 -0.405960  0.074320  0.236719   \n",
      "s_gx -0.151228 -0.050065  0.236490  0.995565  0.170180 -0.308350 -0.152779   \n",
      "s_gy -0.447394 -0.568131 -0.389204  0.168589  0.997921 -0.480261 -0.317541   \n",
      "s_gz  0.381808  0.427736  0.073173 -0.314376 -0.484370  0.996196  0.140177   \n",
      "\n",
      "             w      s_ax      s_ay      s_az      s_gx      s_gy      s_gz  \n",
      "ax    0.154014  0.996628  0.401094  0.386853 -0.151228 -0.447394  0.381808  \n",
      "ay    0.123725  0.380362  0.958435  0.284934 -0.050065 -0.568131  0.427736  \n",
      "az    0.044574  0.378735  0.297988  0.991789  0.236490 -0.389204  0.073173  \n",
      "gx   -0.134006 -0.154082 -0.082727  0.227420  0.995565  0.168589 -0.314376  \n",
      "gy   -0.281166 -0.450124 -0.604412 -0.405960  0.170180  0.997921 -0.484370  \n",
      "gz    0.241553  0.378081  0.425095  0.074320 -0.308350 -0.480261  0.996196  \n",
      "s     0.663280  0.207241  0.373624  0.236719 -0.152779 -0.317541  0.140177  \n",
      "w     1.000000  0.177344  0.296588  0.113654 -0.140204 -0.292067  0.261012  \n",
      "s_ax  0.177344  1.000000  0.406345  0.386968 -0.163922 -0.453577  0.397990  \n",
      "s_ay  0.296588  0.406345  1.000000  0.334600 -0.087333 -0.612299  0.450117  \n",
      "s_az  0.113654  0.386968  0.334600  1.000000  0.208339 -0.412223  0.099986  \n",
      "s_gx -0.140204 -0.163922 -0.087333  0.208339  1.000000  0.167411 -0.306964  \n",
      "s_gy -0.292067 -0.453577 -0.612299 -0.412223  0.167411  1.000000 -0.494923  \n",
      "s_gz  0.261012  0.397990  0.450117  0.099986 -0.306964 -0.494923  1.000000  \n"
     ]
    }
   ],
   "source": [
    "corr_matrix = df.corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "0672af7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f164a8956d0>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.heatmap(corr_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf9b95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ede89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0cb6d31",
   "metadata": {},
   "source": [
    "## Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "19e31164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "118c9a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s</th>\n",
       "      <th>ax</th>\n",
       "      <th>ay</th>\n",
       "      <th>az</th>\n",
       "      <th>gx</th>\n",
       "      <th>gy</th>\n",
       "      <th>gz</th>\n",
       "      <th>s_ax</th>\n",
       "      <th>s_ay</th>\n",
       "      <th>s_az</th>\n",
       "      <th>s_gx</th>\n",
       "      <th>s_gy</th>\n",
       "      <th>s_gz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>0.428055</td>\n",
       "      <td>0.417413</td>\n",
       "      <td>0.492369</td>\n",
       "      <td>0.362039</td>\n",
       "      <td>0.519005</td>\n",
       "      <td>0.306546</td>\n",
       "      <td>0.630032</td>\n",
       "      <td>0.436960</td>\n",
       "      <td>0.535975</td>\n",
       "      <td>0.377265</td>\n",
       "      <td>0.505179</td>\n",
       "      <td>0.288827</td>\n",
       "      <td>0.671721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>0.245578</td>\n",
       "      <td>0.344886</td>\n",
       "      <td>0.485400</td>\n",
       "      <td>0.333801</td>\n",
       "      <td>0.582074</td>\n",
       "      <td>0.381552</td>\n",
       "      <td>0.464036</td>\n",
       "      <td>0.346031</td>\n",
       "      <td>0.509449</td>\n",
       "      <td>0.338616</td>\n",
       "      <td>0.582898</td>\n",
       "      <td>0.381800</td>\n",
       "      <td>0.464907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.396449</td>\n",
       "      <td>0.326456</td>\n",
       "      <td>0.321734</td>\n",
       "      <td>0.646711</td>\n",
       "      <td>0.622602</td>\n",
       "      <td>0.573059</td>\n",
       "      <td>0.396449</td>\n",
       "      <td>0.326456</td>\n",
       "      <td>0.321734</td>\n",
       "      <td>0.646711</td>\n",
       "      <td>0.622602</td>\n",
       "      <td>0.573059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>0.129359</td>\n",
       "      <td>0.501119</td>\n",
       "      <td>0.515460</td>\n",
       "      <td>0.287757</td>\n",
       "      <td>0.532446</td>\n",
       "      <td>0.379183</td>\n",
       "      <td>0.764133</td>\n",
       "      <td>0.513148</td>\n",
       "      <td>0.530326</td>\n",
       "      <td>0.286926</td>\n",
       "      <td>0.529250</td>\n",
       "      <td>0.379140</td>\n",
       "      <td>0.786538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>0.322889</td>\n",
       "      <td>0.350756</td>\n",
       "      <td>0.484896</td>\n",
       "      <td>0.365554</td>\n",
       "      <td>0.617625</td>\n",
       "      <td>0.318815</td>\n",
       "      <td>0.548687</td>\n",
       "      <td>0.353332</td>\n",
       "      <td>0.516424</td>\n",
       "      <td>0.377681</td>\n",
       "      <td>0.625199</td>\n",
       "      <td>0.307688</td>\n",
       "      <td>0.565284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345444</td>\n",
       "      <td>0.480195</td>\n",
       "      <td>0.336731</td>\n",
       "      <td>0.585826</td>\n",
       "      <td>0.369089</td>\n",
       "      <td>0.460804</td>\n",
       "      <td>0.345444</td>\n",
       "      <td>0.480195</td>\n",
       "      <td>0.336731</td>\n",
       "      <td>0.585826</td>\n",
       "      <td>0.369089</td>\n",
       "      <td>0.460804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.445587</td>\n",
       "      <td>0.353549</td>\n",
       "      <td>0.480363</td>\n",
       "      <td>0.359930</td>\n",
       "      <td>0.556486</td>\n",
       "      <td>0.363150</td>\n",
       "      <td>0.506949</td>\n",
       "      <td>0.357808</td>\n",
       "      <td>0.522730</td>\n",
       "      <td>0.375248</td>\n",
       "      <td>0.551535</td>\n",
       "      <td>0.358963</td>\n",
       "      <td>0.519339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>0.271132</td>\n",
       "      <td>0.464647</td>\n",
       "      <td>0.724028</td>\n",
       "      <td>0.456825</td>\n",
       "      <td>0.520171</td>\n",
       "      <td>0.245010</td>\n",
       "      <td>0.742449</td>\n",
       "      <td>0.484268</td>\n",
       "      <td>0.787158</td>\n",
       "      <td>0.480999</td>\n",
       "      <td>0.511593</td>\n",
       "      <td>0.224354</td>\n",
       "      <td>0.786086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322946</td>\n",
       "      <td>0.537710</td>\n",
       "      <td>0.323961</td>\n",
       "      <td>0.603676</td>\n",
       "      <td>0.372627</td>\n",
       "      <td>0.441015</td>\n",
       "      <td>0.322946</td>\n",
       "      <td>0.537710</td>\n",
       "      <td>0.323961</td>\n",
       "      <td>0.603676</td>\n",
       "      <td>0.372627</td>\n",
       "      <td>0.441015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.503912</td>\n",
       "      <td>0.461850</td>\n",
       "      <td>0.436532</td>\n",
       "      <td>0.356532</td>\n",
       "      <td>0.522630</td>\n",
       "      <td>0.373633</td>\n",
       "      <td>0.524505</td>\n",
       "      <td>0.497521</td>\n",
       "      <td>0.471958</td>\n",
       "      <td>0.372888</td>\n",
       "      <td>0.507387</td>\n",
       "      <td>0.371885</td>\n",
       "      <td>0.543519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>533 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            s        ax        ay        az        gx        gy        gz  \\\n",
       "157  0.428055  0.417413  0.492369  0.362039  0.519005  0.306546  0.630032   \n",
       "447  0.245578  0.344886  0.485400  0.333801  0.582074  0.381552  0.464036   \n",
       "551  0.000000  0.396449  0.326456  0.321734  0.646711  0.622602  0.573059   \n",
       "475  0.129359  0.501119  0.515460  0.287757  0.532446  0.379183  0.764133   \n",
       "588  0.322889  0.350756  0.484896  0.365554  0.617625  0.318815  0.548687   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "255  0.000000  0.345444  0.480195  0.336731  0.585826  0.369089  0.460804   \n",
       "197  0.445587  0.353549  0.480363  0.359930  0.556486  0.363150  0.506949   \n",
       "638  0.271132  0.464647  0.724028  0.456825  0.520171  0.245010  0.742449   \n",
       "446  0.000000  0.322946  0.537710  0.323961  0.603676  0.372627  0.441015   \n",
       "184  0.503912  0.461850  0.436532  0.356532  0.522630  0.373633  0.524505   \n",
       "\n",
       "         s_ax      s_ay      s_az      s_gx      s_gy      s_gz  \n",
       "157  0.436960  0.535975  0.377265  0.505179  0.288827  0.671721  \n",
       "447  0.346031  0.509449  0.338616  0.582898  0.381800  0.464907  \n",
       "551  0.396449  0.326456  0.321734  0.646711  0.622602  0.573059  \n",
       "475  0.513148  0.530326  0.286926  0.529250  0.379140  0.786538  \n",
       "588  0.353332  0.516424  0.377681  0.625199  0.307688  0.565284  \n",
       "..        ...       ...       ...       ...       ...       ...  \n",
       "255  0.345444  0.480195  0.336731  0.585826  0.369089  0.460804  \n",
       "197  0.357808  0.522730  0.375248  0.551535  0.358963  0.519339  \n",
       "638  0.484268  0.787158  0.480999  0.511593  0.224354  0.786086  \n",
       "446  0.322946  0.537710  0.323961  0.603676  0.372627  0.441015  \n",
       "184  0.497521  0.471958  0.372888  0.507387  0.371885  0.543519  \n",
       "\n",
       "[533 rows x 13 columns]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e2d03728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regression:\n",
      "Mean Squared Error: 0.019020129687190523\n",
      "R-squared Score: 0.7359066232119938\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Random Forest Regression\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "rf_mse = mean_squared_error(y_test, rf_predictions)\n",
    "rf_r2 = r2_score(y_test, rf_predictions)\n",
    "print(\"Random Forest Regression:\")\n",
    "print(\"Mean Squared Error:\", rf_mse)\n",
    "print(\"R-squared Score:\", rf_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "206c7379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter Plot\n",
    "plt.scatter(y_test, rf_predictions)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Scatter Plot: Actual vs Predicted')\n",
    "plt.show()\n",
    "\n",
    "# Residual Plot\n",
    "residuals = y_test - rf_predictions\n",
    "plt.scatter(rf_predictions, residuals)\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.axhline(y=0, color='r', linestyle='--')  # Add a horizontal line at y=0\n",
    "plt.title('Residual Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d3686108",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484    0.957990\n",
       "177    0.268387\n",
       "213    0.667498\n",
       "245    0.656730\n",
       "215    0.662361\n",
       "         ...   \n",
       "593    0.657372\n",
       "635    0.689133\n",
       "134    0.740529\n",
       "342    0.012349\n",
       "128    0.676043\n",
       "Name: w, Length: 134, dtype: float64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "243bb8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85899802, 0.63573426, 0.70265177, 0.67637688, 0.6800815 ,\n",
       "       0.77667226, 0.01344307, 0.56395233, 0.67233663, 0.62314028,\n",
       "       0.27944332, 0.65192467, 0.16752754, 0.07937565, 0.75921413,\n",
       "       0.56542406, 0.49551346, 0.81824327, 0.62789676, 0.72742159,\n",
       "       0.58229612, 0.63858113, 0.07212966, 0.38899457, 0.69483626,\n",
       "       0.27870042, 0.59506668, 0.66346481, 0.61262954, 0.00714917,\n",
       "       0.5598733 , 0.34052976, 0.68874907, 0.67392047, 0.62598271,\n",
       "       0.68878217, 0.07164954, 0.93384663, 0.43887948, 0.68346579,\n",
       "       0.25745073, 0.68049765, 0.15771845, 0.09955643, 0.56911188,\n",
       "       0.05389207, 0.64214868, 0.56805927, 0.03735564, 0.76754507,\n",
       "       0.65704989, 0.06562583, 0.37942208, 0.56970783, 0.68357743,\n",
       "       0.64005063, 0.41408447, 0.080163  , 0.53658088, 0.65361373,\n",
       "       0.84935144, 0.17202494, 0.1891225 , 0.59959422, 0.37557026,\n",
       "       0.68067597, 0.28129686, 0.75322648, 0.76772734, 0.22653421,\n",
       "       0.54746308, 0.64540529, 0.69182514, 0.61294962, 0.66085824,\n",
       "       0.0353023 , 0.66594171, 0.64518918, 0.69339145, 0.71486169,\n",
       "       0.69167301, 0.64522376, 0.92044036, 0.71857471, 0.56065794,\n",
       "       0.57801976, 0.65128254, 0.64960459, 0.31135836, 0.69106372,\n",
       "       0.83517807, 0.61466733, 0.50047888, 0.67143369, 0.49569276,\n",
       "       0.3071467 , 0.13509854, 0.66783823, 0.61561546, 0.59621709,\n",
       "       0.63498666, 0.11839071, 0.84322944, 0.65128254, 0.40360632,\n",
       "       0.08255298, 0.73963028, 0.88880835, 0.68357743, 0.41048901,\n",
       "       0.56049321, 0.7661998 , 0.49670289, 0.09249494, 0.71786268,\n",
       "       0.6725468 , 0.4849427 , 0.6964364 , 0.73584218, 0.70452309,\n",
       "       0.52506792, 0.67252507, 0.02620795, 0.67392047, 0.48793801,\n",
       "       0.03304248, 0.70545122, 0.50626253, 0.472837  , 0.6474737 ,\n",
       "       0.6661793 , 0.66737911, 0.02222796, 0.64036972])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4dc576c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Regression:\n",
      "Mean Squared Error: 0.03622671304272525\n",
      "R-squared Score: 0.49699423007474774\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate Support Vector Regression\n",
    "svr_model = SVR()\n",
    "svr_model.fit(X_train, y_train)\n",
    "svr_predictions = svr_model.predict(X_test)\n",
    "svr_mse = mean_squared_error(y_test, svr_predictions)\n",
    "svr_r2 = r2_score(y_test, svr_predictions)\n",
    "print(\"Support Vector Regression:\")\n",
    "print(\"Mean Squared Error:\", svr_mse)\n",
    "print(\"R-squared Score:\", svr_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9546000d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484    0.957990\n",
       "177    0.268387\n",
       "213    0.667498\n",
       "245    0.656730\n",
       "215    0.662361\n",
       "         ...   \n",
       "593    0.657372\n",
       "635    0.689133\n",
       "134    0.740529\n",
       "342    0.012349\n",
       "128    0.676043\n",
       "Name: w, Length: 134, dtype: float64"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4d0bfa12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.71741165,  0.74053059,  0.76985296,  0.68963712,  0.76285158,\n",
       "        0.74222579,  0.08357789,  0.56810217,  0.63190778,  0.58102501,\n",
       "        0.23144146,  0.72383167,  0.25688556,  0.37574651,  0.77870379,\n",
       "        0.45620412,  0.44576175,  0.83207236,  0.71439374,  0.76791962,\n",
       "        0.54156635,  0.54655766, -0.01228349,  0.16657173,  0.61177022,\n",
       "        0.27840638,  0.53877257,  0.63460993,  0.60843652,  0.14487377,\n",
       "        0.54824951,  0.73757721,  0.62663832,  0.63593608,  0.63324391,\n",
       "        0.65645444,  0.22366556,  0.85813204,  0.26269811,  0.74937756,\n",
       "        0.26445293,  0.61463932,  0.05547892,  0.17907058,  0.43808659,\n",
       "        0.05948686,  0.72837336,  0.41039093, -0.01839958,  0.70774783,\n",
       "        0.35893845,  0.10484975,  0.26315857,  0.60225726,  0.51551252,\n",
       "        0.64213666,  0.60581094,  0.16888147,  0.78426416,  0.61609844,\n",
       "        0.69212776,  0.2749095 ,  0.23622179,  0.65807451,  0.53529953,\n",
       "        0.61419823,  0.25031581,  0.66730219,  0.81426593,  0.23043594,\n",
       "        0.52388837,  0.59570846,  0.66017348,  0.6315915 ,  0.78618971,\n",
       "        0.08894382,  0.63898261,  0.47630856,  0.68291211,  0.71242586,\n",
       "        0.67351561,  0.72986827,  0.89847037,  0.74259397,  0.64080572,\n",
       "        0.70562887,  0.7013565 ,  0.64042475,  0.2877848 ,  0.63279862,\n",
       "        0.70879046,  0.63770516,  0.49892421,  0.61211321,  0.27127975,\n",
       "        0.31266529,  0.22946619,  0.59845702,  0.45289437,  0.49752529,\n",
       "        0.68265373,  0.22147013,  0.73034872,  0.7013565 ,  0.31707068,\n",
       "        0.0603702 ,  0.73325235,  0.8054    ,  0.51551252,  0.48544424,\n",
       "        0.62339951,  0.71918184, -0.04388344,  0.20054364,  0.71599959,\n",
       "        0.61375282,  0.64129657,  0.61036472,  0.73510093,  0.61824123,\n",
       "        0.76891471,  0.78879194,  0.11511   ,  0.63593608,  0.53938582,\n",
       "        0.15012309,  0.78356639,  0.55167415,  0.35407   ,  0.61234775,\n",
       "        0.58232855,  0.69956   ,  0.11215088,  0.72486373])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svr_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b913a28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Layer Perceptron Regression:\n",
      "Mean Squared Error: 0.030746854599303427\n",
      "R-squared Score: 0.5730817407513009\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate MLP Regression\n",
    "mlp_model = MLPRegressor(hidden_layer_sizes=(500, 500), activation='relu', solver='adam', random_state=42)\n",
    "mlp_model.fit(X_train, y_train)\n",
    "mlp_predictions = mlp_model.predict(X_test)\n",
    "mlp_mse = mean_squared_error(y_test, mlp_predictions)\n",
    "mlp_r2 = r2_score(y_test, mlp_predictions)\n",
    "print(\"Multi-Layer Perceptron Regression:\")\n",
    "print(\"Mean Squared Error:\", mlp_mse)\n",
    "print(\"R-squared Score:\", mlp_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7660bd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484    0.957990\n",
       "177    0.268387\n",
       "213    0.667498\n",
       "245    0.656730\n",
       "215    0.662361\n",
       "         ...   \n",
       "593    0.657372\n",
       "635    0.689133\n",
       "134    0.740529\n",
       "342    0.012349\n",
       "128    0.676043\n",
       "Name: w, Length: 134, dtype: float64"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "e2ea73cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.74826937,  0.69245843,  0.70911157,  0.67250881,  0.70427825,\n",
       "        0.76123843, -0.05157858,  0.62246881,  0.64040321,  0.5928098 ,\n",
       "        0.32746233,  0.6800782 ,  0.17853414,  0.31971273,  0.70964687,\n",
       "        0.56783635,  0.64857387,  0.82275236,  0.62230744,  0.73850677,\n",
       "        0.59904108,  0.59768718,  0.10569031,  0.38228989,  0.63569969,\n",
       "        0.26255827,  0.59840694,  0.65741493,  0.63405346,  0.23889723,\n",
       "        0.59892707,  0.69218659,  0.66881137,  0.65294945,  0.61572602,\n",
       "        0.67206285,  0.22597544,  0.91065897,  0.30035983,  0.72025457,\n",
       "        0.23871936,  0.64702425,  0.06585632,  0.24220582,  0.46022031,\n",
       "        0.04771928,  0.69513386,  0.4019551 ,  0.23235406,  0.7582712 ,\n",
       "        0.35627254,  0.08212431,  0.24172956,  0.62805893,  0.53489836,\n",
       "        0.65115288,  0.68272784,  0.13284367,  0.64259424,  0.63060751,\n",
       "        0.74004257,  0.25070655,  0.23528516,  0.69384046,  0.58891728,\n",
       "        0.64236241,  0.23947492,  0.64539357,  0.786953  ,  0.22300011,\n",
       "        0.53380925,  0.62429018,  0.66938778,  0.62292569,  0.72049501,\n",
       "       -0.01606737,  0.65749616,  0.56753245,  0.65016355,  0.76372838,\n",
       "        0.67970059,  0.6963224 ,  0.87480149,  0.70442305,  0.68333202,\n",
       "        0.69549834,  0.57993881,  0.68799351,  0.27606432,  0.65138743,\n",
       "        0.68559549,  0.64608398,  0.49326507,  0.65307767,  0.26068298,\n",
       "        0.29946175,  0.227025  ,  0.64344617,  0.47628834,  0.53883161,\n",
       "        0.64698104,  0.26646835,  0.68700914,  0.57993881,  0.46267829,\n",
       "        0.03125144,  0.68320111,  0.75606241,  0.53489836,  0.48437472,\n",
       "        0.6725585 ,  0.73779051,  0.23292488,  0.16972963,  0.67355062,\n",
       "        0.68251511,  0.62795772,  0.65026441,  0.74486485,  0.6502969 ,\n",
       "        0.70544719,  0.6122559 ,  0.01190974,  0.65294945,  0.46810454,\n",
       "        0.18152137,  0.82701198,  0.60887075,  0.36847487,  0.63768115,\n",
       "        0.63032944,  0.7260564 , -0.01769763,  0.74035051])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc9de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a70fe08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "375767e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "17/17 [==============================] - 9s 6ms/step - loss: 0.0854\n",
      "Epoch 2/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0701\n",
      "Epoch 3/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0630\n",
      "Epoch 4/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0583\n",
      "Epoch 5/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0525\n",
      "Epoch 6/100\n",
      "17/17 [==============================] - 0s 11ms/step - loss: 0.0492\n",
      "Epoch 7/100\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.0458\n",
      "Epoch 8/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0439\n",
      "Epoch 9/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0418\n",
      "Epoch 10/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0406\n",
      "Epoch 11/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0401\n",
      "Epoch 12/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0392\n",
      "Epoch 13/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0385\n",
      "Epoch 14/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0374\n",
      "Epoch 15/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0371\n",
      "Epoch 16/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0366\n",
      "Epoch 17/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0358\n",
      "Epoch 18/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0358\n",
      "Epoch 19/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0354\n",
      "Epoch 20/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0351\n",
      "Epoch 21/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0343\n",
      "Epoch 22/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0341\n",
      "Epoch 23/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0350\n",
      "Epoch 24/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0342\n",
      "Epoch 25/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0349\n",
      "Epoch 26/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0359\n",
      "Epoch 27/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0378\n",
      "Epoch 28/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0340\n",
      "Epoch 29/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0332\n",
      "Epoch 30/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0327\n",
      "Epoch 31/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0329\n",
      "Epoch 32/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0336\n",
      "Epoch 33/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0323\n",
      "Epoch 34/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0326\n",
      "Epoch 35/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0322\n",
      "Epoch 36/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0327\n",
      "Epoch 37/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0324\n",
      "Epoch 38/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0321\n",
      "Epoch 39/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0321\n",
      "Epoch 40/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0316\n",
      "Epoch 41/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0312\n",
      "Epoch 42/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0316\n",
      "Epoch 43/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0316\n",
      "Epoch 44/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0323\n",
      "Epoch 45/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0317\n",
      "Epoch 46/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0323\n",
      "Epoch 47/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0314\n",
      "Epoch 48/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0314\n",
      "Epoch 49/100\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0313\n",
      "Epoch 50/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0319\n",
      "Epoch 51/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0315\n",
      "Epoch 52/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0314\n",
      "Epoch 53/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0308\n",
      "Epoch 54/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0320\n",
      "Epoch 55/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0307\n",
      "Epoch 56/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0310\n",
      "Epoch 57/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0315\n",
      "Epoch 58/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0313\n",
      "Epoch 59/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0306\n",
      "Epoch 60/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0306\n",
      "Epoch 61/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0303\n",
      "Epoch 62/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0304\n",
      "Epoch 63/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0303\n",
      "Epoch 64/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0305\n",
      "Epoch 65/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0305\n",
      "Epoch 66/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0304\n",
      "Epoch 67/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0301\n",
      "Epoch 68/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0300\n",
      "Epoch 69/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0302\n",
      "Epoch 70/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0319\n",
      "Epoch 71/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0301\n",
      "Epoch 72/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0303\n",
      "Epoch 73/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0304\n",
      "Epoch 74/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0306\n",
      "Epoch 75/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0300\n",
      "Epoch 76/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0314\n",
      "Epoch 77/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0305\n",
      "Epoch 78/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0304\n",
      "Epoch 79/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0297\n",
      "Epoch 80/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0298\n",
      "Epoch 81/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0298\n",
      "Epoch 82/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0298\n",
      "Epoch 83/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0299\n",
      "Epoch 84/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0300\n",
      "Epoch 85/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0302\n",
      "Epoch 86/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0297\n",
      "Epoch 87/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0293\n",
      "Epoch 88/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0296\n",
      "Epoch 89/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0293\n",
      "Epoch 90/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0292\n",
      "Epoch 91/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0296\n",
      "Epoch 92/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0292\n",
      "Epoch 93/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0292\n",
      "Epoch 94/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0291\n",
      "Epoch 95/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0292\n",
      "Epoch 96/100\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0298\n",
      "Epoch 97/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0291\n",
      "Epoch 98/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0294\n",
      "Epoch 99/100\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0294\n",
      "Epoch 100/100\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0292\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.0321\n",
      "5/5 [==============================] - 0s 3ms/step\n",
      "Loss: 0.032064855098724365\n",
      "Predictions: [[0.7679982 ]\n",
      " [0.677173  ]\n",
      " [0.70153034]\n",
      " [0.6774674 ]\n",
      " [0.6876216 ]\n",
      " [0.76524526]\n",
      " [0.01337586]\n",
      " [0.6448797 ]\n",
      " [0.6435226 ]\n",
      " [0.5928492 ]\n",
      " [0.30992803]\n",
      " [0.67547226]\n",
      " [0.17210157]\n",
      " [0.36288744]\n",
      " [0.68846124]\n",
      " [0.60357076]\n",
      " [0.6692339 ]\n",
      " [0.8039876 ]\n",
      " [0.5857985 ]\n",
      " [0.7609435 ]\n",
      " [0.5876186 ]\n",
      " [0.56964344]\n",
      " [0.3140754 ]\n",
      " [0.42324263]\n",
      " [0.68201965]\n",
      " [0.30810097]\n",
      " [0.63984495]\n",
      " [0.661334  ]\n",
      " [0.60313237]\n",
      " [0.3236662 ]\n",
      " [0.61460716]\n",
      " [0.68102753]\n",
      " [0.7147653 ]\n",
      " [0.6474559 ]\n",
      " [0.62683487]\n",
      " [0.6872463 ]\n",
      " [0.2912792 ]\n",
      " [0.8704189 ]\n",
      " [0.38041124]\n",
      " [0.7365416 ]\n",
      " [0.27406594]\n",
      " [0.65209043]\n",
      " [0.09768277]\n",
      " [0.29644004]\n",
      " [0.46599638]\n",
      " [0.15359412]\n",
      " [0.6842768 ]\n",
      " [0.40008333]\n",
      " [0.31297746]\n",
      " [0.75790846]\n",
      " [0.38246697]\n",
      " [0.1390213 ]\n",
      " [0.28083676]\n",
      " [0.6615812 ]\n",
      " [0.5631338 ]\n",
      " [0.63113123]\n",
      " [0.6408218 ]\n",
      " [0.20866501]\n",
      " [0.6521775 ]\n",
      " [0.6227151 ]\n",
      " [0.75633633]\n",
      " [0.2335996 ]\n",
      " [0.28653222]\n",
      " [0.7116013 ]\n",
      " [0.6339119 ]\n",
      " [0.630546  ]\n",
      " [0.27904367]\n",
      " [0.6650034 ]\n",
      " [0.81571007]\n",
      " [0.27759612]\n",
      " [0.5596135 ]\n",
      " [0.6468662 ]\n",
      " [0.6839453 ]\n",
      " [0.647892  ]\n",
      " [0.69850075]\n",
      " [0.04556268]\n",
      " [0.6409528 ]\n",
      " [0.622482  ]\n",
      " [0.65861905]\n",
      " [0.77414036]\n",
      " [0.69822407]\n",
      " [0.72440255]\n",
      " [0.830871  ]\n",
      " [0.6954141 ]\n",
      " [0.7086904 ]\n",
      " [0.71375036]\n",
      " [0.63281983]\n",
      " [0.6934013 ]\n",
      " [0.30072227]\n",
      " [0.65050983]\n",
      " [0.6814393 ]\n",
      " [0.63464594]\n",
      " [0.46527737]\n",
      " [0.67357457]\n",
      " [0.3001266 ]\n",
      " [0.33279696]\n",
      " [0.28279656]\n",
      " [0.63238496]\n",
      " [0.39975414]\n",
      " [0.55494654]\n",
      " [0.7215277 ]\n",
      " [0.3154207 ]\n",
      " [0.6855974 ]\n",
      " [0.63281983]\n",
      " [0.52675563]\n",
      " [0.09054955]\n",
      " [0.6732724 ]\n",
      " [0.80612916]\n",
      " [0.5631338 ]\n",
      " [0.555618  ]\n",
      " [0.6807194 ]\n",
      " [0.7490549 ]\n",
      " [0.30217615]\n",
      " [0.21045236]\n",
      " [0.67371446]\n",
      " [0.6853436 ]\n",
      " [0.54508704]\n",
      " [0.66726565]\n",
      " [0.7597158 ]\n",
      " [0.6634981 ]\n",
      " [0.6973966 ]\n",
      " [0.59131026]\n",
      " [0.07333474]\n",
      " [0.6474559 ]\n",
      " [0.39830494]\n",
      " [0.2509318 ]\n",
      " [0.8173543 ]\n",
      " [0.64555335]\n",
      " [0.38981655]\n",
      " [0.6235147 ]\n",
      " [0.6759087 ]\n",
      " [0.7556411 ]\n",
      " [0.040245  ]\n",
      " [0.7528902 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the model\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(16, input_shape=(13,), activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_model()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Print the loss and predictions\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b66dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9dc72d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2fe9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d21fec87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=10, min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "cdce2faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# # Create polynomial features up to degree 2\n",
    "# poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# X_poly = poly_features.fit_transform(X)\n",
    "\n",
    "# # Continue with model training using X_poly as the input features\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# from sklearn.metrics import make_scorer\n",
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# early_stop = EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "# # Define the model\n",
    "# def create_model(optimizer, hidden_layers, neurons):\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(tf.keras.layers.Dense(neurons, input_shape=(X_poly.shape[1],), activation='relu')) # Update input shape\n",
    "#     for i in range(hidden_layers):\n",
    "#         model.add(tf.keras.layers.Dense(neurons, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dense(1))\n",
    "#     model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Define the parameters for the grid search\n",
    "# param_grid = {\n",
    "#     'optimizer': ['adam'],\n",
    "#     'hidden_layers': [3],\n",
    "#     'neurons': [16]\n",
    "# }\n",
    "\n",
    "# # Create the model\n",
    "# model = KerasRegressor(build_fn=create_model, epochs=1000)\n",
    "\n",
    "# # Perform the grid search\n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "# history = grid_search.fit(X_poly, y_train, callbacks=[reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "4aec4416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5141/3913296211.py:33: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasRegressor(build_fn=create_model, epochs=1000)\n",
      "2023-07-22 12:54:51.030047: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-22 12:54:51.030470: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-22 12:54:51.030478: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-22 12:54:56.700183: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-07-22 12:54:56.700308: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-07-22 12:54:56.701028: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-07-22 12:54:56.701028: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-07-22 12:54:56.701153: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-07-22 12:54:56.701153: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-07-22 12:55:29.046261: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-22 12:55:29.046256: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-22 12:55:29.046335: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-22 12:55:29.069183: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-22 12:55:29.069255: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-07-22 12:55:29.069278: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-22 12:55:29.069278: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-07-22 12:55:29.069364: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-07-22 12:55:29.069363: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-07-22 12:56:38.588603: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2023-07-22 12:56:38.588762: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2023-07-22 12:56:38.588822: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2023-07-22 12:56:38.589473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: Dhyey-Ubuntu\n",
      "2023-07-22 12:56:38.589507: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: Dhyey-Ubuntu\n",
      "2023-07-22 12:56:38.589586: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: Dhyey-Ubuntu\n",
      "2023-07-22 12:56:38.589591: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: Dhyey-Ubuntu\n",
      "2023-07-22 12:56:38.589953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 390.157.0\n",
      "2023-07-22 12:56:38.590030: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 390.157.0\n",
      "2023-07-22 12:56:38.590075: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 390.157.0\n",
      "2023-07-22 12:56:38.590103: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 390.157.0\n",
      "2023-07-22 12:56:38.590198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 390.157.0\n",
      "2023-07-22 12:56:38.590258: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 390.157.0\n",
      "2023-07-22 12:56:38.590426: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: Dhyey-Ubuntu\n",
      "2023-07-22 12:56:38.590500: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: Dhyey-Ubuntu\n",
      "2023-07-22 12:56:38.590859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 390.157.0\n",
      "2023-07-22 12:56:38.591010: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 390.157.0\n",
      "2023-07-22 12:56:38.591072: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 390.157.0\n",
      "2023-07-22 12:56:38.771860: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-22 12:56:38.771973: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-22 12:56:38.771972: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "Epoch 1/1000\n",
      "Epoch 1/1000\n",
      "12/12 [==============================] - 3s 7ms/step - loss: 0.4315 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "12/12 [==============================] - 3s 8ms/step - loss: 0.2101 - accuracy: 0.0028 - lr: 0.0010\n",
      "12/12 [==============================] - 3s 9ms/step - loss: 0.3311 - accuracy: 0.0028 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.1213 - accuracy: 0.0000e+00Epoch 2/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.3481 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.2418 - accuracy: 0.0028 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.3064 - accuracy: 0.0000e+00Epoch 3/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0813 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.3043 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.1545 - accuracy: 0.0028 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.2894 - accuracy: 0.0000e+00Epoch 4/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0727 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.2783 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0931 - accuracy: 0.0028 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0671 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.2522 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0725 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0645 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.2234 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0650 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0637 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.1908 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0605 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0610 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.1521 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0513 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0596 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.1085 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0487 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0573 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0719 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0471 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0559 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0552 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0482 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0546 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0481 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0453 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "Epoch 13/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0525 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0433 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0480 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0423 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0429 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0506 - accuracy: 0.0085Epoch 15/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0508 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0427 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0430 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0486 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "Epoch 15/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0411 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0475 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0401 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0406 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0459 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0396 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0406 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0448 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0397 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0397 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0431 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0518 - accuracy: 0.0000e+00Epoch 20/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0392 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0418 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0401 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0388 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0404 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0394 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0381 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0404 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "Epoch 22/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0389 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0391 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0372 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0387 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0393 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0366 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 24/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0379 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0377 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0362 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0375 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0366 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0369 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0376 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0357 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0359 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0371 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0367 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0559 - accuracy: 0.0312Epoch 28/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0350 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0382 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0380 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "Epoch 29/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0372 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0380 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0352 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0357 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0404 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0343 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0345 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0366 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0338 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0359 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0365 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0357 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0340 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0359 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0333 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0415 - accuracy: 0.0000e+00Epoch 34/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0340 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0359 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0328 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "Epoch 35/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0342 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0361 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0352 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "Epoch 36/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0345 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0336 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0352 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "Epoch 37/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0333 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0329 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0361 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "Epoch 38/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0342 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0348 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0363 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "Epoch 39/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0361 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0370 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0347 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0335 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0373 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0349 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0346 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0354 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0325 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0328 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0366 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0315 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0329 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0352 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0314 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0329 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0345 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0309 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0330 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0347 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0309 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0326 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0396 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0325 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0355 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0308 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0322 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0303 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0336 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "Epoch 49/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0321 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0338 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0304 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0327 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "Epoch 50/1000\n",
      "Epoch 50/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0321 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0305 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.0332 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0299 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0319 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0316 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0321 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0331 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "Epoch 52/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0330 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0319 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0314 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "Epoch 54/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0342 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0351 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0305 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "Epoch 55/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0338 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0308 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0333 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0334 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0304 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0335 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0331 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0301 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0316 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0355 - accuracy: 0.0000e+00Epoch 58/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0326 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0318 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0319 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0333 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0300 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "Epoch 60/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0320 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0337 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0295 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0452 - accuracy: 0.0000e+00Epoch 61/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0317 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0296 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0342 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0305 - accuracy: 0.0312Epoch 61/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0313 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0302 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0388 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0310 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0319 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0325 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0344 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0510 - accuracy: 0.0312Epoch 64/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0357 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0344 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0340 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "Epoch 65/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0317 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0317 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0333 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0403 - accuracy: 0.0000e+00Epoch 65/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0302 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0312 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0337 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "Epoch 66/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0295 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0311 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0230 - accuracy: 0.0000e+00Epoch 67/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0305 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0331 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "Epoch 70/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0305 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0360 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0233 - accuracy: 0.0625Epoch 71/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0317 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0333 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0288 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0319 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0327 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0304 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0316 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0320 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0298 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0317 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0295 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0281 - accuracy: 0.0000e+00Epoch 75/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0301 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0348 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0295 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0315 - accuracy: 0.0000e+00Epoch 76/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0315 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0357 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0290 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "Epoch 77/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0303 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0325 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0312 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0306 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0317 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0309 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0395 - accuracy: 0.0000e+00Epoch 79/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0305 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0296 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0311 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0402 - accuracy: 0.0000e+00Epoch 78/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0307 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0315 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0332 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0286 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0304 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0314 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0318 - accuracy: 0.0000e+00Epoch 81/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0295 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0314 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0309 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "Epoch 82/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0299 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0308 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0312 - accuracy: 0.0000e+00Epoch 83/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0312 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0313 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0323 - accuracy: 0.0000e+00Epoch 84/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "Epoch 87/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0322 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0306 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0286 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0317 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0216 - accuracy: 0.0312Epoch 86/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0304 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "Epoch 89/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0311 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0300 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0298 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0417 - accuracy: 0.0000e+00Epoch 89/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0314 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0308 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0351 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "Epoch 89/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0284 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0312 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0309 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0282 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0304 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0347 - accuracy: 0.0312Epoch 92/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0316 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0302 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0201 - accuracy: 0.0000e+00Epoch 93/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0308 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0318 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0320 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0305 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0319 - accuracy: 0.0000e+00Epoch 94/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0284 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0307 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0308 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "Epoch 96/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0283 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0298 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0307 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "Epoch 96/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0295 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0310 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "Epoch 97/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0309 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0303 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "Epoch 99/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0279 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0311 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "Epoch 99/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0339 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0367 - accuracy: 0.0312Epoch 101/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0343 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0306 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0295 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0325 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0356 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0296 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0349 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0172 - accuracy: 0.0000e+00Epoch 103/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0304 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0320 - accuracy: 0.0312Epoch 104/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0296 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0305 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0222 - accuracy: 0.0000e+00Epoch 105/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0298 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0175 - accuracy: 0.0000e+00Epoch 107/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0283 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0295 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0313 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "Epoch 107/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0302 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0311 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "Epoch 108/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0312 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "Epoch 110/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0282 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0289 - accuracy: 0.0000e+00Epoch 111/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0301 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0269 - accuracy: 0.0000e+00Epoch 112/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0289 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0215 - accuracy: 0.0000e+00Epoch 111/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0311 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0279 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0287 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0160 - accuracy: 0.0625Epoch 112/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0317 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 112/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0283 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0295 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0304 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0274 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0294 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0318 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 116/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0299 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0315 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0277 - accuracy: 0.0000e+00Epoch 117/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0309 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 116/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0308 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 118/1000\n",
      "Epoch 116/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 117/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 119/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0337 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0292 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 117/1000\n",
      "Epoch 118/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 120/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0287 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0309 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 119/1000\n",
      "Epoch 118/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0297 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 121/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 120/1000\n",
      "Epoch 119/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0284 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0312 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 121/1000\n",
      "Epoch 120/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0274 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0298 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0196 - accuracy: 0.0000e+00Epoch 121/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 124/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0282 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0294 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0270 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0279 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 124/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0315 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0291 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 126/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0287 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0296 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0311 - accuracy: 0.0000e+00Epoch 124/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0278 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 127/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0314 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 126/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 128/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 127/1000\n",
      "Epoch 126/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 129/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0302 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 127/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0293 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0299 - accuracy: 0.0312Epoch 128/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0270 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 130/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0300 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 128/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 129/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 131/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 129/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 130/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 132/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0314 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 130/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 131/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 133/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0296 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 131/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0298 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 132/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0297 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 134/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0286 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0318 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 133/1000\n",
      "Epoch 132/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0278 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 135/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0315 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 134/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0291 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 133/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 136/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0315 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 135/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0301 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 134/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0274 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 137/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0300 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 135/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0304 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 136/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 138/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 136/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 137/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 139/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0299 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 137/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 138/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 140/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0286 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 138/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0303 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0513 - accuracy: 0.0000e+00Epoch 139/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0300 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 141/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 139/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 140/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 142/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 140/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0187 - accuracy: 0.0312Epoch 141/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0319 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 143/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 141/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0304 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 142/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0297 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 144/1000\n",
      "Epoch 142/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0348 - accuracy: 0.0000e+00Epoch 143/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0282 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 145/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0295 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 144/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0301 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 143/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0274 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 146/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0294 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0318 - accuracy: 0.0000e+00Epoch 145/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 144/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 147/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 146/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0292 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 145/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 148/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 147/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0282 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 146/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 149/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 148/1000\n",
      "Epoch 147/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 150/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0285 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0295 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 149/1000\n",
      "Epoch 148/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0280 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 151/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0302 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 149/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0306 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 150/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 150/1000\n",
      "Epoch 152/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 151/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0296 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0322 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 151/1000\n",
      "Epoch 153/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 152/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 154/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 152/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0274 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0230 - accuracy: 0.0000e+00Epoch 153/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 155/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0301 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0274 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 154/1000\n",
      "Epoch 153/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0265 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 156/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0276 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0290 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 155/1000\n",
      "Epoch 154/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 157/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 156/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0196 - accuracy: 0.0000e+00Epoch 155/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 158/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 157/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0282 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 156/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 159/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 158/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0305 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 157/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 160/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 159/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0295 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 158/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 161/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0278 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 160/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0294 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 159/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 162/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 161/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 160/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 163/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 162/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0304 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 161/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 164/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 163/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0314 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 162/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 165/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 164/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0284 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 163/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 166/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0264 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 165/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 164/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 167/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 166/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 165/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0299 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 168/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0287 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 167/1000\n",
      "Epoch 166/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0312 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0292 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 169/1000\n",
      "Epoch 167/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 168/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 170/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 168/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0278 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 169/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 171/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0272 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 170/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0286 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 169/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 172/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 170/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0300 - accuracy: 0.0312Epoch 171/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 173/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 171/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 172/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 172/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0216 - accuracy: 0.0312Epoch 174/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0299 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 173/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 173/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 175/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 174/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 174/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 176/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0273 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 175/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0279 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 175/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 177/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 176/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 176/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 178/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 177/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0274 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0314 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 179/1000\n",
      "Epoch 177/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 178/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0282 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 180/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 178/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.0000e+00 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 179/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0312 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 181/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 179/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0258 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 180/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 182/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0276 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 180/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 181/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 183/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0284 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 181/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 182/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 184/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0287 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 182/1000\n",
      "Epoch 183/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 185/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0313 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 183/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 184/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0281 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 186/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 184/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0255 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 185/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0295 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 187/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0287 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 185/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 186/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 188/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 186/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 187/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 189/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 187/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 188/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 190/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0267 - accuracy: 0.0000e+00Epoch 188/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 189/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 191/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0278 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 189/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0260 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 190/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0258 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0418 - accuracy: 0.0000e+00Epoch 192/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0274 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 190/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 191/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0168 - accuracy: 0.0000e+00Epoch 193/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 191/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 194/1000\n",
      "Epoch 192/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 192/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 195/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0260 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 193/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0222 - accuracy: 0.0000e+00Epoch 193/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 196/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0287 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 194/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0291 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 194/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 197/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 195/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 195/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 198/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0276 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 196/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 196/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 199/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 197/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 197/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 200/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0157 - accuracy: 0.0312Epoch 198/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 198/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 201/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0304 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 199/1000\n",
      "Epoch 199/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 202/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 200/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0336 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 200/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0260 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 203/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 201/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0291 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 201/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 204/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 202/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 202/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 205/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 203/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0282 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 203/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 206/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 204/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0294 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 204/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 207/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 205/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 205/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 208/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0249 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 206/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0289 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 206/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 209/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0249 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 207/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0274 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 207/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 210/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 208/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 208/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 211/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 209/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0405 - accuracy: 0.0000e+00Epoch 209/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 212/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 210/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0256 - accuracy: 0.0000e+00Epoch 210/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0273 - accuracy: 0.0000e+00Epoch 213/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0252 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 211/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0271 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 211/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0161 - accuracy: 0.0312Epoch 214/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 212/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0278 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0366 - accuracy: 0.0000e+00Epoch 212/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0158 - accuracy: 0.0000e+00Epoch 215/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 213/1000\n",
      "Epoch 213/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0200 - accuracy: 0.0000e+00Epoch 216/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 214/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 214/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 217/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 215/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 215/1000\n",
      "Epoch 218/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0250 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 216/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 216/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0267 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 219/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0300 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 217/1000\n",
      "Epoch 217/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 220/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 218/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 218/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 221/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 219/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 219/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 222/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 220/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 220/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 223/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 221/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 221/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 224/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0279 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0246 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 222/1000\n",
      "Epoch 222/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 225/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0296 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 223/1000\n",
      "Epoch 223/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 226/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 224/1000\n",
      "Epoch 224/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 227/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 225/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0507 - accuracy: 0.0312Epoch 225/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 228/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 226/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0314 - accuracy: 0.0312Epoch 226/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0274 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 229/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 227/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0248 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 227/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0330 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 230/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 228/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0246 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 228/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 231/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0285 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 229/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0248 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 229/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 232/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 230/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 233/1000\n",
      "Epoch 230/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 231/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 234/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 231/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0282 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 232/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 235/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0241 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 232/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 233/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0299 - accuracy: 0.0000e+00Epoch 236/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 233/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0303 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 234/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0291 - accuracy: 0.0000e+00Epoch 237/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 234/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 235/1000\n",
      "Epoch 238/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 235/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 239/1000\n",
      "Epoch 236/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0246 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 236/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0271 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0258 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 237/1000\n",
      "Epoch 240/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0237 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 237/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 238/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0311 - accuracy: 0.0000e+00Epoch 241/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 238/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 242/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0266 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 239/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 239/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 243/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0278 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 240/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 240/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 244/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0269 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 241/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0239 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 241/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0252 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 245/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 242/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 242/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 246/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 243/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 243/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 247/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 244/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 244/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0236 - accuracy: 0.0000e+00Epoch 248/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0286 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 245/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0256 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 245/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0257 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 249/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 246/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 246/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0249 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 250/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0265 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 247/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 247/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 251/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0268 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 248/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 248/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 252/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 249/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 249/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 253/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 250/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 250/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0242 - accuracy: 0.0000e+00Epoch 254/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0263 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 251/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 251/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0254 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 255/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 252/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 252/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 256/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 253/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0260 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 253/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 257/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 254/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0130 - accuracy: 0.0000e+00Epoch 254/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 258/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 255/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0322 - accuracy: 0.0000e+00Epoch 255/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0190 - accuracy: 0.0000e+00Epoch 259/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 256/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0268 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 256/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0269 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 260/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0316 - accuracy: 0.0312Epoch 257/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 257/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 261/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 258/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 258/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 262/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 259/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 263/1000\n",
      "Epoch 259/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 260/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0298 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0272 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 264/1000\n",
      "Epoch 260/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0246 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 261/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0274 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 261/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 265/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 262/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 262/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 266/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 263/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 263/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 267/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 264/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0310 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 264/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 268/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 265/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 265/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0252 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 269/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 266/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0273 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 266/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0251 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 270/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 267/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 267/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 268/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 271/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 268/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 272/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0226 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0263 - accuracy: 0.0000e+00Epoch 269/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 269/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 270/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0252 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 273/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 270/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0232 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 271/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0261 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 274/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 271/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0241 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 272/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0321 - accuracy: 0.0000e+00Epoch 275/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 272/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 273/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 276/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 273/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 274/1000\n",
      "Epoch 277/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 274/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 275/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 278/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 275/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 279/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0231 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 276/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0276 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 276/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 277/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0266 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 280/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0271 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 277/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 278/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 281/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 278/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 279/1000\n",
      "Epoch 282/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 279/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 280/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0273 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 283/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 280/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 281/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 284/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0311 - accuracy: 0.0312Epoch 281/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 282/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 285/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0345 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 282/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 283/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 286/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0299 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 283/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 284/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0274 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 287/1000\n",
      "Epoch 284/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 285/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 285/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 288/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 286/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0260 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 286/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0258 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 289/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0236 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 287/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 287/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0264 - accuracy: 0.0000e+00Epoch 290/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 288/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0268 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 291/1000\n",
      "Epoch 288/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 289/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 292/1000\n",
      "Epoch 289/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 290/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0305 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 293/1000\n",
      "Epoch 290/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 291/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0278 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 294/1000\n",
      "Epoch 291/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 292/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0278 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0233 - accuracy: 0.0000e+00Epoch 292/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0243 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 295/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 293/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0189 - accuracy: 0.0000e+00Epoch 293/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 296/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 294/1000\n",
      "Epoch 294/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 297/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 295/1000\n",
      "Epoch 295/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 298/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0295 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 296/1000\n",
      "Epoch 296/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 299/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 297/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 297/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 300/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 298/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 301/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0213 - accuracy: 0.0312Epoch 298/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 299/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0338 - accuracy: 0.0000e+00Epoch 302/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0292 - accuracy: 0.0000e+00Epoch 299/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 300/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 300/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0208 - accuracy: 0.0625Epoch 303/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 301/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0210 - accuracy: 0.0000e+00Epoch 301/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0247 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 304/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0230 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 302/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0248 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 302/1000\n",
      "Epoch 305/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 303/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 303/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0379 - accuracy: 0.0312Epoch 306/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 307/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0225 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 304/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0264 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 304/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 305/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0253 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 308/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 305/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 306/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 309/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0261 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 306/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 307/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 310/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0275 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 307/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 308/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0175 - accuracy: 0.0000e+00Epoch 311/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 308/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 309/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0266 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 312/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 309/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 310/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 313/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0273 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 310/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0238 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 311/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0189 - accuracy: 0.0000e+00Epoch 314/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 311/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 312/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 315/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 312/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0230 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 313/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0257 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 316/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0263 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 313/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0224 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 314/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0274 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0254 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 314/1000\n",
      "Epoch 317/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 315/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0263 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0267 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 315/1000\n",
      "Epoch 318/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0227 - accuracy: 0.0000e+00Epoch 316/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 316/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 317/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 319/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0260 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 317/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0249 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 320/1000\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0237 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 318/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0254 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0214 - accuracy: 0.0000e+00Epoch 318/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0251 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 321/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0228 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 319/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 319/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 322/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 320/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 320/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 323/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 321/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0203 - accuracy: 0.0000e+00Epoch 321/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 324/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 322/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0264 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 322/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0269 - accuracy: 0.0000e+00Epoch 325/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 323/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 323/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 326/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 324/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 324/1000\n",
      "Epoch 327/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 325/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 328/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0254 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0132 - accuracy: 0.0000e+00Epoch 325/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 326/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 329/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 326/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 327/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 327/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0252 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0176 - accuracy: 0.0312Epoch 330/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0294 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 328/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 328/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0246 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 331/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 329/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 329/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 330/1000\n",
      "Epoch 332/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 330/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 331/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0228 - accuracy: 0.0000e+00Epoch 333/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 331/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 332/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0257 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 334/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 332/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 333/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 335/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 333/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 334/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 336/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0251 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 334/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 335/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 337/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 336/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 335/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 338/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 337/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 336/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 339/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 338/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 337/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 340/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 339/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 338/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 341/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 340/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 339/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0447 - accuracy: 0.0000e+00Epoch 342/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 341/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 340/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0280 - accuracy: 0.0000e+00Epoch 343/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 342/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 341/1000\n",
      "Epoch 344/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 343/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 345/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0270 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0181 - accuracy: 0.0000e+00Epoch 342/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 344/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 346/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0255 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 343/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 345/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 347/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0254 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 346/1000\n",
      "Epoch 344/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 348/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 345/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 347/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 349/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 346/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 348/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0244 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 350/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 349/1000\n",
      "Epoch 347/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0244 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 351/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0177 - accuracy: 0.0000e+00Epoch 350/1000\n",
      "Epoch 348/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 352/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 349/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0184 - accuracy: 0.0000e+00Epoch 351/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0241 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 353/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 352/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 350/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 354/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 353/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 355/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0258 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0213 - accuracy: 0.0000e+00Epoch 351/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0217 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 354/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0366 - accuracy: 0.0000e+00Epoch 356/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 352/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 357/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0243 - accuracy: 0.0000e+00Epoch 355/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0183 - accuracy: 0.0000e+00Epoch 353/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 358/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 356/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0212 - accuracy: 0.0312Epoch 354/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 357/1000\n",
      "Epoch 359/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0383 - accuracy: 0.0312Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 358/1000\n",
      "Epoch 360/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 356/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 359/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 357/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 361/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 360/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 358/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 362/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0217 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 359/1000\n",
      "Epoch 361/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 363/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 360/1000\n",
      "Epoch 362/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 364/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0230 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 363/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0266 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0240 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 365/1000\n",
      "Epoch 361/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0248 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 364/1000\n",
      "Epoch 366/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0283 - accuracy: 0.0000e+00Epoch 362/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 365/1000\n",
      "Epoch 363/1000\n",
      "Epoch 367/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 366/1000\n",
      "Epoch 364/1000\n",
      "Epoch 368/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 367/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 369/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0167 - accuracy: 0.0000e+00Epoch 365/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 366/1000\n",
      "Epoch 368/1000\n",
      "Epoch 370/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 367/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 371/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 369/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 368/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 372/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0160 - accuracy: 0.0000e+00Epoch 370/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 369/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0248 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 371/1000\n",
      "Epoch 373/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 370/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 372/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 374/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 371/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 373/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 375/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 372/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 374/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0238 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 376/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 373/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 375/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 377/1000\n",
      "Epoch 374/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 376/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 378/1000\n",
      "Epoch 375/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 377/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 379/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0242 - accuracy: 0.0000e+00Epoch 378/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0264 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0425 - accuracy: 0.0000e+00Epoch 376/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 380/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0224 - accuracy: 0.0312Epoch 379/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0171 - accuracy: 0.0000e+00Epoch 377/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 380/1000\n",
      "Epoch 381/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 378/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 382/1000\n",
      "Epoch 381/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 379/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 383/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 382/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0241 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 380/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 384/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 383/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 381/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 385/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 382/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0289 - accuracy: 0.0000e+00Epoch 384/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0248 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 383/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0240 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 386/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 385/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0247 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 384/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0254 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 387/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0203 - accuracy: 0.0000e+00Epoch 386/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0248 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 385/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0249 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 388/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 387/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 386/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 389/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 388/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 390/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0316 - accuracy: 0.0000e+00Epoch 387/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 389/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 388/1000\n",
      "Epoch 391/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 390/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0279 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 389/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0313 - accuracy: 0.0625Epoch 392/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 391/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0282 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 393/1000\n",
      "Epoch 390/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 392/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 391/1000\n",
      "Epoch 394/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 393/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 392/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 395/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 394/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 393/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 396/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 395/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 394/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0261 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 397/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 396/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 395/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 398/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 397/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 396/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 399/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 398/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 397/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 400/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 399/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 401/1000\n",
      "Epoch 398/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 400/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 402/1000\n",
      "Epoch 399/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 401/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 403/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 400/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 402/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 404/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0252 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 401/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0191 - accuracy: 0.0000e+00Epoch 403/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 405/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 402/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 404/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 406/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 403/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 405/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 407/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 404/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 406/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 408/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 407/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0249 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 405/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0246 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 409/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 408/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 406/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 410/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 409/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 407/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 411/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 410/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 408/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 412/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 411/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 409/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 413/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 412/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0239 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 410/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 414/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 413/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 411/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 415/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 414/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 412/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0236 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 416/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0104 - accuracy: 0.0000e+00Epoch 415/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0157 - accuracy: 0.0000e+00Epoch 413/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 417/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 414/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0337 - accuracy: 0.0000e+00Epoch 416/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0241 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 415/1000\n",
      "Epoch 417/1000\n",
      "Epoch 418/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 418/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 419/1000\n",
      "Epoch 416/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 419/1000\n",
      "Epoch 420/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0270 - accuracy: 0.0312Epoch 417/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 421/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0286 - accuracy: 0.0000e+00Epoch 418/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0200 - accuracy: 0.0000e+00Epoch 420/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 422/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0236 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 419/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 421/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 423/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 420/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0176 - accuracy: 0.0000e+00Epoch 422/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 424/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0235 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 421/1000\n",
      "Epoch 423/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 425/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0238 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 422/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 424/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0230 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 426/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0238 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 423/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0253 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 427/1000\n",
      "Epoch 425/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 424/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 428/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 426/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 425/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 429/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 427/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 426/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 430/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 428/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 427/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0243 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 431/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 429/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 428/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 432/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 430/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 429/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0240 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 433/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0263 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 431/1000\n",
      "Epoch 430/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0231 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 434/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0251 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 431/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 432/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0253 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 435/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0247 - accuracy: 0.0000e+00Epoch 432/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 433/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0244 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 433/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0231 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 436/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0197 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 434/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0253 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 434/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0240 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 437/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 435/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0248 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 435/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 438/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 436/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 436/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 439/1000\n",
      "Epoch 437/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0246 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 437/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 438/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0095 - accuracy: 0.0000e+00Epoch 440/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 439/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 438/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0239 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 441/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 439/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 440/1000\n",
      "Epoch 442/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 440/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 441/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0087 - accuracy: 0.0000e+00Epoch 443/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 442/1000\n",
      "Epoch 441/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0231 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 444/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 443/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 442/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 445/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 444/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0236 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 443/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0236 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 446/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 445/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 444/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 447/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 446/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 445/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0209 - accuracy: 0.0312Epoch 448/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 447/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0122 - accuracy: 0.0000e+00Epoch 446/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 449/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 448/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0231 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0160 - accuracy: 0.0000e+00Epoch 447/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0231 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 450/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 448/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0413 - accuracy: 0.0000e+00Epoch 449/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0243 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 451/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0231 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 449/1000\n",
      "Epoch 450/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 452/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 450/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 451/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 451/1000\n",
      "Epoch 453/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 452/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 454/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0309 - accuracy: 0.0312Epoch 452/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 453/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 455/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0085 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 453/1000\n",
      "Epoch 454/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0228 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 456/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 455/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0248 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0252 - accuracy: 0.0000e+00Epoch 454/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0244 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 457/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0195 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0241 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 455/1000\n",
      "Epoch 456/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0232 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 458/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0195 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 457/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0231 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0108 - accuracy: 0.0000e+00Epoch 456/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 459/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 458/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0226 - accuracy: 0.0000e+00Epoch 457/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0240 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 460/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0226 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 459/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 458/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0247 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 461/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0225 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 459/1000\n",
      "Epoch 460/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 462/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 460/1000\n",
      "Epoch 461/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 463/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 462/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 461/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 464/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 463/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 462/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0241 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 465/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0215 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 464/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0257 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 463/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0232 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 466/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 465/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0239 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 464/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 467/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 466/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 468/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0151 - accuracy: 0.0000e+00Epoch 465/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 467/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0147 - accuracy: 0.0000e+00Epoch 469/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 466/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 468/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 470/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0240 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0251 - accuracy: 0.0000e+00Epoch 467/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 469/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0230 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 471/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0238 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 468/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 470/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 472/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 469/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0194 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 471/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0224 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 473/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0228 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 470/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 474/1000\n",
      "Epoch 472/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 471/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 473/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 475/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0248 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 472/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 476/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 474/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0235 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 473/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 477/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 475/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0236 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 474/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 478/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0245 - accuracy: 0.0000e+00Epoch 476/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 475/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 479/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0320 - accuracy: 0.0000e+00Epoch 477/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0231 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 476/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 478/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0296 - accuracy: 0.0000e+00Epoch 480/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 477/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 481/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 479/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0232 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 478/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 482/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0129 - accuracy: 0.0000e+00Epoch 480/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0235 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 479/1000\n",
      "Epoch 483/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0357 - accuracy: 0.0312Epoch 481/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 484/1000\n",
      "Epoch 480/1000\n",
      "Epoch 482/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 485/1000\n",
      "Epoch 483/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0346 - accuracy: 0.0000e+00Epoch 481/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 486/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 484/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0230 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 482/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 487/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 485/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0228 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 483/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 488/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 486/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 484/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      " 9/12 [=====================>........] - ETA: 0s - loss: 0.0204 - accuracy: 0.0000e+00Epoch 489/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0193 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 487/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0228 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 485/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 490/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0193 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 488/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 491/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0260 - accuracy: 0.0000e+00Epoch 486/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0238 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0191 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 492/1000\n",
      "Epoch 489/1000\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.0233 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 487/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0198 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0243 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 490/1000\n",
      "Epoch 493/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0235 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 488/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0195 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 491/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0245 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0210 - accuracy: 0.0000e+00Epoch 494/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 492/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0241 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0335 - accuracy: 0.0000e+00Epoch 489/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0238 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 495/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0191 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 493/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0246 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.0085    Epoch 490/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0220 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 496/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0258 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 491/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0193 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0207 - accuracy: 0.0000e+00Epoch 494/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0220 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 497/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0228 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 492/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0247 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 495/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0233 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 498/1000\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.0229 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 493/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 496/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0231 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 499/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 497/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0235 - accuracy: 0.0085 - lr: 0.0010\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0227 - accuracy: 0.0085    Epoch 494/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 500/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0194 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 498/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0233 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 495/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0229 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 501/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0191 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 499/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0224 - accuracy: 0.0085 - lr: 0.0010\n",
      "11/12 [==========================>...] - ETA: 0s - loss: 0.0225 - accuracy: 0.0085    Epoch 496/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0225 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 502/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 500/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0229 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 497/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0355 - accuracy: 0.0312Epoch 503/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 501/1000\n",
      "12/12 [==============================] - ETA: 0s - loss: 0.0224 - accuracy: 0.0085Epoch 504/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0224 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 498/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 502/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 505/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 499/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0193 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 503/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 500/1000\n",
      "Epoch 506/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 504/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 501/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0274 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 507/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 505/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0239 - accuracy: 0.0000e+00Epoch 502/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 508/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 506/1000\n",
      "Epoch 503/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 509/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 507/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0192 - accuracy: 0.0000e+00Epoch 504/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0232 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 510/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0194 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 508/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 505/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0236 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 511/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 509/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 512/1000\n",
      "Epoch 506/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 513/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 510/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 507/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 514/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 511/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0164 - accuracy: 0.0000e+00Epoch 508/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 515/1000\n",
      "Epoch 512/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0259 - accuracy: 0.0000e+00Epoch 509/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 513/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0230 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0224 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 516/1000\n",
      "Epoch 510/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 514/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 511/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0236 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 517/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 515/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0161 - accuracy: 0.0000e+00Epoch 512/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0230 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 518/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 516/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0132 - accuracy: 0.0000e+00Epoch 513/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 519/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 517/1000\n",
      "Epoch 514/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 520/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 518/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0247 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 515/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0238 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 521/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 519/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0259 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 516/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 522/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0187 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 520/1000\n",
      "Epoch 517/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 523/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 521/1000\n",
      "Epoch 518/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0232 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 524/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 522/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0250 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 519/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 523/1000\n",
      "Epoch 525/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 520/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 526/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0275 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 524/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 521/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 527/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0241 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 525/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 522/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 528/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 526/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 523/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0178 - accuracy: 0.0000e+00Epoch 529/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 527/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 524/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 530/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 528/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 525/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 531/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 529/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 526/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 532/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 530/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 527/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 533/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 531/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0240 - accuracy: 0.0000e+00Epoch 528/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 534/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0224 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 529/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0189 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0110 - accuracy: 0.0312Epoch 532/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0212 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 535/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 533/1000\n",
      "Epoch 530/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 536/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 534/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 531/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 537/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 535/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 532/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 538/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 533/1000\n",
      "Epoch 536/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 539/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0191 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 537/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0230 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0103 - accuracy: 0.0000e+00Epoch 534/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 540/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0189 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 538/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0158 - accuracy: 0.0000e+00Epoch 535/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 541/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 539/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 536/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 542/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 540/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 537/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 543/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 541/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 538/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 544/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 542/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 539/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0230 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 545/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 543/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 540/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 546/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 544/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 541/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 547/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 545/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 542/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 548/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 546/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 543/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 549/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 547/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0304 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 544/1000\n",
      "Epoch 550/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 548/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 545/1000\n",
      "Epoch 551/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 549/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 546/1000\n",
      "Epoch 552/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 550/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 553/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 547/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 551/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 548/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0144 - accuracy: 0.0000e+00Epoch 554/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 552/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 549/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 555/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 553/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 550/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 556/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0195 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 554/1000\n",
      "Epoch 551/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 557/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 552/1000\n",
      "Epoch 555/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 558/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 553/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 556/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0228 - accuracy: 0.0000e+00Epoch 559/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 554/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0187 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 557/1000\n",
      "Epoch 560/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 555/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 561/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 558/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 556/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 562/1000\n",
      "Epoch 559/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 557/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 560/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 563/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 558/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 561/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 564/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0239 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 559/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 562/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0228 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 565/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 560/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0341 - accuracy: 0.0312Epoch 563/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 566/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 561/1000\n",
      "Epoch 564/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 567/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 562/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 565/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 568/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 566/1000\n",
      "Epoch 563/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 569/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0228 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 564/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0189 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 567/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 570/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 565/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0187 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0187 - accuracy: 0.0312Epoch 568/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0247 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 571/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0241 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 566/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0190 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 569/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 572/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0246 - accuracy: 0.0085 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 567/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0188 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 570/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 573/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0239 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 568/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 571/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0232 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 574/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 569/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0186 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 572/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 575/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 570/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 573/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 576/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 571/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 574/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 577/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0237 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 572/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 575/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 578/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0224 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 573/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 576/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 579/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0224 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 577/1000\n",
      "Epoch 574/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 580/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 578/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 575/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 581/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 579/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 576/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0240 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 582/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0197 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 580/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 577/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 583/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 581/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 578/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 584/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 582/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0119 - accuracy: 0.0000e+00Epoch 579/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 585/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 583/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0232 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 580/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 586/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0208 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 584/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0242 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 581/1000\n",
      "Epoch 587/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 585/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 588/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0199 - accuracy: 0.0312Epoch 582/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 586/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 589/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 583/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 587/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 590/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0190 - accuracy: 0.0000e+00Epoch 584/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 588/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 585/1000\n",
      "Epoch 591/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0185 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 589/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 586/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 592/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 590/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 587/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 593/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 588/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0148 - accuracy: 0.0000e+00Epoch 591/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 594/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 589/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0194 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 592/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 595/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 590/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 593/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0232 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 596/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 591/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0179 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 594/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 597/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 592/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0185 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 598/1000\n",
      "Epoch 595/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 593/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 596/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 599/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 594/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 597/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 600/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 595/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0217 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 601/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0183 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0226 - accuracy: 0.0000e+00Epoch 598/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0230 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 596/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 602/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 597/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0226 - accuracy: 0.0312Epoch 599/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 603/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 600/1000\n",
      "Epoch 598/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 604/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 599/1000\n",
      "Epoch 601/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 605/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0182 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 602/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0251 - accuracy: 0.0000e+00Epoch 600/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 606/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0188 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 603/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0105 - accuracy: 0.0000e+00Epoch 601/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 607/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0181 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 604/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0217 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 602/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 608/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 605/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0225 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 603/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 609/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 606/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0249 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 604/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0228 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 610/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 607/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0241 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 605/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 611/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 608/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 606/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 612/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 609/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 607/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 613/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 610/1000\n",
      "Epoch 608/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0201 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 614/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 611/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0225 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0153 - accuracy: 0.0000e+00Epoch 609/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 615/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0187 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 612/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0225 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 610/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0211 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 616/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 613/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 611/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 617/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 614/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 612/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 618/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 615/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 613/1000\n",
      "Epoch 619/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0187 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 616/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 620/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 614/1000\n",
      "Epoch 617/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 621/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0085 - accuracy: 0.0625Epoch 618/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0187 - accuracy: 0.0000e+00Epoch 615/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 622/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 619/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 616/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 623/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 620/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 617/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 624/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0302 - accuracy: 0.0625Epoch 621/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 618/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 622/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0202 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 625/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0217 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 619/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0192 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 623/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 626/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 620/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 627/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 624/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 621/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 625/1000\n",
      "Epoch 628/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 622/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0181 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 626/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0258 - accuracy: 0.0000e+00Epoch 629/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 623/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 630/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0121 - accuracy: 0.0000e+00Epoch 627/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 624/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 631/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 628/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 625/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 632/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0235 - accuracy: 0.0000e+00Epoch 629/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 633/1000\n",
      "Epoch 626/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 630/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 634/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0220 - accuracy: 0.0312Epoch 627/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0179 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 631/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 635/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 628/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 632/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 636/1000\n",
      "Epoch 629/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0179 - accuracy: 0.0000e+00Epoch 633/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 637/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0179 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 630/1000\n",
      "Epoch 634/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 638/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0198 - accuracy: 0.0000e+00Epoch 631/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 635/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0217 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 632/1000\n",
      "Epoch 639/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 636/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 640/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 633/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 637/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 641/1000\n",
      "Epoch 634/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 638/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 635/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0191 - accuracy: 0.0312Epoch 642/1000\n",
      "Epoch 639/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 636/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 640/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0206 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 643/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 637/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0129 - accuracy: 0.0000e+00Epoch 641/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 644/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0223 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 642/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0225 - accuracy: 0.0000e+00Epoch 645/1000\n",
      "Epoch 638/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 643/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 646/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 639/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 644/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 647/1000\n",
      "Epoch 640/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 645/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 641/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0346 - accuracy: 0.0000e+00Epoch 648/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 646/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 642/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0235 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 649/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0189 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 647/1000\n",
      "Epoch 643/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 650/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 648/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 644/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 651/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 649/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 645/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0105 - accuracy: 0.0000e+00Epoch 652/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 650/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 646/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 653/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 651/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0216 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 647/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 654/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 652/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 648/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 655/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 653/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 649/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 656/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 654/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 650/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 657/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 655/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 651/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0145 - accuracy: 0.0312Epoch 658/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 656/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 659/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 652/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0182 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 657/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 653/1000\n",
      "Epoch 660/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 658/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 654/1000\n",
      "Epoch 661/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 662/1000\n",
      "Epoch 659/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0093 - accuracy: 0.0000e+00Epoch 655/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 663/1000\n",
      "Epoch 660/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 656/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 664/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0185 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 661/1000\n",
      "Epoch 657/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0192 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0177 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 662/1000\n",
      "Epoch 665/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 658/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 663/1000\n",
      "Epoch 666/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 659/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 667/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 664/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 660/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 668/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 665/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 661/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 669/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 666/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 662/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 670/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0175 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 667/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 663/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 671/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 668/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 664/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 672/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 669/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 665/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 673/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 670/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 666/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 674/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 671/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 667/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0194 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 675/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 672/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0208 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0119 - accuracy: 0.0000e+00Epoch 668/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 676/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 673/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 669/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 677/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 674/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 670/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 678/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0179 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0141 - accuracy: 0.0000e+00Epoch 675/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 671/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 679/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 676/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0209 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 672/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0193 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 680/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0175 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 677/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 673/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0290 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 681/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 678/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 674/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 682/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0364 - accuracy: 0.0312Epoch 679/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 675/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 683/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 680/1000\n",
      "Epoch 676/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 684/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 681/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0161 - accuracy: 0.0000e+00Epoch 677/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0203 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 685/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 682/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 678/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 686/1000\n",
      "Epoch 683/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 679/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 687/1000\n",
      "Epoch 684/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 680/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 688/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 685/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 681/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 689/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 686/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 682/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 690/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 687/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 683/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 691/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 688/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 684/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 692/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 689/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 685/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 693/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 690/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 686/1000\n",
      "Epoch 694/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0179 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 691/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 695/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0190 - accuracy: 0.0000e+00Epoch 687/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 692/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 696/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 688/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 693/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 697/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 689/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 694/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 698/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 690/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0175 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 695/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 699/1000\n",
      "Epoch 691/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 696/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 700/1000\n",
      "Epoch 692/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0170 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 697/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 693/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0197 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 698/1000\n",
      "Epoch 701/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 694/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 699/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 702/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 695/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 700/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 703/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 696/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0184 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 701/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 704/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0208 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 697/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 702/1000\n",
      "Epoch 705/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 698/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0192 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 706/1000\n",
      "Epoch 703/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 699/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 707/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0178 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 704/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0206 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 700/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 708/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 705/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 701/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 709/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 706/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0211 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 702/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 710/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 707/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0209 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 703/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 711/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0175 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 708/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0208 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 704/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0191 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0239 - accuracy: 0.0000e+00Epoch 712/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 709/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 713/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0239 - accuracy: 0.0312Epoch 705/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0223 - accuracy: 0.0312Epoch 710/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0221 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 714/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0206 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 706/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0171 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 711/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 715/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 707/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0175 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0145 - accuracy: 0.0000e+00Epoch 712/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 716/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 708/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0170 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 713/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 717/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 714/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 709/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 718/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 710/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0187 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 715/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0189 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 719/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 711/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0162 - accuracy: 0.0000e+00Epoch 716/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 720/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 712/1000\n",
      "Epoch 717/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0186 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 721/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0208 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0169 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 713/1000\n",
      "Epoch 718/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0187 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 722/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0219 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0178 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 714/1000\n",
      "Epoch 719/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0216 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 723/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0177 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 720/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 715/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 724/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 721/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 716/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0194 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 725/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 722/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0209 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 717/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 726/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 723/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 718/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0189 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 727/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0193 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 724/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0211 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 719/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 728/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0149 - accuracy: 0.0312Epoch 725/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 720/1000\n",
      "Epoch 729/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 726/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 721/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 730/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0144 - accuracy: 0.0312Epoch 727/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 722/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 731/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0070 - accuracy: 0.0312Epoch 728/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 723/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 732/1000\n",
      "Epoch 729/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 724/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 733/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 730/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 725/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 731/1000\n",
      "Epoch 734/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 726/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0242 - accuracy: 0.0000e+00Epoch 732/1000\n",
      "Epoch 735/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 727/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 733/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0186 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0268 - accuracy: 0.0000e+00Epoch 736/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0209 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 728/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0173 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 734/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 737/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 729/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0169 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 735/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 738/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 730/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0173 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 739/1000\n",
      "Epoch 731/1000\n",
      "Epoch 736/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 732/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 740/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0276 - accuracy: 0.0312Epoch 737/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 741/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0292 - accuracy: 0.0312Epoch 733/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0172 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 738/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0184 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 742/1000\n",
      "Epoch 734/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 739/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 743/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 735/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0173 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 740/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 736/1000\n",
      "Epoch 744/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 741/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 737/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0169 - accuracy: 0.0000e+00Epoch 745/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 742/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 738/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 746/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 743/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 739/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0193 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 747/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0181 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 744/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      "10/12 [========================>.....] - ETA: 0s - loss: 0.0196 - accuracy: 0.0094    Epoch 748/1000\n",
      "12/12 [==============================] - 0s 7ms/step - loss: 0.0202 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0192 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 740/1000\n",
      "Epoch 745/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 749/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 746/1000\n",
      "Epoch 741/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 750/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0172 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 747/1000\n",
      "Epoch 742/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0185 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 751/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0172 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 748/1000\n",
      "Epoch 743/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 752/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 744/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0170 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0127 - accuracy: 0.0312Epoch 749/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 753/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 745/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0171 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 750/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 754/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 751/1000\n",
      "Epoch 746/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 755/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 752/1000\n",
      "Epoch 747/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0182 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 756/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0184 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 753/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 748/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0188 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 757/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 754/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0335 - accuracy: 0.0000e+00Epoch 749/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 758/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 750/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0270 - accuracy: 0.0312Epoch 755/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0186 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 759/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 751/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 756/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 760/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 752/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0192 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 757/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 761/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 753/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 758/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0185 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 762/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0213 - accuracy: 0.0312Epoch 754/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 759/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 763/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 755/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 760/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 764/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0213 - accuracy: 0.0000e+00Epoch 756/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0167 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 761/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0179 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 765/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0136 - accuracy: 0.0312Epoch 757/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 762/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 766/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0228 - accuracy: 0.0000e+00Epoch 758/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0171 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 763/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 767/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 759/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 768/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 764/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 760/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0185 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 769/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0168 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 765/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0209 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 761/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0171 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 766/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 762/1000\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.0197 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 770/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0172 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 767/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0209 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 763/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 771/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0170 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0217 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 764/1000\n",
      "Epoch 768/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 772/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0167 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 769/1000\n",
      "Epoch 765/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0186 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 773/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 770/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 766/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0181 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 774/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 771/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0211 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 767/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 775/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0169 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 772/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 768/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 776/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 773/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 769/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0184 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 777/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 774/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 770/1000\n",
      "Epoch 778/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0169 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 775/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 771/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 779/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0171 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 776/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0151 - accuracy: 0.0000e+00Epoch 780/1000\n",
      "Epoch 772/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 777/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 781/1000\n",
      "Epoch 773/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 774/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0195 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 778/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0178 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 782/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0172 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 779/1000\n",
      "Epoch 775/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0182 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 783/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 780/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 776/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 784/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 781/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 777/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 785/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0375 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 782/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0651 - accuracy: 0.0000e+00Epoch 778/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 786/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0471 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 783/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0201 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 787/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0103 - accuracy: 0.0000e+00Epoch 779/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0419 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 784/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 780/1000\n",
      "Epoch 788/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0369 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 785/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 789/1000\n",
      "Epoch 781/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0352 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 786/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 790/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 782/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0345 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 787/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 791/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 783/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0331 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 788/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0186 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 784/1000\n",
      "Epoch 792/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0340 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 789/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 793/1000\n",
      "Epoch 785/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0322 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 790/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 786/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 794/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0322 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 791/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0229 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 795/1000\n",
      "Epoch 787/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0313 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 792/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 796/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0217 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 788/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0328 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 793/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 797/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0203 - accuracy: 0.0312Epoch 789/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0309 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 794/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 798/1000\n",
      "Epoch 790/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0312 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 795/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 799/1000\n",
      "Epoch 791/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0314 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 796/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 800/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0208 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 792/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0304 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 797/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0178 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 801/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 793/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0302 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 798/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 802/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0168 - accuracy: 0.0312Epoch 794/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0303 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 799/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 803/1000\n",
      "Epoch 795/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0301 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 800/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 804/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 796/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0300 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 801/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0179 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 805/1000\n",
      "Epoch 797/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 802/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 798/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0135 - accuracy: 0.0000e+00Epoch 806/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0295 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 803/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 807/1000\n",
      "Epoch 799/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0295 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 804/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 808/1000\n",
      "Epoch 800/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0296 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 805/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 809/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 801/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0292 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 806/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 802/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0184 - accuracy: 0.0000e+00Epoch 810/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 807/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 803/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 811/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0284 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 808/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 804/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0153 - accuracy: 0.0312Epoch 812/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0281 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 809/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 805/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0171 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 810/1000\n",
      "Epoch 813/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 806/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 811/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0309 - accuracy: 0.0000e+00Epoch 814/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 807/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0282 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 812/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 815/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 808/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0279 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 813/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 816/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0236 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 809/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0275 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 817/1000\n",
      "Epoch 814/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 810/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 818/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0300 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 815/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 811/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 819/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0292 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 816/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0216 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 820/1000\n",
      "Epoch 812/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 817/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0178 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 813/1000\n",
      "Epoch 821/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 818/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0184 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 822/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0199 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0313 - accuracy: 0.0000e+00Epoch 814/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 819/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 823/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0119 - accuracy: 0.0000e+00Epoch 815/1000\n",
      "Epoch 820/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 824/1000\n",
      "Epoch 821/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0208 - accuracy: 0.0000e+00Epoch 816/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 822/1000\n",
      "Epoch 825/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0270 - accuracy: 0.0000e+00Epoch 817/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 826/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 818/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0271 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0502 - accuracy: 0.0000e+00Epoch 823/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 827/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0242 - accuracy: 0.0000e+00Epoch 819/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0113 - accuracy: 0.0000e+00Epoch 824/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0193 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 828/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 825/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0206 - accuracy: 0.0312Epoch 820/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 829/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0149 - accuracy: 0.0312Epoch 826/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0212 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0287 - accuracy: 0.0000e+00Epoch 821/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 830/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0118 - accuracy: 0.0000e+00Epoch 827/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 822/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0179 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 831/1000\n",
      "Epoch 828/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 823/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 832/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0091 - accuracy: 0.0000e+00Epoch 829/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 824/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 830/1000\n",
      "Epoch 833/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 825/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0000e+00 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 831/1000\n",
      "Epoch 834/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 826/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 832/1000\n",
      "Epoch 835/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 827/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 833/1000\n",
      "Epoch 836/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 828/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 837/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 829/1000\n",
      "Epoch 834/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 838/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 830/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0231 - accuracy: 0.0312Epoch 835/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 831/1000\n",
      "Epoch 839/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0277 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 836/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 832/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0103 - accuracy: 0.0000e+00Epoch 840/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0268 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 837/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 833/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 841/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 838/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 834/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 842/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 839/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 835/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 843/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 840/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 836/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0170 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 844/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 841/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 837/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 845/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 842/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0320 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 838/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 846/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 843/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0278 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 839/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 847/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0219 - accuracy: 0.0312Epoch 844/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 840/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 845/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0179 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0349 - accuracy: 0.0000e+00Epoch 848/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 841/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 846/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0434 - accuracy: 0.0000e+00Epoch 849/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 842/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 847/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0242 - accuracy: 0.0000e+00Epoch 850/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 848/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 843/1000\n",
      "Epoch 851/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 849/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 844/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0194 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 852/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 850/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 845/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 853/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 851/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 846/1000\n",
      "Epoch 854/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 852/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 855/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0216 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 847/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 853/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 856/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 848/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 854/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 857/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 849/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 855/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 858/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 850/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0241 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 856/1000\n",
      "Epoch 859/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 851/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 857/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 860/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 852/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 858/1000\n",
      "Epoch 861/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 853/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0179 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 862/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0253 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 859/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 854/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 863/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 860/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 855/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 864/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 861/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 856/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 865/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 862/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0214 - accuracy: 0.0000e+00Epoch 857/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 866/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 863/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 858/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 867/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0240 - accuracy: 0.0312Epoch 864/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 859/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 868/1000\n",
      "Epoch 865/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 860/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 869/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 866/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 861/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 870/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 867/1000\n",
      "Epoch 862/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 871/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0238 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 868/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 863/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0178 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 872/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 864/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0251 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0178 - accuracy: 0.0000e+00Epoch 869/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 873/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 865/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 870/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 874/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 866/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 871/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 875/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 867/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0249 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 872/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 876/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 868/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0241 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 873/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0246 - accuracy: 0.0000e+00Epoch 877/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 869/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 874/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 878/1000\n",
      "Epoch 870/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 875/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 871/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0167 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0135 - accuracy: 0.0312Epoch 879/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 876/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 880/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0118 - accuracy: 0.0000e+00Epoch 872/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 877/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0165 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 881/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0109 - accuracy: 0.0000e+00Epoch 873/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 878/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 882/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 874/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 879/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0172 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 883/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 875/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0268 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 880/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0173 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 884/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0119 - accuracy: 0.0625Epoch 876/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0239 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 885/1000\n",
      "Epoch 881/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 877/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0246 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 882/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 886/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 878/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 883/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 887/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0330 - accuracy: 0.0312Epoch 879/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0249 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 884/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 888/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 880/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 889/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0255 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 885/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0369 - accuracy: 0.0000e+00Epoch 881/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 890/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 882/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 886/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 891/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 883/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0237 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 887/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0179 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0167 - accuracy: 0.0000e+00Epoch 892/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 884/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 888/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 893/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 885/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 889/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0172 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 894/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 886/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 890/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 895/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 887/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 891/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0165 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 896/1000\n",
      "Epoch 888/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0231 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 892/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0166 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 897/1000\n",
      "Epoch 889/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 893/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 898/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0203 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0239 - accuracy: 0.0000e+00Epoch 890/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0243 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 894/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0166 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 899/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 891/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 895/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 900/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 892/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 896/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 901/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0208 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 893/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0113 - accuracy: 0.0000e+00Epoch 897/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 902/1000\n",
      "Epoch 894/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 898/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 895/1000\n",
      "Epoch 903/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 899/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0163 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 896/1000\n",
      "Epoch 904/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 900/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 897/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0168 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 905/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0234 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 901/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 898/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0168 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 906/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0259 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 902/1000\n",
      "Epoch 899/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0169 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 907/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0233 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 903/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 900/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0164 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 908/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 904/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 901/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0162 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 909/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 905/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0258 - accuracy: 0.0000e+00Epoch 902/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 910/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 906/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0182 - accuracy: 0.0000e+00Epoch 903/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0178 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 911/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 907/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0201 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 904/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 912/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 908/1000\n",
      "Epoch 905/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0173 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 913/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 906/1000\n",
      "Epoch 909/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 914/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0100 - accuracy: 0.0312Epoch 907/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 910/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 915/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0266 - accuracy: 0.0000e+00Epoch 908/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0226 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 911/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0191 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 916/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0208 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 909/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0225 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 912/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0183 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 917/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 910/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0245 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 913/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 918/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0227 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 914/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0206 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 911/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0172 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 919/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0221 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 915/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 912/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0164 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 920/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0194 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 913/1000\n",
      "12/12 [==============================] - 0s 6ms/step - loss: 0.0229 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 916/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0186 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 921/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 914/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 917/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 922/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 915/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 918/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 923/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 916/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 919/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 924/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 917/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0225 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 925/1000\n",
      "Epoch 920/1000\n",
      "Epoch 918/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 926/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 919/1000\n",
      "Epoch 921/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 927/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 920/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 922/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 928/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 921/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0191 - accuracy: 0.0000e+00Epoch 923/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 929/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 922/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 924/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 930/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 925/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 923/1000\n",
      "Epoch 931/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0229 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 926/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0201 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 932/1000\n",
      "Epoch 924/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 927/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 925/1000\n",
      "Epoch 933/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 928/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0179 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 926/1000\n",
      "Epoch 934/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 929/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 935/1000\n",
      "Epoch 927/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 930/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0169 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0171 - accuracy: 0.0000e+00Epoch 936/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 928/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0220 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0167 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 931/1000\n",
      "Epoch 937/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0193 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 929/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 932/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 938/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0143 - accuracy: 0.0000e+00Epoch 930/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 933/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 931/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 939/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 934/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 932/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 940/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 935/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 933/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0247 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 941/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 936/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 934/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 942/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 937/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 935/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 938/1000\n",
      "Epoch 943/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 936/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 939/1000\n",
      "Epoch 944/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 937/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 940/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 945/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0194 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 938/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0154 - accuracy: 0.0000e+00Epoch 941/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 946/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 939/1000\n",
      "Epoch 942/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0172 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 947/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 940/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0122 - accuracy: 0.0000e+00Epoch 943/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0167 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0089 - accuracy: 0.0000e+00Epoch 948/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 941/1000\n",
      "Epoch 944/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 949/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 942/1000\n",
      "Epoch 945/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0181 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 950/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 943/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0095 - accuracy: 0.0312Epoch 946/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0171 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 951/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0433 - accuracy: 0.0000e+00Epoch 944/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0230 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 947/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 945/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0175 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0248 - accuracy: 0.0000e+00Epoch 952/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 948/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 946/1000\n",
      "Epoch 953/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 949/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 950/1000\n",
      "Epoch 954/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0311 - accuracy: 0.0000e+00Epoch 947/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 951/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0254 - accuracy: 0.0000e+00Epoch 955/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0121 - accuracy: 0.0000e+00Epoch 948/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0171 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0226 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 956/1000\n",
      "Epoch 952/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 949/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0171 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 957/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0129 - accuracy: 0.0312Epoch 953/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 950/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0167 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 958/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 954/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 951/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0167 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 959/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 955/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0193 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 952/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0168 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 960/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 956/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 961/1000\n",
      "Epoch 953/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 957/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 962/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 954/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 958/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 963/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 955/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 959/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0174 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 964/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 956/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 960/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0169 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 965/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0252 - accuracy: 0.0000e+00Epoch 961/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0193 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0208 - accuracy: 0.0000e+00Epoch 957/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 966/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 962/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0197 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 958/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0175 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 967/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 959/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 963/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0171 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 968/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 960/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 964/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0166 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 969/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 961/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 965/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0166 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 970/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 962/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 966/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0165 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 971/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 963/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 967/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 972/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 964/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 968/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0169 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 973/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 965/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 969/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0171 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 974/1000\n",
      "Epoch 966/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 970/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 967/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0160 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0193 - accuracy: 0.0000e+00Epoch 975/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 971/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 976/1000\n",
      "Epoch 968/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 972/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 969/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 977/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 973/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0198 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 978/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0192 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 974/1000\n",
      "Epoch 970/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0164 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 979/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 975/1000\n",
      "Epoch 971/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0161 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 980/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 972/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0201 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 976/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0164 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 981/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 973/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 977/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0164 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 982/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0241 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 974/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 978/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0187 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 983/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0217 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 975/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0165 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 984/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0090 - accuracy: 0.0000e+00Epoch 979/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 976/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0174 - accuracy: 0.0000e+00Epoch 985/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 980/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 986/1000\n",
      "Epoch 977/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 981/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0160 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 987/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0197 - accuracy: 0.0000e+00Epoch 978/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 982/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 979/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 983/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0212 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 988/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 980/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0202 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 984/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0189 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 989/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 981/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 985/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 990/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 982/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 986/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 991/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0188 - accuracy: 0.0000e+00Epoch 983/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 992/1000\n",
      "Epoch 987/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 984/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0163 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 988/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0178 - accuracy: 0.0000e+00Epoch 993/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0193 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 985/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 989/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0162 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 994/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0207 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 986/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 990/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0170 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0156 - accuracy: 0.0000e+00Epoch 995/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 987/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 991/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 996/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 988/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 992/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0173 - accuracy: 0.0085 - lr: 0.0010\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0349 - accuracy: 0.0000e+00Epoch 997/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0188 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 989/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0179 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 993/1000\n",
      " 1/12 [=>............................] - ETA: 0s - loss: 0.0170 - accuracy: 0.0000e+00Epoch 998/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0178 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0189 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 999/1000\n",
      "Epoch 990/1000\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.0195 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 994/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0166 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 1000/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 991/1000\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.0190 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 995/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0163 - accuracy: 0.0085 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 992/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 996/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 993/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 997/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 994/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 998/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 995/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 999/1000\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 996/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0184 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "Epoch 1000/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0233 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 997/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0197 - accuracy: 0.0000e+00 - lr: 0.0010\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0233 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 998/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 999/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 0.0085 - lr: 0.0010\n",
      "Epoch 1000/1000\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.0210 - accuracy: 0.0085 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0169\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0412 - accuracy: 0.0000e+00\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0363 - accuracy: 0.0000e+00\n",
      "Epoch 1/1000\n",
      "17/17 [==============================] - 3s 7ms/step - loss: 0.3725 - accuracy: 0.0019 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.2636 - accuracy: 0.0019 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.1516 - accuracy: 0.0019 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0690 - accuracy: 0.0019 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0529 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0477 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0442 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0418 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0400 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0385 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0371 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0368 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0364 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0355 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0358 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0351 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0347 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0343 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0346 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0336 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0339 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0342 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 23/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0338 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 24/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0331 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 25/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0329 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 26/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0326 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 27/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0329 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 28/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0327 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 29/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 30/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0325 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 31/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0319 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 32/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0326 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 33/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0320 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 34/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0318 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 35/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0317 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 36/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0318 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 37/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0326 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 38/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0314 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 39/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0323 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 40/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0317 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 41/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0317 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 42/1000\n",
      "17/17 [==============================] - 0s 8ms/step - loss: 0.0315 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 43/1000\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0311 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 44/1000\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0312 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 45/1000\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0321 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 46/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0322 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 47/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0309 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 48/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0315 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 49/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0315 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 50/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0307 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 51/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 52/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0305 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 53/1000\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0309 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 54/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0314 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 55/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0302 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 56/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 57/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0300 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 58/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0302 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 59/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0303 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 60/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0299 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 61/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0312 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 62/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0322 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 63/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0307 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 64/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 65/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0306 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 66/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0300 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 67/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0299 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 68/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0295 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 69/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0294 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 70/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 71/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0293 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 72/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0292 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 73/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0294 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 74/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0290 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 75/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0298 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 76/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 77/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0297 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 78/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 79/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 80/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 81/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 82/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 83/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0300 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 84/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0292 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 85/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 86/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0282 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 87/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0283 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 88/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0288 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 89/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 90/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0291 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 91/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 92/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 93/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0284 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 94/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0282 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 95/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 96/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0284 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 97/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0283 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 98/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 99/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0286 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 100/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0279 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 101/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 102/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 103/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 104/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 105/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 106/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0285 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 107/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0293 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 108/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0280 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 109/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 110/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0274 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 111/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 112/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 113/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0292 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 114/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0291 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 115/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0294 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 116/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0290 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 117/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0277 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 118/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 119/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 120/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 121/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 122/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 123/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 124/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 125/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 126/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 127/1000\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0265 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 128/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0272 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 129/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 130/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0271 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 131/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0283 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 132/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0286 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 133/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 134/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 135/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0282 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 136/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0276 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 137/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0274 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 138/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0273 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 139/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0271 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 140/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 141/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0275 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 142/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0289 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 143/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0280 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 144/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 145/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 146/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 147/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 148/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 149/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0267 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 150/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 151/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 152/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 153/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 154/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0269 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 155/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 156/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 157/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0260 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 158/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 159/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0262 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 160/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 161/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 162/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 163/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 164/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 165/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0260 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 166/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0259 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 167/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0258 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 168/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 169/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0270 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 170/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0265 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 171/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 172/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 173/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 174/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 175/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 176/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0259 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 177/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 178/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 179/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0259 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 180/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 181/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 182/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0261 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 183/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 184/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 185/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0264 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 186/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 187/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 188/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 189/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 190/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 191/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 192/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 193/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 194/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 195/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0253 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 196/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 197/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 198/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 199/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0256 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 200/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 201/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 202/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 203/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 204/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 205/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 206/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 207/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0256 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 208/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0249 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 209/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 210/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0260 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 211/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 212/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0259 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 213/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 214/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 215/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0255 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 216/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0255 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 217/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 218/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 219/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 220/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 221/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0257 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 222/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0256 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 223/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0254 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 224/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 225/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 226/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 227/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0245 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 228/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 229/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 230/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 231/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 232/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 233/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 234/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 235/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 236/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 237/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 238/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0258 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 239/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 240/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 241/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0245 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 242/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 243/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 244/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0244 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 245/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 246/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0256 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 247/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 248/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 249/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0253 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 250/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 251/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 252/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 253/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 254/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 255/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 256/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 257/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 258/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 259/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 260/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 261/1000\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0248 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 262/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 263/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 264/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 265/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 266/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 267/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0251 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 268/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 269/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 270/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0248 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 271/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0247 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 272/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 273/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 274/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 275/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 276/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 277/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0244 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 278/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 279/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 280/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0255 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 281/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 282/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0241 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 283/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0241 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 284/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 285/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 286/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0244 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 287/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0240 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 288/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 289/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0244 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 290/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 291/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0239 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 292/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 293/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 294/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 295/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 296/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0252 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 297/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 298/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0255 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 299/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 300/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 301/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 302/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 303/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 304/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 305/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 306/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 307/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 308/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 309/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 310/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 311/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0234 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 312/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 313/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 314/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 315/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0249 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 316/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 317/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 318/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 319/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0234 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 320/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 321/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 322/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 323/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 324/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 325/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0249 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 326/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 327/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 328/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 329/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 330/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 331/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0232 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 332/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 333/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 334/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 335/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 336/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0239 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 337/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 338/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0235 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 339/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0236 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 340/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 341/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 342/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 343/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 344/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 345/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0246 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 346/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0238 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 347/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0234 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 348/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0248 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 349/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0243 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 350/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0240 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 351/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 352/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 353/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 354/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0239 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 355/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 356/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0232 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 357/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0232 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 358/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 359/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 360/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 361/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 362/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 363/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 364/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0234 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 365/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 366/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 367/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0226 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 368/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0230 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 369/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0228 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 370/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0229 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 371/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 372/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 373/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0235 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 374/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0233 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 375/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 376/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0238 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 377/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 378/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0226 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 379/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 380/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0223 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 381/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0227 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 382/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0226 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 383/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0242 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 384/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0241 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 385/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 386/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 387/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 388/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 389/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0244 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 390/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 391/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0232 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 392/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 393/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 394/1000\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0223 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 395/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 396/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0231 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 397/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 398/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 399/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0233 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 400/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 401/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 402/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 403/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 404/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 405/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 406/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 407/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 408/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 409/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 410/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 411/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0231 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 412/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 413/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 414/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 415/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 416/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 417/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 418/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 419/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 420/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 421/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 422/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 423/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 424/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 425/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0224 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 426/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 427/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 428/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 429/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 430/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 431/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 432/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 433/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 434/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 435/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 436/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 437/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 438/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 439/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 440/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0235 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 441/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 442/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 443/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 444/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 445/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 446/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 447/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 448/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 449/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 450/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0223 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 451/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0229 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 452/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 453/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 454/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 455/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 456/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 457/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 458/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 459/1000\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 460/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 461/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 462/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 463/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 464/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 465/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 466/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0223 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 467/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 468/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 469/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 470/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 471/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 472/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 473/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 474/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 475/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0223 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 476/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 477/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 478/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 479/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0223 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 480/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 481/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 482/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 483/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 484/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 485/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 486/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 487/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 488/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 489/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 490/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 491/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 492/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 493/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 494/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 495/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 496/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 497/1000\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0224 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 498/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 499/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 500/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 501/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0226 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 502/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0236 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 503/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 504/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 505/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0250 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 506/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0253 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 507/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0237 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 508/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0236 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 509/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0231 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 510/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 511/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 512/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0220 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 513/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 514/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 515/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 516/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 517/1000\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 518/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 519/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 520/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 521/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0239 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 522/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0232 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 523/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 524/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 525/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 526/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 527/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 528/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 529/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0223 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 530/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0220 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 531/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 532/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 533/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 534/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 535/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 536/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 537/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0230 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 538/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0237 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 539/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0226 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 540/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 541/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0232 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 542/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0224 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 543/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 544/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 545/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0225 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 546/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 547/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0224 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 548/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 549/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 550/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 551/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 552/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 553/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 554/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 555/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0222 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 556/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 557/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 558/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 559/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 560/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 561/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 562/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 563/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 564/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 565/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 566/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 567/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 568/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0231 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 569/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 570/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 571/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0216 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 572/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0216 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 573/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 574/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 575/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0208 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 576/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 577/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 578/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0208 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 579/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0210 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 580/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0224 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 581/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 582/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0216 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 583/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 584/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 585/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 586/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 587/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 588/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 589/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 590/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 591/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 592/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 593/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 594/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 595/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 596/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 597/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0211 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 598/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 599/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 600/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 601/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 602/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 603/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 604/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 605/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 606/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 607/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 608/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 609/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 610/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 611/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 612/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 613/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 614/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 615/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 616/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0226 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 617/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 618/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0227 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 619/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0238 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 620/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0214 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 621/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0208 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 622/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0224 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 623/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 624/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 625/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 626/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 627/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 628/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 629/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 630/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 631/1000\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 632/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 633/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 634/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 635/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 636/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 637/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 638/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 639/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 640/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 641/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 642/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 643/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0213 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 644/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 645/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 646/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 647/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 648/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 649/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 650/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 651/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 652/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 653/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 654/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 655/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 656/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 657/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 658/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 659/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 660/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0203 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 661/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 662/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 663/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0221 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 664/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 665/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0220 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 666/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 667/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 668/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 669/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 670/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0203 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 671/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 672/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 673/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 674/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0216 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 675/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0211 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 676/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0210 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 677/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 678/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0207 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 679/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 680/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0212 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 681/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 682/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 683/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 684/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 685/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0211 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 686/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 687/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 688/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 689/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 690/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0220 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 691/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 692/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 693/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 694/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 695/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0215 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 696/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 697/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 698/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0205 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 699/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0208 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 700/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0202 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 701/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 702/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0202 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 703/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 704/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 705/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0203 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 706/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0200 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 707/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 708/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0219 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 710/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 711/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0223 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 712/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0216 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 713/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 714/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0201 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 715/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 716/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 717/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0210 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 718/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 719/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 720/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0203 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 721/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 722/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0202 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 723/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 724/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 725/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 726/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 727/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 728/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 729/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0198 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 730/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0204 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 731/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 732/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 733/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 734/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 735/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 736/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 737/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 738/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 739/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 740/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 741/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 742/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 743/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 744/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 745/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 746/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 747/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 748/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0199 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 749/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0201 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 750/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 751/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0197 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 752/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 753/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 754/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 755/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 756/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0211 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 757/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0202 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 758/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 759/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 760/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 761/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0202 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 762/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 763/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 764/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 765/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 766/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 767/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 768/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 769/1000\n",
      "17/17 [==============================] - 0s 5ms/step - loss: 0.0194 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 770/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 771/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 772/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0202 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 773/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 774/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0204 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 775/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 776/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 777/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 778/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 779/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 780/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 781/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 782/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0206 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 783/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 784/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 785/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 786/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0197 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 787/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 788/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 789/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0192 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 790/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 791/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0194 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 792/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0200 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 793/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 794/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0203 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 795/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 796/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 797/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 798/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0198 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 799/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 800/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 801/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 802/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0199 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 803/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 804/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0201 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 805/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 806/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0202 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 807/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 808/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 809/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 810/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 811/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 812/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0198 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 813/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0209 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 814/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0226 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 815/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0218 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 816/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 817/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 818/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 819/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 820/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 821/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 822/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0196 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 823/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 824/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 825/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0196 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 826/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 827/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 828/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 829/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0186 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 830/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 831/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 832/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 833/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 834/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 835/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 836/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0204 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 837/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 838/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 839/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 840/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 841/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 842/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 843/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 844/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 845/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 846/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 847/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0210 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 848/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 849/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 850/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0199 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 851/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 852/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 853/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 854/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 855/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 856/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 857/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 858/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 859/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0192 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 860/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 861/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 862/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 863/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0192 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 864/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 865/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0201 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 866/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0207 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 867/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 868/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 869/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 870/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 871/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 872/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 873/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 874/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 875/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 876/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0192 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 877/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 878/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 879/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0191 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 880/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0195 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 881/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 882/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 883/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 884/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 885/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 886/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 887/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0186 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 888/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 889/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 890/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 891/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 892/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0183 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 893/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 894/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 895/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 896/1000\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0188 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 897/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0184 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 898/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 899/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0188 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 900/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 901/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 902/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0184 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 903/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 904/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 905/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 906/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 907/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 908/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 909/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0199 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 910/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0194 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 911/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 912/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 913/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 914/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 915/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 916/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 917/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0179 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 918/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 919/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 920/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 921/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0179 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 922/1000\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0180 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 923/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 924/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0179 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 925/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0182 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 926/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 927/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 928/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 929/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0180 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 930/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 931/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0180 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 932/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 933/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 934/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0192 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 935/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 936/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0179 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 937/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 938/1000\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0187 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 939/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 940/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0182 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 941/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 942/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 943/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 944/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 945/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 946/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 947/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 948/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0190 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 949/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 950/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0178 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 951/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0176 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 952/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 953/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 954/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 955/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 956/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 957/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0179 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 958/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 959/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 960/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 961/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 962/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 963/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 964/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0197 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 965/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 966/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 967/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0193 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 968/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 969/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0182 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 970/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 971/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 972/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 973/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 974/1000\n",
      "17/17 [==============================] - 0s 4ms/step - loss: 0.0186 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 975/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0194 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 976/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0181 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 977/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 978/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 979/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0187 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 980/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0180 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 981/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0184 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 982/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0184 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 983/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 984/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0177 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 985/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0185 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 986/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0177 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 987/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0183 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 988/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0183 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 989/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0189 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 990/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 991/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 992/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0172 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 993/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0169 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 994/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0170 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 995/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0176 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 996/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0170 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 997/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0186 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 998/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0175 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 999/1000\n",
      "17/17 [==============================] - 0s 3ms/step - loss: 0.0174 - accuracy: 0.0056 - lr: 0.0010\n",
      "Epoch 1000/1000\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0177 - accuracy: 0.0056 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "from sklearn.metrics import make_scorer\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "# Define the model\n",
    "def create_model(optimizer, hidden_layers, neurons):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(neurons, input_shape=(13,), activation='relu'))\n",
    "    for i in range(hidden_layers):\n",
    "        model.add(tf.keras.layers.Dense(neurons, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the parameters for the grid search\n",
    "param_grid = {\n",
    "    'optimizer': ['adam'],\n",
    "    'hidden_layers': [3],\n",
    "    'neurons': [16]\n",
    "}\n",
    "\n",
    "\n",
    "#     'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "#     'hidden_layers': [10,13,15],\n",
    "#     'neurons': [64, 128, 256]\n",
    "\n",
    "        \n",
    "# Create the model\n",
    "model = KerasRegressor(build_fn=create_model, epochs=1000)\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "history = grid_search.fit(X_train, y_train, callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "189aa620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from keras.wrappers.scikit_learn import KerasRegressor\n",
    "\n",
    "# from sklearn.metrics import make_scorer\n",
    "# from keras.callbacks import EarlyStopping\n",
    "\n",
    "# early_stop = EarlyStopping(monitor='loss', patience=5)\n",
    "\n",
    "# # Define the model\n",
    "# def create_model(optimizer, hidden_layers, neurons):\n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(tf.keras.layers.Dense(neurons, input_shape=(13,), activation='relu'))\n",
    "#     for i in range(hidden_layers):\n",
    "#         model.add(tf.keras.layers.Dense(neurons, activation='relu'))\n",
    "#     model.add(tf.keras.layers.Dense(1))\n",
    "#     model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# # Define the parameters for the grid search\n",
    "# param_grid = {\n",
    "#     'optimizer': ['adam', 'sgd', 'rmsprop'],\n",
    "#     'hidden_layers': [3, 5, 10, 15],\n",
    "#     'neurons': [16, 32, 64, 128]\n",
    "# }\n",
    "\n",
    "# # Create the model\n",
    "# model = KerasRegressor(build_fn=create_model, epochs=1000)\n",
    "\n",
    "# # Perform the grid search with cross-validation\n",
    "# grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "# history = grid_search.fit(X_train, y_train, callbacks=[reduce_lr])\n",
    "\n",
    "# # Print the best parameters and results\n",
    "# print(\"Best Parameters:\", grid_search.best_params_)\n",
    "# print(\"Best Score (MSE):\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "54e671b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step\n",
      "Y Test:\n",
      "484    0.957990\n",
      "177    0.268387\n",
      "213    0.667498\n",
      "245    0.656730\n",
      "215    0.662361\n",
      "         ...   \n",
      "593    0.657372\n",
      "635    0.689133\n",
      "134    0.740529\n",
      "342    0.012349\n",
      "128    0.676043\n",
      "Name: w, Length: 134, dtype: float64\n",
      "Y Predictions:\n",
      "[ 0.70886886  0.6742857   0.69650495  0.62782     0.6824096   0.73545265\n",
      "  0.01778418  0.5684505   0.5869359   0.5392913   0.01499061  0.63132805\n",
      "  0.19591984  0.129668    0.70640296  0.6435435   0.49159473  0.86927605\n",
      "  0.8348254   0.67620814  0.65017277  0.6598451  -0.3200524   0.09673463\n",
      "  0.599638    0.43552637  0.65576446  0.7158083   0.576055   -0.00297921\n",
      "  0.55553734  0.65180624  0.6723437   0.59311473  0.60626996  0.6247215\n",
      "  0.1812439   0.9366733   0.6196428   0.7639147   0.37617058  0.5897447\n",
      "  0.15427755  0.03627904  0.5740591   0.04646906  0.68572336  0.4814998\n",
      " -0.19521429  0.7444046   0.39861548  0.18541443  0.32317683  0.59834623\n",
      "  0.5666162   0.627177    0.45020193  0.17032537  0.7648084   0.5879684\n",
      "  0.7218939   0.13118263  0.20015779  0.6773988   0.44330394  0.5943853\n",
      "  0.3036946   0.6190595   0.71758795  0.18081713  0.5628405   0.5894051\n",
      "  0.6341092   0.58401173  0.665043   -0.00361125  0.6086354   0.50221753\n",
      "  0.63030934  0.7356415   0.6641273   0.3949268   0.8475945   0.6924865\n",
      "  0.7129454   0.62578994  0.70649105  0.60692966  0.2988723   0.592028\n",
      "  0.66204834  0.6081849   0.39278507  0.63906884  0.37508196  0.40772504\n",
      "  0.21318673  0.59368116  0.64743406  0.6904363   0.6728329   0.07391481\n",
      "  0.6701069   0.70649105  0.16769548  0.12630028  0.64343     0.8230321\n",
      "  0.5666162   0.45982194  0.5829927   0.6989736  -0.02509136 -0.1725523\n",
      "  0.6347549   0.5760328   0.4349844   0.6626366   0.7387638   0.59659654\n",
      "  0.70092404  0.847548    0.00984244  0.59311473  0.37059373  0.09668711\n",
      "  0.81307447  0.63585746  0.41597176  0.57788545  0.66495883  0.7237241\n",
      "  0.02355179  0.71923184]\n",
      "Mean Squared Error: 0.037984576709159154\n",
      "R-squared Score: 0.47258639694024873\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Y Test:\")\n",
    "print(y_test)\n",
    "print(\"Y Predictions:\")\n",
    "print(y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b58f32e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_.model\n",
    "best_model.save('newbest_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "5814dffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484    0.957990\n",
       "177    0.268387\n",
       "213    0.667498\n",
       "245    0.656730\n",
       "215    0.662361\n",
       "         ...   \n",
       "593    0.657372\n",
       "635    0.689133\n",
       "134    0.740529\n",
       "342    0.012349\n",
       "128    0.676043\n",
       "Name: w, Length: 134, dtype: float64"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "3088f0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.70886886,  0.6742857 ,  0.69650495,  0.62782   ,  0.6824096 ,\n",
       "        0.73545265,  0.01778418,  0.5684505 ,  0.5869359 ,  0.5392913 ,\n",
       "        0.01499061,  0.63132805,  0.19591984,  0.129668  ,  0.70640296,\n",
       "        0.6435435 ,  0.49159473,  0.86927605,  0.8348254 ,  0.67620814,\n",
       "        0.65017277,  0.6598451 , -0.3200524 ,  0.09673463,  0.599638  ,\n",
       "        0.43552637,  0.65576446,  0.7158083 ,  0.576055  , -0.00297921,\n",
       "        0.55553734,  0.65180624,  0.6723437 ,  0.59311473,  0.60626996,\n",
       "        0.6247215 ,  0.1812439 ,  0.9366733 ,  0.6196428 ,  0.7639147 ,\n",
       "        0.37617058,  0.5897447 ,  0.15427755,  0.03627904,  0.5740591 ,\n",
       "        0.04646906,  0.68572336,  0.4814998 , -0.19521429,  0.7444046 ,\n",
       "        0.39861548,  0.18541443,  0.32317683,  0.59834623,  0.5666162 ,\n",
       "        0.627177  ,  0.45020193,  0.17032537,  0.7648084 ,  0.5879684 ,\n",
       "        0.7218939 ,  0.13118263,  0.20015779,  0.6773988 ,  0.44330394,\n",
       "        0.5943853 ,  0.3036946 ,  0.6190595 ,  0.71758795,  0.18081713,\n",
       "        0.5628405 ,  0.5894051 ,  0.6341092 ,  0.58401173,  0.665043  ,\n",
       "       -0.00361125,  0.6086354 ,  0.50221753,  0.63030934,  0.7356415 ,\n",
       "        0.6641273 ,  0.3949268 ,  0.8475945 ,  0.6924865 ,  0.7129454 ,\n",
       "        0.62578994,  0.70649105,  0.60692966,  0.2988723 ,  0.592028  ,\n",
       "        0.66204834,  0.6081849 ,  0.39278507,  0.63906884,  0.37508196,\n",
       "        0.40772504,  0.21318673,  0.59368116,  0.64743406,  0.6904363 ,\n",
       "        0.6728329 ,  0.07391481,  0.6701069 ,  0.70649105,  0.16769548,\n",
       "        0.12630028,  0.64343   ,  0.8230321 ,  0.5666162 ,  0.45982194,\n",
       "        0.5829927 ,  0.6989736 , -0.02509136, -0.1725523 ,  0.6347549 ,\n",
       "        0.5760328 ,  0.4349844 ,  0.6626366 ,  0.7387638 ,  0.59659654,\n",
       "        0.70092404,  0.847548  ,  0.00984244,  0.59311473,  0.37059373,\n",
       "        0.09668711,  0.81307447,  0.63585746,  0.41597176,  0.57788545,\n",
       "        0.66495883,  0.7237241 ,  0.02355179,  0.71923184], dtype=float32)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "f6d242dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03798457607626915, 0.0]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "b1563a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_13 (Dense)            (None, 16)                224       \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,057\n",
      "Trainable params: 1,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "SRAM required: 4.13 KB\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('newbest_model.h5')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Estimate the SRAM required based on the number of parameters and data type\n",
    "num_params = sum(tf.size(p, out_type=tf.dtypes.int64) for p in model.trainable_weights)\n",
    "sram_required = num_params * 4  # Assume float32 data type\n",
    "\n",
    "print(f'SRAM required: {sram_required / 1024:.2f} KB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "5b473fee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 3ms/step - loss: 0.0380 - accuracy: 0.0000e+00\n",
      "Test loss: 0.03798457607626915\n",
      "Test accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "be977674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'hidden_layers': 3, 'neurons': 16, 'optimizer': 'adam'}\n",
      "Best score found:  -0.034733125319083534\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters\n",
    "print(\"Best parameters found: \",grid_search.best_params_)\n",
    "\n",
    "# Print the best score\n",
    "print(\"Best score found: \",grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "165ec520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the best model from the grid search\n",
    "best_model = grid_search.best_estimator_.model\n",
    "\n",
    "# Train the best model to get its history\n",
    "history = best_model.fit(X_train, y_train, epochs=100, callbacks=[reduce_lr], verbose=0)\n",
    "\n",
    "# Plot the accuracy and loss of the training data\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.title('Model Accuracy During Grid Search')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "# plt.ylim(0, 1)\n",
    "plt.show()\n",
    "\n",
    "# Plot the loss of the training data\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model Loss During Grid Search')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "# plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "3228080c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = []\n",
    "for layer in best_model.layers:\n",
    "    if hasattr(layer, 'kernel'):\n",
    "        weights.append(layer.kernel)\n",
    "\n",
    "# Concatenate all the weights into a single 1D array\n",
    "weights = np.concatenate([w.numpy().flatten() for w in weights])\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(weights, bins=100)\n",
    "plt.xlabel('Weight Value')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62994aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c594ac28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72baad08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ba9c122",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c99c1122",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5.74394822e-01,  6.51806176e-01,  7.61782825e-02,  7.64779925e-01,\n",
       "        7.32106566e-01,  6.58279836e-01,  3.06567609e-01,  7.79234290e-01,\n",
       "        6.06064618e-01,  2.09738702e-01,  6.56103671e-01,  5.69050312e-01,\n",
       "        6.37449741e-01,  7.30078220e-01,  3.62903774e-01,  7.23641872e-01,\n",
       "        2.15553641e-01,  6.14256144e-01,  1.31767347e-01,  6.27965450e-01,\n",
       "        6.06312513e-01,  5.65805078e-01,  7.35202134e-01,  8.53527069e-01,\n",
       "        8.47594500e-01,  3.72175932e-01,  6.56103671e-01,  7.35253811e-01,\n",
       "        7.51971006e-01,  2.13186726e-01,  9.37894136e-02,  5.01971066e-01,\n",
       "        8.19589019e-01,  2.98872292e-01,  6.46582425e-01,  7.23498702e-01,\n",
       "        6.33776903e-01,  3.75081837e-01,  7.15098143e-01,  1.74613565e-01,\n",
       "        7.12944746e-01,  1.97475463e-01,  1.95919842e-01, -1.64125875e-01,\n",
       "        6.63887739e-01,  5.96849501e-01,  7.79234290e-01,  6.38523698e-01,\n",
       "        6.03201330e-01,  4.89664078e-01,  6.15557671e-01,  9.36420202e-01,\n",
       "        7.59873986e-01,  3.52848232e-01,  1.82493970e-01,  3.54557455e-01,\n",
       "        8.09870541e-01,  6.58820450e-01,  6.33993566e-01,  7.42629528e-01,\n",
       "        5.74059129e-01,  8.79327059e-01,  7.93830395e-01,  6.88592911e-01,\n",
       "        8.22671890e-01,  6.36582255e-01,  6.53104305e-01,  6.05102718e-01,\n",
       "       -2.97921151e-03,  7.23673850e-02,  8.43954980e-01,  7.39640653e-01,\n",
       "        5.01971066e-01,  7.15710342e-01,  4.17875797e-02,  7.58251548e-01,\n",
       "        8.80933583e-01,  7.01738358e-01,  3.02317381e-01,  3.02317381e-01,\n",
       "        1.50595367e-01,  5.80563784e-01,  7.30602980e-01,  5.65532327e-01,\n",
       "        2.94399083e-01,  3.03694606e-01,  6.74938202e-01,  4.21062469e-01,\n",
       "        6.90100551e-01,  7.46853530e-01,  4.32839155e-01,  5.26402712e-01,\n",
       "        4.85529900e-01,  4.14497375e-01,  6.55779362e-01,  5.96849442e-01,\n",
       "        5.66226691e-02,  1.35360941e-01,  6.37770236e-01,  2.58439556e-02,\n",
       "        6.09220922e-01,  5.89504659e-01,  7.81479031e-02,  5.52906990e-01,\n",
       "        8.82806182e-02,  6.47506177e-01, -2.38823891e-03,  5.95060647e-01,\n",
       "        6.66635871e-01,  7.34049439e-01,  6.79541707e-01,  9.49792266e-01,\n",
       "        2.26810798e-02,  1.66130513e-02,  3.43080521e-01,  3.76170576e-01,\n",
       "        6.18867040e-01, -5.78239560e-04,  5.83393812e-01,  3.56827080e-02,\n",
       "        1.85414433e-01,  8.44693184e-01,  5.86900890e-01,  1.80817127e-01,\n",
       "        7.16085196e-01,  6.04931712e-01,  5.42942762e-01,  5.92948318e-01,\n",
       "        2.55943656e-01,  6.23141170e-01,  5.61336458e-01,  1.36436298e-02,\n",
       "        9.51756090e-02,  8.19589019e-01,  3.83726209e-02,  5.79799116e-01,\n",
       "       -2.48500109e-02,  5.78957856e-01,  5.47695994e-01,  7.21893907e-01,\n",
       "        7.00883567e-01,  4.63415682e-02, -7.03781843e-03,  2.31419414e-01,\n",
       "        2.00157762e-01,  4.63415682e-02,  6.65543437e-01,  5.77008128e-01,\n",
       "        1.10445723e-01,  7.29801595e-01,  5.88629901e-01,  4.97211814e-01,\n",
       "        5.89759111e-01,  7.01704264e-01,  6.71354830e-01,  5.40670812e-01,\n",
       "        7.01704264e-01,  5.23073375e-01,  7.06310630e-01,  7.14464962e-01,\n",
       "        6.86524510e-01,  2.57113099e-01,  7.46975005e-01,  5.24602085e-02,\n",
       "        8.57972026e-01,  6.73880279e-01,  3.85373533e-01,  2.00092793e-02,\n",
       "        5.42942822e-01,  5.74352741e-01,  7.65150428e-01,  9.36673284e-01,\n",
       "        9.34703946e-02,  4.12474155e-01,  5.57723522e-01,  6.38167202e-01,\n",
       "        7.04990745e-01,  7.06605911e-01,  3.85373533e-01,  6.61649704e-01,\n",
       "        7.28085160e-01,  1.14214346e-01,  7.93332696e-01,  6.61976695e-01,\n",
       "        7.35452652e-01,  5.92374146e-01,  6.45480633e-01,  6.81773543e-01,\n",
       "        6.39738798e-01,  4.90293130e-02,  5.72059870e-01,  7.66253173e-01,\n",
       "        5.91851950e-01,  4.84573722e-01,  3.88341427e-01,  7.63440311e-01,\n",
       "        7.63440311e-01,  6.04931712e-01,  3.09277594e-01,  7.41457522e-01,\n",
       "        6.79108202e-01,  5.92303395e-01,  6.70106888e-01,  5.95621943e-01,\n",
       "        7.89907038e-01,  5.98780096e-01,  7.56774306e-01,  5.65413356e-01,\n",
       "        2.31419414e-01,  4.33325887e-01,  6.32643461e-01,  5.23073375e-01,\n",
       "        7.15912580e-01,  5.96690476e-01,  1.41335875e-02,  6.23410702e-01,\n",
       "        2.24332735e-01,  6.99151516e-01,  6.34754896e-01,  6.12223446e-01,\n",
       "        4.63038802e-01,  9.62241888e-01,  8.44693184e-01,  7.28784800e-01,\n",
       "        6.28901005e-01,  8.01690519e-01,  7.71502972e-01,  5.89405119e-01,\n",
       "        7.54067183e-01,  1.50039852e-01,  6.63887739e-01,  9.37892944e-02,\n",
       "        1.64942980e-01,  5.76032817e-01,  5.96596539e-01,  5.87266326e-01,\n",
       "        7.73224056e-01,  7.65042722e-01,  6.73616886e-01,  1.40597433e-01,\n",
       "        8.43954980e-01,  7.44845867e-01,  7.92337894e-01, -7.69982338e-02,\n",
       "        6.90232933e-01,  6.15740299e-01,  1.74613565e-01,  8.47241431e-02,\n",
       "       -9.05733854e-02,  5.93055725e-01,  3.23176831e-01,  6.18850052e-01,\n",
       "        6.64762974e-01,  3.34874690e-01,  1.97475582e-01,  6.64127409e-01,\n",
       "        7.01752961e-01,  1.35688141e-01,  6.33546561e-02,  7.81810522e-01,\n",
       "        6.43422484e-01,  6.44244492e-01,  7.29307711e-01,  6.89897716e-01,\n",
       "        5.77008128e-01,  6.14256144e-01,  6.60363615e-01,  1.85445547e-02,\n",
       "        5.77939808e-01,  6.66564584e-01,  3.94322723e-02,  1.40346065e-02,\n",
       "        7.24265039e-01,  1.82493970e-01,  8.79327059e-01,  7.66253293e-01,\n",
       "        9.49792266e-01,  6.66470170e-01,  9.36420202e-01,  1.65497869e-01,\n",
       "        6.63958669e-01,  7.29741573e-01,  3.29873338e-02,  6.26296699e-02,\n",
       "        6.17618799e-01,  5.91481268e-01,  5.75806677e-01,  3.98615420e-01,\n",
       "        6.62048340e-01,  6.20555162e-01,  6.10887229e-01,  7.47407734e-01,\n",
       "        5.85572243e-01,  5.62278092e-01, -9.27135348e-03,  7.03164116e-02,\n",
       "        1.77841783e-02,  7.41457522e-01,  8.54529142e-02,  6.22644067e-01,\n",
       "        5.53543121e-02,  2.94916987e-01,  3.13042998e-01,  7.64779925e-01,\n",
       "        8.01690519e-01,  7.84355640e-01,  6.09220922e-01,  6.51377738e-01,\n",
       "        6.55188560e-01,  8.82806182e-02,  6.05933964e-01,  1.66019440e-01,\n",
       "        2.31438726e-02,  7.99142420e-01,  6.38904333e-01,  6.94513619e-01,\n",
       "        6.10579133e-01,  6.78957999e-01,  3.13043177e-01,  5.27879894e-01,\n",
       "        6.65452302e-01,  1.71238482e-02,  6.15866601e-01,  8.23032081e-01,\n",
       "        5.90380073e-01,  7.47407734e-01,  6.76858306e-01,  7.41225004e-01,\n",
       "        6.88971400e-01,  6.18046045e-01,  5.54734051e-01,  2.14936808e-01,\n",
       "        6.98010385e-01,  5.19164264e-01,  6.69415712e-01,  6.30309343e-01,\n",
       "        6.58647776e-01,  3.43080521e-01,  7.13210478e-02,  6.38904333e-01,\n",
       "        7.08868861e-01,  6.37449741e-01,  5.88247478e-01,  4.40474987e-01,\n",
       "        6.83394134e-01,  6.26255751e-01,  7.39148110e-02,  7.91324854e-01,\n",
       "        6.26084685e-01,  4.33325887e-01,  6.26346350e-01,  2.06064299e-01,\n",
       "        5.57017326e-03,  6.98572814e-01,  3.56827080e-02,  1.40346065e-02,\n",
       "        6.19059503e-01,  6.53282583e-01,  6.76439464e-01,  6.58345163e-01,\n",
       "        8.55995774e-01,  5.78957856e-01,  5.69572031e-01,  6.81106269e-01,\n",
       "        6.12223446e-01,  6.53212547e-01,  7.73987234e-01,  6.94092810e-01,\n",
       "        9.84244049e-03,  2.59284675e-01,  1.19711012e-02,  4.87985313e-01,\n",
       "        3.03540945e-01,  6.00711703e-01,  6.35450542e-01,  7.91324854e-01,\n",
       "        6.03895187e-01,  7.35253811e-01,  5.87580144e-01,  2.37298563e-01,\n",
       "        5.99637985e-01,  7.47284114e-01,  3.42033654e-02,  6.10826731e-01,\n",
       "        7.23301053e-01,  6.74938202e-01,  6.15740299e-01,  6.21971548e-01,\n",
       "        9.62241888e-01,  7.00043678e-01,  4.10251319e-02,  4.96038020e-01,\n",
       "        5.95403790e-01,  3.80074739e-01,  6.41936541e-01,  5.96690476e-01,\n",
       "        3.70593727e-01,  6.09360933e-01,  6.36582255e-01,  5.87006569e-01,\n",
       "        6.96032465e-01,  7.05349684e-01,  5.93068004e-01,  3.06567609e-01,\n",
       "        7.09193051e-01,  5.39291322e-01,  7.67294228e-01,  5.29289544e-01,\n",
       "        6.56425893e-01,  6.18046045e-01,  7.77156472e-01,  7.00062275e-01,\n",
       "        5.24441779e-01,  6.88821435e-01,  6.58647537e-01,  6.08751416e-01,\n",
       "        7.71502972e-01,  3.10242385e-01,  6.69415712e-01,  5.56774557e-01,\n",
       "        7.15710342e-01,  7.69262731e-01,  5.92028022e-01,  5.90788126e-01,\n",
       "        5.39381862e-01,  7.37466693e-01,  7.77565122e-01,  6.71437740e-01,\n",
       "        6.06064618e-01,  1.13482930e-01,  2.11536646e-01,  6.83239043e-01,\n",
       "        6.88592911e-01,  6.95578039e-01,  7.06605911e-01,  7.69262731e-01,\n",
       "        5.59570909e-01,  7.34141946e-01,  6.27183437e-01,  4.96417165e-01,\n",
       "        7.02156961e-01,  8.57972026e-01,  5.83393812e-01,  5.95707119e-01,\n",
       "        4.97211814e-01,  6.32813811e-01,  5.94932973e-01,  5.99452555e-01,\n",
       "        3.72175932e-01,  6.48141444e-01,  2.49387488e-01,  2.35517919e-02,\n",
       "        4.15971756e-01,  5.79733133e-01,  7.23498702e-01,  8.80933583e-01,\n",
       "        6.79541707e-01,  6.30983412e-01,  5.74138761e-01,  5.96507549e-01,\n",
       "        7.00924039e-01,  6.18975520e-01,  2.31389150e-01,  7.18080163e-01,\n",
       "        5.68199694e-01,  5.47959864e-01,  6.15719676e-01,  6.20640755e-01,\n",
       "        7.54123211e-01,  6.95968986e-01,  8.55995774e-01,  6.14834011e-01,\n",
       "        7.57575512e-01, -3.61125171e-03,  3.02740633e-02,  6.33910656e-01,\n",
       "        7.61782825e-02,  4.73603159e-02,  5.62793374e-01,  2.79888839e-01,\n",
       "        3.04709911e-01,  7.89907038e-01,  5.26434064e-01,  5.85928917e-01,\n",
       "        7.10097671e-01,  5.94692886e-01,  6.91145003e-01,  4.57765013e-02,\n",
       "        6.03105962e-01,  7.65042722e-01,  3.54557455e-01,  6.18900895e-01,\n",
       "        2.39080176e-01,  7.32806981e-01, -6.67622089e-02,  4.87985313e-01,\n",
       "        7.39640653e-01,  1.59717426e-02,  6.40737295e-01,  8.44057381e-01,\n",
       "        8.09870541e-01,  2.26804614e-02,  6.16254926e-01,  6.20648801e-01,\n",
       "        5.02532944e-02,  5.32431424e-01,  6.90436304e-01,  7.47284114e-01,\n",
       "        5.85928917e-01,  6.63717031e-01,  6.66984618e-01,  6.76439404e-01,\n",
       "        7.25320876e-01,  1.50039852e-01, -2.10340470e-02,  5.87136090e-01,\n",
       "        2.95939445e-01,  3.04709911e-01,  6.19750798e-01,  5.94385326e-01,\n",
       "        6.62276626e-01,  5.83600104e-02,  3.09277594e-01,  5.26402712e-01,\n",
       "        6.98245585e-01,  5.66226691e-02,  7.04990745e-01,  6.13311946e-01,\n",
       "        3.20904493e-01,  7.36065686e-01,  4.63939428e-01,  1.14214413e-01,\n",
       "        6.32035971e-01], dtype=float32)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_train)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "e1120bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157    0.678760\n",
       "447    0.206644\n",
       "551    0.152507\n",
       "475    0.948506\n",
       "588    0.736305\n",
       "         ...   \n",
       "255    0.163003\n",
       "197    0.702297\n",
       "638    0.524105\n",
       "446    0.003729\n",
       "184    0.703828\n",
       "Name: w, Length: 533, dtype: float64"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3d553e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484    0.957990\n",
       "177    0.268387\n",
       "213    0.667498\n",
       "245    0.656730\n",
       "215    0.662361\n",
       "         ...   \n",
       "593    0.657372\n",
       "635    0.689133\n",
       "134    0.740529\n",
       "342    0.012349\n",
       "128    0.676043\n",
       "Name: w, Length: 134, dtype: float64"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "a6b98f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.70886886,  0.6742857 ,  0.69650495,  0.62782   ,  0.6824096 ,\n",
       "        0.73545265,  0.01778418,  0.5684505 ,  0.5869359 ,  0.5392913 ,\n",
       "        0.01499061,  0.63132805,  0.19591984,  0.129668  ,  0.70640296,\n",
       "        0.6435435 ,  0.49159473,  0.86927605,  0.8348254 ,  0.67620814,\n",
       "        0.65017277,  0.6598451 , -0.3200524 ,  0.09673463,  0.599638  ,\n",
       "        0.43552637,  0.65576446,  0.7158083 ,  0.576055  , -0.00297921,\n",
       "        0.55553734,  0.65180624,  0.6723437 ,  0.59311473,  0.60626996,\n",
       "        0.6247215 ,  0.1812439 ,  0.9366733 ,  0.6196428 ,  0.7639147 ,\n",
       "        0.37617058,  0.5897447 ,  0.15427755,  0.03627904,  0.5740591 ,\n",
       "        0.04646906,  0.68572336,  0.4814998 , -0.19521429,  0.7444046 ,\n",
       "        0.39861548,  0.18541443,  0.32317683,  0.59834623,  0.5666162 ,\n",
       "        0.627177  ,  0.45020193,  0.17032537,  0.7648084 ,  0.5879684 ,\n",
       "        0.7218939 ,  0.13118263,  0.20015779,  0.6773988 ,  0.44330394,\n",
       "        0.5943853 ,  0.3036946 ,  0.6190595 ,  0.71758795,  0.18081713,\n",
       "        0.5628405 ,  0.5894051 ,  0.6341092 ,  0.58401173,  0.665043  ,\n",
       "       -0.00361125,  0.6086354 ,  0.50221753,  0.63030934,  0.7356415 ,\n",
       "        0.6641273 ,  0.3949268 ,  0.8475945 ,  0.6924865 ,  0.7129454 ,\n",
       "        0.62578994,  0.70649105,  0.60692966,  0.2988723 ,  0.592028  ,\n",
       "        0.66204834,  0.6081849 ,  0.39278507,  0.63906884,  0.37508196,\n",
       "        0.40772504,  0.21318673,  0.59368116,  0.64743406,  0.6904363 ,\n",
       "        0.6728329 ,  0.07391481,  0.6701069 ,  0.70649105,  0.16769548,\n",
       "        0.12630028,  0.64343   ,  0.8230321 ,  0.5666162 ,  0.45982194,\n",
       "        0.5829927 ,  0.6989736 , -0.02509136, -0.1725523 ,  0.6347549 ,\n",
       "        0.5760328 ,  0.4349844 ,  0.6626366 ,  0.7387638 ,  0.59659654,\n",
       "        0.70092404,  0.847548  ,  0.00984244,  0.59311473,  0.37059373,\n",
       "        0.09668711,  0.81307447,  0.63585746,  0.41597176,  0.57788545,\n",
       "        0.66495883,  0.7237241 ,  0.02355179,  0.71923184], dtype=float32)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_test = best_model.predict(X_test)\n",
    "y_pred_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "977f64ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step\n",
      "Mean Absolute Error:  0.1312273708274972\n",
      "Mean Squared Error:  0.037984576709159154\n",
      "Root Mean Squared Error:  0.19489632297495804\n",
      "R2 Score:  0.47258639694024873\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculating MAE\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error: \", mae)\n",
    "\n",
    "# Calculating MSE\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error: \", mse)\n",
    "\n",
    "# Calculating RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error: \", rmse)\n",
    "\n",
    "# Calculating the R2 score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R2 Score: \", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb2739b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "7d98b9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03798457607626915, 0.0]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_.model\n",
    "best_model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "14410143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 13) dtype=float32 (created by layer 'dense_13_input')>]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea59dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c8724e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc32b9aa",
   "metadata": {},
   "source": [
    "## Model Quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5b65fbe9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpza9rtl3v/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpza9rtl3v/assets\n",
      "2023-04-03 15:38:47.590689: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-04-03 15:38:47.590794: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-04-03 15:38:47.671798: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpza9rtl3v\n",
      "2023-04-03 15:38:47.678150: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-04-03 15:38:47.678233: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpza9rtl3v\n",
      "2023-04-03 15:38:47.738865: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2023-04-03 15:38:47.741560: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-04-03 15:38:48.188734: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpza9rtl3v\n",
      "2023-04-03 15:38:48.203332: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 550558 microseconds.\n",
      "2023-04-03 15:38:48.953498: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2c8498e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6916"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "095811af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpnnm3wusv/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpnnm3wusv/assets\n",
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
      "2023-04-03 15:38:52.449657: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-04-03 15:38:52.449695: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-04-03 15:38:52.449921: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpnnm3wusv\n",
      "2023-04-03 15:38:52.451991: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-04-03 15:38:52.452033: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpnnm3wusv\n",
      "2023-04-03 15:38:52.460692: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-04-03 15:38:52.541043: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpnnm3wusv\n",
      "2023-04-03 15:38:52.560427: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 110507 microseconds.\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_quant_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e0c0879e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6920"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "16caf5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tflite_model.tflite\", \"wb\") as f:\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "61a928f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tflite_quant_model.tflite\", \"wb\") as f:\n",
    "  f.write(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "58a5b14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size = 6KBs.\n"
     ]
    }
   ],
   "source": [
    "tflite_model_size = len(tflite_model) / 1024\n",
    "print('Model size = %dKBs.' % tflite_model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "10a0ee57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size = 6KBs.\n"
     ]
    }
   ],
   "source": [
    "tflite_model_size = len(tflite_quant_model) / 1024\n",
    "print('Quantized model size = %dKBs.' % tflite_model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2b7f944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xxd -i tflite_quant_model.tflite > tflite_quant_model_data.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f5e4257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the saved model from file\n",
    "# fin_model = tf.keras.models.load_model('tflite_quant_model.tflite')\n",
    "\n",
    "# # Call the summary method to see the model architecture\n",
    "# fin_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da09147d",
   "metadata": {},
   "source": [
    "### Saving Model Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d4b087ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract and save the weights and biases\n",
    "# weights_to_cpp(model, \"quantized_weights_and_biases.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76378fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77246f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import binascii\n",
    "\n",
    "## Output the quantized tflite model to a c-style header\n",
    "def convert_to_c_array(bytes) -> str:\n",
    "  hexstr = binascii.hexlify(bytes).decode(\"UTF-8\")\n",
    "  hexstr = hexstr.upper()\n",
    "  array = [\"0x\" + hexstr[i:i + 2] for i in range(0, len(hexstr), 2)]\n",
    "  array = [array[i:i+10] for i in range(0, len(array), 10)]\n",
    "  return \",\\n  \".join([\", \".join(e) for e in array])\n",
    "\n",
    "tflite_binary = open('tflite_quant_model.tflite', 'rb').read()\n",
    "ascii_bytes = convert_to_c_array(tflite_binary)\n",
    "header_file = \"const unsigned char model_tflite[] = {\\n  \" + ascii_bytes + \"\\n};\\nunsigned int model_tflite_len = \" + str(len(tflite_binary)) + \";\"\n",
    "with open(\"model.h5\", \"w\") as f:\n",
    "    f.write(header_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ddc05b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import binascii\n",
    "\n",
    "## Output the quantized tflite model to a c-style header\n",
    "def convert_to_c_array(bytes) -> str:\n",
    "  hexstr = binascii.hexlify(bytes).decode(\"UTF-8\")\n",
    "  hexstr = hexstr.upper()\n",
    "  array = [\"0x\" + hexstr[i:i + 2] for i in range(0, len(hexstr), 2)]\n",
    "  array = [array[i:i+10] for i in range(0, len(array), 10)]\n",
    "  return \",\\n  \".join([\", \".join(e) for e in array])\n",
    "\n",
    "tflite_binary = open('tflite_model.tflite', 'rb').read()\n",
    "ascii_bytes = convert_to_c_array(tflite_binary)\n",
    "header_file = \"const unsigned char model_tflite[] = {\\n  \" + ascii_bytes + \"\\n};\\nunsigned int model_tflite_len = \" + str(len(tflite_binary)) + \";\"\n",
    "with open(\"modelold.h\", \"w\") as f:\n",
    "    f.write(header_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a26e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33ac7333",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load the TensorFlow Lite model\n",
    "# interpreter = tf.lite.Interpreter(model_path='tflite_model.tflite')\n",
    "\n",
    "# # Convert the TensorFlow Lite model to a TensorFlow model\n",
    "# converter = tf.lite.TFLiteConverter.from_lite_model_file('model.tflite')\n",
    "# tf_model = converter.convert()\n",
    "\n",
    "# # Prune the TensorFlow model\n",
    "# pruned_model = tf.keras.models.Model(inputs=tf_model.inputs, outputs=tf_model.outputs)\n",
    "# pruned_model.compile(optimizer='adam',\n",
    "#                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#                      metrics=['accuracy'])\n",
    "\n",
    "# # Set the pruning hyperparameters\n",
    "# pruning_params = {\n",
    "#   'pruning_schedule': tf.keras.optimizers.sparsity.PolynomialDecay(initial_sparsity=0.50,\n",
    "#                                                                      final_sparsity=0.90,\n",
    "#                                                                      begin_step=2000,\n",
    "#                                                                      end_step=8000)\n",
    "# }\n",
    "\n",
    "# # Compile the pruned model\n",
    "# pruned_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f574cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the pruned model to the data\n",
    "# pruned_model.fit(train_images, train_labels, epochs=4, validation_split=0.1)\n",
    "\n",
    "# # Convert the pruned TensorFlow model back to TensorFlow Lite\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n",
    "# tflite_model = converter.convert()\n",
    "\n",
    "# # Save the pruned TensorFlow Lite model\n",
    "# with open('pruned_model.tflite', 'wb') as f:\n",
    "#     f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca778888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# # Load the model\n",
    "# model = tf.keras.models.load_model('model.h5')\n",
    "\n",
    "# # Prune the model\n",
    "# pruning_params = {\n",
    "#   'pruning_schedule': tf.keras.optimizers.schedules.ConstantSchedule(0.5),\n",
    "# }\n",
    "\n",
    "# pruned_model = tf.keras.models.model_from_json(model.to_json())\n",
    "\n",
    "# pruning_layer = tf.keras.layers.SpatialPruning(**pruning_params)\n",
    "# pruned_model = pruning_layer(pruned_model)\n",
    "\n",
    "# # Compile the pruned model\n",
    "# pruned_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Save the pruned model\n",
    "# pruned_model.save('pruned_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "69604a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fit the pruned model to the data\n",
    "# pruned_model.fit(train_images, train_labels, epochs=4, validation_split=0.1)\n",
    "\n",
    "# # Convert the pruned TensorFlow model back to TensorFlow Lite\n",
    "# converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n",
    "# tflite_model = converter.convert()\n",
    "\n",
    "# # Save the pruned TensorFlow Lite model\n",
    "# with open('pruned_model.tflite', 'wb') as f:\n",
    "#     f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "107b046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the TensorFlow Lite model.\n",
    "# interpreter = tf.lite.Interpreter(model_path=\"tflite_quant_model.tflite\")\n",
    "\n",
    "# # Allocate memory for the model.\n",
    "# interpreter.allocate_tensors()\n",
    "\n",
    "# # Get the tensor details of the model.\n",
    "# input_details = interpreter.get_input_details()\n",
    "# output_details = interpreter.get_output_details()\n",
    "\n",
    "# # Extract the weight and bias values of the model.\n",
    "# weights = [interpreter.get_tensor(i[\"index\"]) for i in input_details]\n",
    "# biases = [interpreter.get_tensor(i[\"index\"]) for i in output_details]\n",
    "\n",
    "# # Do something with the weights and biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f5d0fcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpreter = tf.lite.Interpreter(model_path=\"tflite_quant_model.tflite\")\n",
    "# interpreter.allocate_tensors()\n",
    "\n",
    "# # Get the tensor details of all the tensors in the model.\n",
    "# all_tensor_details = interpreter.get_tensor_details()\n",
    "\n",
    "# weights = []\n",
    "# biases = []\n",
    "# for tensor_details in all_tensor_details:\n",
    "#   # Get the tensor data.\n",
    "#   tensor = interpreter.get_tensor(tensor_details[\"index\"])\n",
    "  \n",
    "#   # Check if the tensor is a weight or a bias.\n",
    "#   # You can do this by checking the shape of the tensor, or by checking its name if the model has tensor names.\n",
    "#   if tensor.ndim == 2:\n",
    "#     weights.append(tensor.flatten())\n",
    "#   elif tensor.ndim == 1:\n",
    "#     biases.append(tensor.flatten())\n",
    "\n",
    "# # Combine the weights and biases into single 1D arrays.\n",
    "# weights = np.concatenate(weights)\n",
    "# biases = np.concatenate(biases)\n",
    "\n",
    "# # Do something with the weights and biases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7b1c8a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of input tensor: 52\n",
      "Memory usage of output tensor: 4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"tflite_quant_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get the input and output tensor details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Get the memory usage of input and output tensors\n",
    "input_tensor_data = interpreter.get_tensor(input_details[0][\"index\"])\n",
    "output_tensor_data = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "\n",
    "print(\"Memory usage of input tensor:\", input_tensor_data.nbytes)\n",
    "print(\"Memory usage of output tensor:\", output_tensor_data.nbytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "48119810",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69147"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e469bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_flat = np.concatenate([weight.flatten() for weight in weights])\n",
    "\n",
    "with open(\"weights.txt\", \"w\") as f:\n",
    "    for weight in weights_flat:\n",
    "        f.write(f\"{weight}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8467179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# # Load the TensorFlow Lite model into a TensorFlow Keras model\n",
    "# converter = tf.lite.TFLiteConverter.from_file(\"tflite_model.tflite\")\n",
    "# model = converter.convert()\n",
    "\n",
    "# # Set up pruning rules\n",
    "# pruning_params = {\n",
    "#   \"pruning_schedule\": tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50, final_sparsity=0.90, begin_step=2000, end_step=8000)\n",
    "# }\n",
    "\n",
    "# # Prune the TensorFlow Keras model\n",
    "# pruned_model = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)\n",
    "\n",
    "# # Convert the pruned TensorFlow Keras model back to TensorFlow Lite model\n",
    "# pruned_tflite_model = tf.lite.TFLiteConverter.from_keras_model(pruned_model).convert()\n",
    "\n",
    "# # Save the pruned TensorFlow Lite model\n",
    "# with open(\"pruned_tf_quant_model.tflite\", \"wb\") as f:\n",
    "#   f.write(pruned_tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc10a29b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0692e53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5da54f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b24accd0",
   "metadata": {},
   "source": [
    "## Model Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "1a56cf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_to_cpp(model, filename=\"weights_and_biases.txt\"):\n",
    "    model.summary()\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for l in range(len(model.layers)):\n",
    "        W, B = model.layers[l].get_weights()\n",
    "        weights.append(W.flatten())\n",
    "        biases.append(B.flatten())\n",
    "    \n",
    "    z = []\n",
    "    b = []\n",
    "    for i in np.array(weights):\n",
    "        for l in i:\n",
    "            z.append(l)\n",
    "    for i in np.array(biases):\n",
    "        for l in i:\n",
    "            b.append(l)\n",
    "    with open(filename, \"w\") as f:\n",
    "      f.write(\"weights: {\")\n",
    "      for i in range(len(z)):\n",
    "        if (i < len(z)-1):\n",
    "          f.write(str(z[i])+\", \")\n",
    "        else:\n",
    "          f.write(str(z[i]))\n",
    "      f.write(\"}\\n\\n\")\n",
    "\n",
    "      f.write(\"biases: {\")\n",
    "      for i in range(len(b)):\n",
    "        if (i < len(b)-1):\n",
    "          f.write(str(b[i])+\", \")\n",
    "        else:\n",
    "          f.write(str(b[i]))\n",
    "      f.write(\"}\\n\\n\")\n",
    "    \n",
    "      arch = []\n",
    "    \n",
    "      arch.append(model.layers[0].input_shape[1])\n",
    "      for i in range(1, len(model.layers)):\n",
    "          arch.append(model.layers[i].input_shape[1])\n",
    "      arch.append(model.layers[len(model.layers)-1].output_shape[1])\n",
    "      f.write(\"Architecture: {\")\n",
    "      for i in range(len(arch)):\n",
    "          if (i < len(arch)-1):\n",
    "              f.write(str(arch[i])+\", \")\n",
    "          else:\n",
    "              f.write(str(arch[i]))\n",
    "      f.write(\"}\")\n",
    "      print(\"Architecture (alpha):\", arch)\n",
    "      print(\"Layers:\", len(arch))\n",
    "    print(\"Weights: \", z)\n",
    "    print(\"Biases: \", b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "4de0885e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_13 (Dense)            (None, 16)                224       \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 16)                272       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,057\n",
      "Trainable params: 1,057\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Architecture (alpha): [13, 16, 16, 16, 16, 1]\n",
      "Layers: 6\n",
      "Weights:  [0.37744573, -1.975618, 0.34271008, 0.36747003, 0.22167443, 0.43038625, -0.31267166, -0.41646576, -1.3227768, 0.49677336, -0.1161232, -0.42988172, -0.16588292, 0.14681195, -0.044621557, -2.536467, -0.391296, 0.10750384, 0.033209443, -0.0241099, 0.5231453, 0.16819344, -0.15570402, 0.15094732, -0.015659206, -0.33716857, 0.15223545, 0.45578966, 0.100363046, 0.248147, 0.28430703, -0.067956716, -0.053975664, 0.0008686177, 0.08312839, -0.3165061, 0.47757366, -0.39588106, -0.28590804, 0.043262362, 0.445677, -0.28884894, -0.120381474, -0.10890532, 0.14704984, -0.033850867, -0.4352022, -0.3006125, -0.35928884, 0.3119569, -0.3293259, -0.26154083, 0.47302255, 0.12583975, -0.24285977, -0.22041328, 0.18132246, 0.6205009, -0.1902596, 0.3317355, 0.3145977, 0.1238346, 0.11108615, 0.3878982, 0.3378823, 0.14047259, -0.28860795, -0.1877662, -0.45228952, -0.06310246, 0.18077809, -0.33560473, -0.31519136, -0.24704343, -0.15806174, 0.1502041, -0.012650427, 0.29948828, -0.1105924, 0.37415233, -0.3978164, 0.20559138, 0.06292027, -0.35990143, 0.29314974, -0.05686634, -0.12487453, 0.13252476, 0.17631862, -0.14889689, -0.05089301, -0.33605283, 0.46988136, 0.42896667, -0.4334911, 0.29654628, 0.16995381, -0.35115334, -0.32162967, 0.15313774, -0.17933764, 0.19571239, -0.078693, -0.20972048, -0.38121837, -0.2654721, -0.3239782, 0.38261926, -0.3782034, 0.12796476, 0.29379496, 0.18960771, -0.09306376, 0.43770075, -0.09980196, -0.2630093, 0.58211815, -0.2945069, 0.27543294, -0.016554447, -0.0031139106, -0.13402528, 0.07747191, 0.011111191, -0.5092743, 0.4453265, 0.5820741, 0.07766105, -0.14005077, 0.4512747, -0.14540005, 0.17599678, -0.23469272, 0.017765863, -0.24836756, -0.44628546, 0.40333426, 0.01921699, 0.144876, 0.09305196, -0.67744637, -0.10684979, 0.43626404, -0.5866427, 0.026800666, 0.29916757, 0.17226917, 0.046866238, 0.6767545, -0.33278972, -0.38994628, 0.33057812, -0.18315996, 0.14101586, -0.19728771, -0.17671084, -0.13754323, -0.23283422, -0.070808604, 0.34785262, 0.25248903, -0.27736723, -0.108933836, 0.22227156, -0.41214195, 0.1516484, 0.4295249, 0.2816327, -0.492655, 0.4316985, 0.260395, 0.64328283, 0.34257618, -0.19171087, -0.21995562, -0.24781695, -0.34355012, -0.027048621, -0.35588464, -0.4182679, 0.055277944, 0.6039729, -0.16325116, -0.28908116, 0.046115164, -0.39426976, -0.26557937, -0.48301136, 0.39756757, 0.3674015, -0.35137293, -0.011315636, -0.3392198, -0.3017802, -0.009950429, -0.062057376, -0.27043104, 0.4650979, -0.28907764, -0.016802775, 0.36934823, 0.2276609, -0.013303161, -0.26526618, -0.38493043, 0.05849674, -0.33244878, 0.103962906, -0.22794335, -0.40367186, 0.3087168, 0.24374679, -0.37014207, 0.36171186, -0.3949833, 0.3039604, 0.07261309, 0.31329778, -0.26879075, 0.32043964, -0.103223085, 0.061571926, -0.10544285, 0.13033053, 0.09515075, -0.379843, -0.18909514, -0.83768755, 0.21341966, 0.8772677, -0.7942877, -0.033926457, -0.58557194, -0.11813254, -0.28694668, 0.8810744, -0.2318181, -0.24212162, 0.052500777, 0.32614154, 0.42012885, -0.18847334, 0.3354703, -0.15416539, -0.047388524, -0.4079807, -0.18275735, -0.026388526, 0.3243893, 0.04869625, 0.43033335, 0.27614513, -0.016862214, -0.3579499, -0.37196976, -0.36246246, 0.06343073, -0.22842872, 0.21646443, 0.08756462, -0.015222281, 0.065247506, 0.19235006, 0.08777359, 0.035092026, 0.32384184, -0.16356277, 0.34772268, 0.29258046, -0.1658464, 0.027330875, -0.09806934, 0.025897412, 0.2148455, 0.011321634, 0.5359521, -1.2813292, -0.07248107, 0.55467546, -0.31054288, -0.28408802, -0.029663898, -0.31797278, -0.40085465, 0.28322482, 0.04220197, 0.61985105, 0.2552107, 0.4261492, 0.23020874, 0.18813923, -0.4219767, 0.18303381, 0.39414778, 0.1624088, -0.08611569, 0.30653316, -0.01979711, 0.07131199, -0.31483185, -0.56254953, -0.06282224, -0.5588588, -0.15307777, 0.2770935, -0.41545114, 0.3990884, 0.15927693, -0.14864278, 0.12460527, 0.02037561, 0.1858168, 0.13622251, -0.20055883, 0.42376927, 0.40487346, 0.19663379, 0.41132542, 0.2512587, 0.3973669, 0.22818227, 0.28611815, 0.0010028481, 0.065302275, -0.36131352, 0.2583334, 0.039772652, 0.3705735, 0.2296357, -0.41139418, -0.19332212, -0.3813982, -0.3353888, 0.4124274, -0.2637721, 0.14740106, -0.26391703, -0.0637311, -0.10247648, 0.115096815, -0.76274425, 0.8027365, -0.08126488, 0.033398718, 0.25352076, -0.5306756, 0.19149281, 0.63994193, -0.22744367, -0.03315454, -0.35725203, 0.6216274, -0.59598297, 0.5992419, -0.23668116, -0.41410202, 0.5356434, -0.17821775, 0.18582202, -0.030225486, 0.28218594, 0.6849291, 0.3905747, 0.1462737, 0.2690141, 0.19427791, -0.2092341, -0.42732707, -0.4006888, -0.0016408563, 0.30780843, -0.16511351, 0.38491324, -0.25637293, -0.17989454, -0.2967834, -0.317793, -0.17103979, 0.122644395, -0.43167555, 0.0050283372, 0.20316061, 0.03563136, 0.16196176, 0.3332973, 0.44253787, -0.41446593, 0.37748307, -0.657039, 0.2294385, -0.24023648, -0.32839248, -0.2664046, 0.17583828, 0.19817202, 0.5178204, -0.114565894, -0.07371802, 0.39043, 0.2199449, 0.7703735, 0.9777068, -0.016144723, -0.11731876, -0.28625506, 0.7853486, -0.41040435, -0.42790982, -0.2468561, -0.30496466, -0.13160703, 0.7654438, -0.029093273, 0.24927463, -1.0453113, 0.48338056, 0.29851434, 0.021686437, -0.41647837, 0.049545817, 0.0983199, -0.12801117, 0.41966593, -0.38523898, 0.45349124, -0.03246142, -0.15330409, -0.12656188, -0.033150434, -0.29560763, -0.30152473, -0.14928705, -0.006501534, -0.18441962, -0.04549679, -0.054660168, -0.04820136, 0.6915045, -0.0052326624, -0.40682703, -0.6315302, -0.37137344, 0.07085167, 0.28184712, 0.6838676, -0.24922809, 0.26791406, 0.52576303, 0.2011752, 0.2995075, -0.0032186508, 0.99493426, 0.52531326, 0.38549525, 0.54284835, 0.33676502, -0.8820437, -0.65121996, -0.5063936, 0.19663882, 0.9273376, 0.109894656, 0.17586361, -0.56334263, 0.18667553, 0.3068146, -0.24142125, 0.23090601, 0.20967068, 0.4409226, 0.41188663, -0.13633722, -0.3783502, 0.23242849, -0.42429402, -0.102036, 0.009731233, 0.0045569884, -0.30046648, -0.29227063, 0.38650385, 0.14811403, 0.32653326, 0.06841128, 0.31737328, 0.37056208, 0.018523358, -0.030337354, -0.0089214975, -0.37205333, 0.12980948, -0.26038036, -0.189094, -0.43951273, 0.51776963, -0.14489342, -0.06378031, -0.13621238, -0.20333304, -0.0992668, -0.052333742, -0.1541431, -0.10020089, -0.13907063, -0.13199571, 0.08895847, -0.4129977, -0.3335427, 0.21441683, -0.1265952, 0.056735307, -0.21755101, -0.46276987, -0.2737155, 0.16238189, 0.14396282, 0.13200724, 0.032468367, -0.6518774, -0.076428026, 0.19213934, -0.08982578, -0.36625743, 0.22287399, -0.2849815, -0.0077564907, 0.47268617, 0.037853308, -0.19709836, 0.37425083, -0.094466835, 0.048118804, 0.10843161, -0.076065645, 0.6036783, -0.027498307, -0.25976348, -0.40898275, 1.6913424, 0.047465563, 0.120817035, 0.16639872, -0.3097558, -0.39418346, 0.14530832, 0.466355, 0.23981243, 0.35497168, 0.016384011, 0.36906803, -0.2971848, 0.17145221, 0.007177192, -0.105638616, 0.26158464, 0.12971492, -0.27767462, -0.52402455, -0.22877495, 0.37717947, 0.026416877, -0.07207956, 0.32332674, -0.46284068, 0.19486621, 0.25969395, -0.19143078, -0.24730901, -0.5520171, -0.3463238, -0.6549132, -0.6064103, -0.36243188, 0.20266911, 0.48325202, 0.29712692, 0.4025418, -0.30307215, -0.00069364905, 0.17429397, -0.24480891, 0.06571311, -0.0036757886, -0.36641628, 0.386241, -0.042579502, -0.05011505, 0.30893013, -0.33931768, -0.3724109, -0.1734179, 0.22401538, -0.23033392, -0.4208277, -0.39308873, -0.351434, 0.035066865, -0.089680694, 0.42955992, 0.12816007, 0.05676581, 0.1490138, 0.09478358, -0.9046956, -0.25336593, 0.43358147, 0.29861355, 0.35242763, -0.17685352, -0.0648064, 0.53949344, 0.80850923, -1.0974424, -0.6450039, -0.9254696, 0.22401479, 0.3258303, 0.22352138, -0.21563771, 0.30691835, 0.14385054, 0.014209706, -0.6717639, 0.04073033, 0.51687396, 0.21205164, 0.053434703, 0.098947935, 0.058152363, 0.2474677, -0.30534357, -0.17621432, 0.106893316, -0.24470313, -0.20489456, 0.10974929, -0.33786386, 0.06363944, 0.47149375, 0.36940798, 0.21118312, 0.3372803, 0.16348152, 0.73843837, -0.11192965, -0.2676442, 0.5450947, -0.5269318, 0.04933149, -0.39169404, 1.0010315, 0.44734585, -0.18713062, -0.07007569, 0.025696058, 0.061422236, -1.1707927, 0.9453924, -0.25896817, 0.45476273, -0.28595138, -0.74620676, 0.7870334, 0.4784686, 0.55555224, -0.3411699, 0.20786804, 0.8928466, -0.3785451, 0.8837963, -1.1266474, -1.4515709, -0.017074773, 0.32578397, -0.39241084, 0.07188192, 0.051522072, -0.26242232, -0.15241666, -0.2688852, 0.09012845, -0.13607484, 0.3823163, -0.062065598, -0.24075, 0.34828028, -0.30753094, -0.2038993, 0.42789644, -1.461187, 0.6326645, -0.2180716, -0.14151613, 0.76504767, 0.29317808, 0.2746202, 0.52560925, -0.3489255, 0.2688792, -0.15677525, -0.4211022, 0.3796758, -0.64337605, 0.31820816, 0.60428065, 0.7635516, 0.24783047, 1.2932074, -0.94702715, 0.7222486, 0.4705261, -0.24751152, -0.19975513, 0.16778395, -0.4132725, 0.5262189, -0.31320003, 0.6711057, -0.79294044, -0.052549075, -0.2766986, 0.012175849, -0.7378471, -0.12761097, -0.019622296, 0.6367653, 0.13951267, -0.14211792, 0.5947303, 0.2719509, -0.29754505, 0.055572085, 0.51872694, 0.2733946, -0.26446295, 0.17042644, 0.12146307, -1.3090088, 0.3188173, -0.24727875, -0.36630073, -0.09498196, -0.086714014, -0.2584919, -0.26521072, 0.21345669, 0.08239528, -0.048640575, 0.50638247, 0.20215158, 0.1276648, -1.1764394, 0.2576092, 0.13741107, -0.099884674, 0.360865, -0.33873892, -0.35361624, 0.42142728, -0.56407607, 0.34200814, 0.3085444, -0.10782048, -0.3648182, 0.25249562, -0.11373913, -0.6409077, -0.30692306, -0.23177736, -0.94991904, 0.44300744, -0.46065053, -0.18787435, 0.26018962, 0.28973147, 0.6043834, 0.043903187, 0.09481417, 0.49084005, 0.23008159, 0.0977139, -0.30732945, 0.49729708, -0.916627, -0.29000008, 0.4155359, 0.2200432, 0.020642413, -0.24890985, 0.047031313, -0.2928782, 0.16967845, 0.13587944, -0.15780959, 0.23314358, 0.28149077, 0.49418545, -0.4292927, -0.1659438, -0.18992993, -0.13165502, -0.055589564, -0.067707136, 0.23994942, -0.2647853, 0.028170912, -0.5221727, 0.122030586, 0.33051217, -0.36990115, -0.37928596, -0.0055585424, 0.5219413, 0.12449192, 0.16279379, 0.32973596, 0.15425688, -1.2659422, 0.37183094, 0.02589301, -0.06728575, 0.046837986, -0.41721335, 0.5709954, -0.25103256, -0.1342087, 0.64358974, 1.1798122, -0.42312753, -0.30008847, 0.7836674, -1.1309087, -0.09015748, -0.3329067, 0.36647165, -0.190862, 0.3797271, 0.18316337, -0.4297629, -0.49631906, -0.010398642, 0.58794135, -0.2991145, 0.42946216, -0.29676014, 0.1547476, 0.5671954, 0.36104846, -0.29423165, -0.20664176, -0.5935632, 0.38894662, 0.021783799, -0.18847775, -0.8079755, 0.11148809, 0.016988723, 0.18345188, -0.37429446, 0.5157463, 0.036478836, -0.3077982, 0.14195913, 0.14848791, -0.41444817, 0.30974206, -0.22315526, 0.35579595, 0.4138753, 0.23632465, -0.37465498, -0.28244507, -0.24418606, -0.30029547, -0.3874609, -0.34425864, -0.1059116, -0.025711617, 0.105716415, -0.40055174, 0.13343206, 1.7247021, -0.61231, -0.13985616, -0.040920347, -0.20300452, 0.66773534, -0.3995359, 0.3946514, -0.63937104, -2.2653098, -1.9772017, 0.66376203, 0.18115635, -0.5186848, 1.4468372, -0.455797, -0.29994673, -0.69673157, 0.12187382, -0.015515178, -0.923283, -0.18684234, -0.6229904, 0.4036448, -0.2684288, -1.4454534, -0.638851, -0.05285033, 0.06355887, -0.4143191, -0.12545139, 0.3943573, 0.1928756, -0.14331299, -0.19527148, 0.24209538, -0.3028154, 0.032272995, -0.40665132, -0.4151191, 0.09848717, 0.083170325, -0.41746575, 0.18357304, 0.061403334, -0.17898604, 0.100319415, -0.18554191, 0.72742224, -0.5160248, 0.18834756, 0.35068378, -0.41785103, 0.3679239, -0.2861159, -0.3804883, -0.20954937, -1.2357599, -0.52095056, 0.091491766, 0.30209622, 0.39687753, 0.67944753, -0.24242504, 0.26306984, -0.15627047, -0.27554336, -0.29027805, -0.5326936, 0.04487782, -0.088607125, 0.110791065, -0.251664, 0.35538092, 0.16274047, 0.46041882, -0.08840607, 0.074489936, 0.52229047, -0.2676533, 0.06749545, -0.21834478, -0.4532138, 0.40654382, -1.195049, -0.3463419, 0.0601418, 0.18495883, -0.285219, -0.44752657, -0.034678515, -0.22992934, -0.2593738, -0.25684002, 0.35418996, -0.31401157, 0.80265784, -0.5171602, -0.27458173, -0.30160555, -0.43954524, -0.8370899, -0.5091825, 0.068309925, -0.17601638, -1.0906916, -0.18014944, 0.31128782, -0.4565776, -0.5047798, 0.6576818]\n",
      "Biases:  [-0.016398966, -0.1159133, 0.0, 0.0, -0.00035698657, -0.15729229, 0.0, -0.0026061719, 0.050456237, 0.18308727, 0.0, -0.04166867, 0.080085166, -0.08525059, 0.20133582, -0.07090731, -0.09753101, -0.016535325, 0.0, 0.070504725, -0.089041434, -0.05656308, -0.00568247, 0.0, 0.13853216, -0.1349743, 0.019823767, 0.04516028, 0.0075938064, -0.028343549, -0.08955264, -0.10335885, 0.046775635, -0.0371664, -0.08606679, 0.02094636, 0.075214125, -0.033210773, 0.017462015, -0.052028697, -0.2960243, -0.015191593, 0.095562, -0.07043441, 0.0, -0.1325552, 0.090032555, -0.0106325885, -0.031438436, 0.06176706, 0.13765262, -0.14307632, 0.0, -0.16398881, 0.040267486, 0.0026020173, -0.07511298, -0.07429017, 0.03455117, -0.05724263, 0.10309279, -0.021617522, 0.05632395, 0.06202442, 0.115279905]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5141/1168452030.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  for i in np.array(weights):\n",
      "/tmp/ipykernel_5141/1168452030.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  for i in np.array(biases):\n"
     ]
    }
   ],
   "source": [
    "weights_to_cpp(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9961d764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a2303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce737628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324685fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weights:  {0.44759887, 0.26112598, 0.19550373, 0.38450304, -0.19942415, 0.45077077, 0.2448067, -0.386346, 0.46039426, -0.2633852, -0.14693251, 0.029998004, 0.26927653, 0.268861, 0.37914807, 0.3704555, -0.07572216, -0.042278253, -0.14979152, 0.29536772, 0.49309924, 0.37326008, 0.00046372414, 0.47503603, 0.32581848, 0.43288672, -0.3540966, -0.1936517, -0.11599537, 0.1667245, -0.3148607, 0.20089751, 0.092566215, 0.06071432, -0.24219085, -0.09162846, -0.5084259, 0.14774743, 0.31585282, -0.08556196, 0.10712649, 0.09112793, -0.30268672, -0.45085737, -0.41424358, 0.047639906, 0.05202987, 0.35616368, -0.2703103, -0.24902382, 0.041463565, -0.061657786, 0.22187664, -0.34530216, -0.3934287, -0.36733574, -0.38012403, -0.16677065, -0.053679526, 0.43864357, -0.4154656, 0.4304685, 0.07993248, -0.13074203, 0.12888436, -0.036506694, 0.32494897, -0.31653085, -0.36264944, 0.3528421, -0.17461953, 0.13220939, 0.39646652, 0.20851894, -0.019526392, 0.09696001, 0.06380371, -0.35118878, -0.426249, 0.40496346, -0.34380582, 0.07100102, -0.38804695, -0.040148407, 0.23159923, 0.15813844, -0.37390524, 0.17289972, 0.10609063, -0.17268555, 0.06189567, -0.32680768, 0.022088762, -0.08627853, 0.17724726, -0.41947386, 0.4210028, -0.1836859, 0.04904299, 0.25903898, 0.31834427, 0.4578152, -0.058038563, -0.002519777, 0.29601577, 0.2961235, -0.3746404, -0.047587097, 0.30654174, -0.45040092, 0.30664313, -0.06207041, -0.09798511, -0.11341463, -0.1514373, 0.3445458, 0.45595068, -0.4810476, -0.27885717, 0.18512517, -0.41754735, -0.29962814, -0.39868295, 0.05973792, 0.22532357, -0.27841341, 0.18161725, -0.048847284, 0.34014702, -0.18959798, 0.117888406, 0.070061885, 0.2788736, 0.40783605, 0.11325693, -0.254121, -0.17190634, 0.3321252, -0.029944003, -0.087050885, 0.1324138, -0.43203956, 0.31291795, 0.12336364, -0.11117587, 0.07192834, 0.009900805, 0.03137907, -0.49520105, 0.4535007, -0.2572078, -0.40488908, -0.16645037, -0.5343041, 0.39391053, -0.02171126, 0.17656171, -0.15160504, 0.16841581, 0.49074534, -0.17471635, -0.3432138, 0.16525906, -0.165446, 0.13862231, 0.40364408, -0.37934005, 0.02963117, 0.35496774, 0.095929, -0.372563, -0.35069808, 0.3303583, 0.037219375, -0.14582172, -0.2525083, -0.10763638, 0.16154183, -0.11295057, -0.061643895, 0.43975067, -0.21803278, 0.32702, -0.12130152, 0.3496741, -0.34129575, 0.32079864, 0.2906974, -0.1515139, -0.012985647, -0.37638408, -0.052449837, 0.4458725, 0.0061466787, -0.45774415, -0.008519617, -0.307965, -0.19297275, -0.31642652, -0.19321935, -0.3291787, 0.08191305, 0.004797578, -0.13665739, -0.49111062, 0.19636655, -0.25531828, -0.31688347, 0.47196248, 0.32355192, 0.20902279, 0.3112196, 0.43596178, -0.05564092, -0.14077523, 0.30767316, 0.43513697, -0.113756895, -0.31695974, 0.29345435, 0.26461303, -0.36592096, 0.26633766, 0.17245153, -0.041229345, -0.027896315, 0.038393877, -0.3020825, -0.054103546, 0.3098337, 0.1596362, 0.23726203, -0.29712313, 0.4142346, 0.05283237, 0.22813997, -0.12960184, 0.13803495, 0.07180905, 0.26340675, 0.38117263, -0.08164537, 0.43203846, -0.024661599, 0.20000453, -0.18640284, 0.11821291, 0.26020557, 0.03204769, -0.36509743, -0.053893566, 0.29659337, 0.05707522, 0.19432987, -0.04479003, 0.28470367, 0.23291002, 0.19639978, -0.14109278, -0.21565437, 0.27283663, 0.24233216, 0.3085396, -0.09090227, 0.19044013, -0.04312471, 0.034525216, 0.07380149, 0.40742847, -0.21997173, 0.019312918, 0.011853863, 0.38569817, -0.28840387, 0.13556284, -0.45393866, -0.29995206, 0.08828668, 0.37306303, -0.2746134, -0.24900253, 0.23540772, 0.29394466, 0.277063, 0.1894801, -0.16643192, 0.36473098, -0.07669113, -0.22162597, -0.40824562, -0.41862172, 0.17643447, 0.1966648, -0.0002266562, -0.40385905, 0.15684351, -0.06061603, 0.2499609, -0.13045806, 0.34226373, 0.2690665, 0.18673617, -0.46849674, 0.11902086, 0.016728938, -0.07938784, 0.28770122, 0.19902173, 0.22100493, -0.06810433, 0.1290628, 0.13327566, 0.08334455, -0.2965982, 0.017937332, -0.26935083, 0.03471428, 0.28953233, -0.05818799, 0.11195615, 0.20734233, 0.2196348, -0.09718876, -0.5834251, 0.38531673, 0.22808559, -0.2612435, 0.5030446, -0.2815397, 0.2517689, -0.38203168, 0.10125503, 0.20754668, -0.36382973, -0.14705686, 0.3969837, 0.28017312, -0.08722457, -0.34494075, 0.020110937, 0.19241102, 0.20064493, -0.27107576, 0.031061407, 0.23471314, 0.4549788, 0.58557665, -0.0003850263, 0.36960444, -0.26625076, -0.51288474, 0.079593495, -0.5983781, -0.21176781, -0.19259585, 0.009720276, 0.2746066, 0.50323, -0.0009934196, 0.08717135, -0.2943992, 0.021770792, -0.064242974, 0.41019413, -0.4306294, 0.2905776, 0.23266059, -0.03637711, 0.37455973, -0.0015815198, 0.15391669, 0.2888122, -0.29814202, 0.08297381, -0.31042674, 0.41936675, -0.26684988, 0.33009312, 0.19664279, 0.40105382, -0.16222316, -0.22456162, 0.29910263, 0.37260923, 0.06044528, -0.01838848, -0.050979257, 0.28717467, 0.17817977, 0.40560582, -0.06459215, -0.3856986, -0.21999384, 0.41012767, -0.15943757, -0.017224789, -0.036639154, 0.19490239, 0.27925465, 0.008757383, -0.4452172, 0.35223857, -0.2472858, -0.43410018, -0.2080677, 0.28695366, 0.12456915, 0.26228538, -0.04149093, -0.29102713, 0.49308243, 0.4016333, -0.085267276, -0.34446967, -0.17495099, 0.20967399, 0.29945305, 0.19103333, 0.13558963, -0.32422835, -0.040523082, -0.02306509, 0.23367777, -0.24160357, 0.3075079, -0.26983273, 0.39587095, -0.16280562, 0.36339346, -0.03964722, 0.015698731, 0.15249595, -0.00070445397, 0.1524668, -0.11847758, -0.1738075, 0.075317144, -0.48222595, -0.04644501, 0.59837437, 0.29791924, 0.12247077, -0.7154112, -0.36226326, 0.071440764, -0.08112632, -0.15290013, 0.034271654, 0.11259378, -0.29861918, 0.26767704, 0.016259236, 0.23289526, 0.051014327, -0.1515653, 0.09247838, 0.24871539, 0.33078384, -0.122993134, -0.27578476, -0.3279433, 0.09664347, 0.25459492, 0.4040854, 0.11511583, -0.13497949, 0.29888114, -0.24022262, 0.23651557, 0.038583618, 0.46229473, -0.036186963, 0.3677836, -0.12789693, -0.31247535, -0.17041521, -0.4304748, 0.5295371, 0.055404365, -0.4086501, -0.24134062, -0.42159775, -0.14025518, -0.081751585, -0.027701706, 0.13719717, 0.32062563, 0.24583843, 0.3414453, -0.1387536, -0.15082988, 0.15909174, 0.3604909, -0.371939, -0.3968285, -0.13827765, -0.2322572, -0.2657127, -0.21212699, -0.24976249, 0.27215794, 0.3237354, 0.3767799, -0.27271265, 0.16966453, 0.25580397, 0.040379353, 0.41800568, -0.31843215, -0.17347819, 0.014401227, 0.3021203, -0.060318537, -0.048098207, -0.5589322, 0.23693058, -0.44455808, -0.25537091, -0.21261063, 0.21812996, 0.034982305, 0.37052456, -0.15407735, 0.13526282, -0.18129548, 0.044708773, 0.06190622, -0.14457715, -0.32758442, -0.13635236, -0.2876538, 0.37029412, 0.1343901, 0.41402796, -0.059141513, 0.25220355, -0.21531965, -0.07742694, -0.4066954, -0.0227284, -0.015428036, -0.2221884, -0.24528009, -0.33628514, 0.31445625, -0.36821437, -0.21178643, 0.36540708, -0.16557093, -0.18278907, 0.450546, 0.15378794, 0.12915452, 0.19851783, -0.13851357, -0.12038481, -0.27356344, -0.41524592, -0.26777363, -0.3755446, 0.2112199, 0.22854319, -0.2990754, -0.56463283, -0.34145775, -0.5795823, -0.27696845, 0.07253161, 0.49518445, 0.09707698, 0.040343042, -0.31551474, -0.11517343, -0.19470854, -0.21316752, -0.28635973, -0.5390974, 0.33772138, -0.36905995, 0.22066972, -0.021684978, 0.13798207, -0.16069216, -0.16286641, -0.52394795, -0.0042509437, 0.03968489, 0.3107012, 0.09121749, 0.1206612, -0.11100748, -0.29995376, -0.13950214, -0.23029362, -0.25070104, -0.16429275, -0.086811624, 0.30146742, 0.031549156, -0.33354056, 0.17032494, 0.26757768, -0.200491, -0.00090107863, -0.2901271, 0.4243973, 0.28986904, 0.012711018, 0.3508076, -0.27917343, -0.013979964, 0.1442383, 0.3352953, 0.2941281, -0.1533978, -0.18816651, 0.13350429, -0.3557186, 0.02258807, -0.32375672, -0.41981822, 0.2576408, -0.3276289, -0.3613995, -0.081474915, -0.07325345, 0.30683896, -0.31139854, -0.72283083, -0.75612557, -0.792728, 0.23041007, 0.0022757645, -0.32267678, 0.1332188, -0.025418907, -0.09939751, 0.15293464, -0.27142012, 0.40048823, 0.20222484, -0.023111343, 0.08558474, -0.3536536, 0.3100568, -0.38939667, 0.3017752, -0.2761243, -0.045572147, -0.26040107, 0.27275568, -0.21172154, -0.2956991, -0.20331356, -0.009608179, 0.074226946, 0.24657452, -0.400386, -0.08890807, 0.10537624, 0.41662297, 0.24220136, 0.45587516, 0.024240464, 0.049872417, 0.10172656, -0.09309574, -0.28023872, 0.38266173, -0.06960219, -0.15003484, 0.019260347, 0.2238774, 0.30011424, 0.31607506, 0.16796046, -0.09831715, 0.37770593, 0.3497869, 0.333998, -0.05485693, -0.13809451, -0.40974247, -0.24166974, 0.05319032, -0.048627548, 0.053993523, 0.41041055, -0.15478118, -0.05563584, -0.6624043, 0.40723473, 0.049923107, -0.28633454, -0.21623963, -0.16637775, 0.2825473, 0.42784777, 0.26013, 0.09428391, -0.0796828, -0.31705564, 0.40290686, 0.34004095, 0.21231246, 0.19719574, 0.07400686, 0.20079464, 0.008107407, -0.0016578616, -0.35292214, -0.22944148, 0.18441026, -0.2169002, -0.30413875, 0.33759236, 0.18632814, -0.08034641, 0.41376433, -0.3882779, 0.21929538, 0.3166705, 0.13352056, -0.26905707, -0.22560722, 0.4391875, -0.011828887, 0.32394457, 0.19694602, -0.64988595, 0.32477728, 0.16700463, 0.31163648, 0.4101571, 0.3686005, 0.019007567, -0.038861662, -0.22453478, -0.4125073, 0.33987817, -0.27378118, -0.083400846, 0.42401436, -0.025404245, 0.41970024, -0.18196923, 0.39915588, -0.3658052, 0.113589436, 0.05820605, -0.29174185, 0.35935882, -0.5575594, -0.2775817, 0.23159763, 0.4729945, 0.47253346, -0.016774328, 0.2816483, -0.2697992, -0.32252863, 0.23233294, -0.38225675, -0.32972378, 0.28506437, 0.18521477, 0.11343432, 0.10704726, 0.22042497, -0.017679444, -0.37340096, 0.013211998, 0.1286615, -0.12641913, -0.17802808, 0.1693047, 0.20178472, 0.10275729, -0.044183224, 0.28609568, -0.05994082, 0.39072037, -0.6068621, 0.17155552, 0.13697252, -0.12502912, -0.21400736, -0.17833732, 0.31137574, 0.23712502, 0.4518487, 0.37553197, 0.37717977, -0.011793757, 0.306617, -0.353249, -0.2214089, -0.018686464, -0.20961286, -0.07246013, 0.06842855, -0.097199924, -0.4272713, -0.1581237, 0.26071185, 0.22946179, 0.34878597, 0.06731686, 0.34106788, 0.37236014, -0.039320465, -0.27374876, 0.20071207, 0.3088001, -0.36248896, 0.18512641, 0.16801065, 0.20628482, 0.17492835, 0.41282555, 0.090453744, 0.2760219, 0.4980631, -0.29069874, 0.30598998, -0.19210401, 0.3027288, -0.43643844, 0.32524526, 0.048430983, 0.086788826, -0.43760976, -0.14664295, 0.09821209, 0.11763248, -0.04267189, 0.3520541, 0.3954583, 0.014902145, 0.11122683, -0.022626013, -0.39378256, 0.43269894, -0.032748938, -0.13828796, 0.024887145, 0.2749234, -0.19273233, -0.34995627, 0.0749587, -0.41279817, 0.18048531, 0.2348634, -0.3145545, 0.21299611, 0.14463069, -0.090702236, 0.17181705, 0.28400052, 0.33169597, 0.19892763, 0.2731048, 0.4246605, 0.23377687, 0.30976477, 0.09325573, -0.13989684, 0.14479497, -0.141247, 0.03883958, -0.32825652, 0.053694755, 0.10774729, 0.3986294, 0.22944489, 0.28739676, 0.12703529, -0.38377568, -0.012970954, -0.26347873, 0.4114408, 0.04025637, 0.26367208, -0.41915637, -0.21442787, -0.032007083, -0.2832813, -0.04426877, 0.0652627, 0.08577582, 0.32938844, -0.15008189, -0.26825994, 0.16777168, 0.0032757586, 0.08952761, 0.25545892, 0.40689972, 0.051397383, -0.122751534, 0.3002623, 0.10019932, -0.2541772, 0.08551011, 0.10585769, 0.14373308, 0.30950966, -0.052251875, 0.12656142, -0.00092755735, 0.15732142, -0.37894496, 0.27250907, 0.18969062, 0.41996512, -0.37623578, 0.16064951, 0.24074027, -0.37409255, -0.18847199, -0.3594549, 0.3693367, -0.20832977, 0.21206811, 0.14170083, 0.14842567, -0.29162788, 0.23036823, 0.09701875, -0.6379243, -0.28784883, -0.47223932, 0.31236076, 0.15811588, 0.07651749, 0.08585637, -0.019357538, 0.18523933, 0.27952322, 0.22034237, -0.038289014, -0.04680873, -0.35267782, -0.13798535, 0.15397748, -0.054507136, 0.06257498, 0.0023951232, 0.2661908, -0.24889962, 0.39024177, 0.09198037, -0.38782477, -0.02698338, 0.059981436, -0.25029522, -0.08071509, 0.17653123, 0.32727256, -0.16374385, 0.35100076, 0.099061996, 0.2926676, -0.089175254, -0.37986544, 0.23979679, -0.4286804, -0.095039904, 0.24509808, 0.1327261, 0.11009517, -0.39106482, 0.20897457, -0.28196415, 0.13838163, 0.022784382, -0.4979184, -0.41080368, 0.38501477, -0.75213, 0.53850865, 0.57971317, 0.36951995, -0.42756104, 0.27265424, -0.30208117, -0.035879463, 0.6849765, 0.38006106, 0.39388627, -0.9354833, -0.2777751}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aaa72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Biases:  {0.04302334, -0.011773386, -0.007550919, 0.017762076, 0.019549057, -0.034284364, 0.0, 0.04088828, -0.0098435255, 0.058216624, 0.0, 0.0, -0.02779871, 0.0, 0.0420049, -0.028798638, 0.021224609, 0.0, -0.009693057, 0.10111676, -0.03411628, 0.033134956, 0.07657477, 0.004672397, 0.0032454398, -0.028938377, 0.12727202, 0.011405003, -0.01534735, -0.010784686, 0.004042835, -0.018769696, -0.0043457747, 0.0, 0.10938503, -0.028589666, -0.026426937, -0.017154902, 0.017863644, 0.0, 0.0069232928, 0.0, -0.011726588, -0.021699728, 0.0, 0.09401358, 0.0, 0.0, -0.000847161, 0.019567134, -0.03062082, 0.10996085, 0.00068597804, 0.00074218353, 3.5207682e-05, -0.0022308768, 0.00055366417, 0.08314156, -0.008843167, 0.0048870044, 0.004043285, 0.0006859399, -0.0052534426, -0.034581512, 0.0015156087}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "50836fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_weights_to_cpp(model, filename=\"quantized_weights_and_biases.txt\"):\n",
    "    model.summary()\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for l in range(len(model.layers)):\n",
    "        W = model.layers[l].get_weights()[0]\n",
    "        B = model.layers[l].get_weights()[1]\n",
    "        W = np.array(W).flatten()\n",
    "        B = np.array(B).flatten()\n",
    "        W = np.int8(W / np.max(np.abs(W)) * 127)\n",
    "        B = np.int32(B * 2**15)\n",
    "        weights.append(W)\n",
    "        biases.append(B)\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"weights: {\")\n",
    "        for i in range(len(weights)):\n",
    "            f.write(\"{\")\n",
    "            for j in range(len(weights[i])):\n",
    "                if (j < len(weights[i])-1):\n",
    "                    f.write(str(weights[i][j])+\", \")\n",
    "                else:\n",
    "                    f.write(str(weights[i][j]))\n",
    "            f.write(\"}\")\n",
    "            if (i < len(weights)-1):\n",
    "                f.write(\", \")\n",
    "        f.write(\"}\\n\\n\")\n",
    "\n",
    "        f.write(\"biases: {\")\n",
    "        for i in range(len(biases)):\n",
    "            if (i < len(biases)-1):\n",
    "                f.write(str(biases[i])+\", \")\n",
    "            else:\n",
    "                f.write(str(biases[i]))\n",
    "        f.write(\"}\\n\\n\")\n",
    "\n",
    "        arch = []\n",
    "\n",
    "        arch.append(model.layers[0].input_shape[1])\n",
    "        for i in range(1, len(model.layers)):\n",
    "            arch.append(model.layers[i].input_shape[1])\n",
    "        arch.append(model.layers[len(model.layers)-1].output_shape[1])\n",
    "        f.write(\"Architecture: {\")\n",
    "        for i in range(len(arch)):\n",
    "            if (i < len(arch)-1):\n",
    "                f.write(str(arch[i])+\", \")\n",
    "            else:\n",
    "                f.write(str(arch[i]))\n",
    "        f.write(\"}\")\n",
    "        print(\"Architecture (alpha):\", arch)\n",
    "        print(\"Layers:\", len(arch))\n",
    "    print(\"Quantized Weights: \", weights)\n",
    "    print(\"Quantized Biases: \", biases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1afd89f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [92]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mquantized_weights_to_cpp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtflite_quant_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [90]\u001b[0m, in \u001b[0;36mquantized_weights_to_cpp\u001b[0;34m(model, filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mquantized_weights_to_cpp\u001b[39m(model, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantized_weights_and_biases.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m()\n\u001b[1;32m      3\u001b[0m     weights \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m     biases \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "quantized_weights_to_cpp(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "11532bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight 0: 0.4475988745689392\n",
      "Weight 1: 0.26112598180770874\n",
      "Weight 2: 0.19550372660160065\n",
      "Weight 3: 0.384503036737442\n",
      "Weight 4: -0.199424147605896\n",
      "Weight 5: 0.450770765542984\n",
      "Weight 6: 0.244806706905365\n",
      "Weight 7: -0.3863460123538971\n",
      "Weight 8: 0.4603942632675171\n",
      "Weight 9: -0.26338520646095276\n",
      "Weight 10: -0.14693251252174377\n",
      "Weight 11: 0.02999800443649292\n",
      "Weight 12: 0.26927652955055237\n",
      "Weight 13: 0.26886099576950073\n",
      "Weight 14: 0.37914806604385376\n",
      "Weight 15: 0.3704555034637451\n",
      "Weight 16: -0.07572215795516968\n",
      "Weight 17: -0.04227825254201889\n",
      "Weight 18: -0.14979152381420135\n",
      "Weight 19: 0.2953677177429199\n",
      "Weight 20: 0.49309924244880676\n",
      "Weight 21: 0.3732600808143616\n",
      "Weight 22: 0.00046372413635253906\n",
      "Weight 23: 0.47503602504730225\n",
      "Weight 24: 0.3258184790611267\n",
      "Weight 25: 0.4328867197036743\n",
      "Weight 26: -0.35409659147262573\n",
      "Weight 27: -0.1936517059803009\n",
      "Weight 28: -0.1159953698515892\n",
      "Weight 29: 0.16672450304031372\n",
      "Weight 30: -0.3148607015609741\n",
      "Weight 31: 0.20089751482009888\n",
      "Weight 32: 0.09256621450185776\n",
      "Weight 33: 0.060714319348335266\n",
      "Weight 34: -0.24219085276126862\n",
      "Weight 35: -0.09162846207618713\n",
      "Weight 36: -0.5084258913993835\n",
      "Weight 37: 0.14774742722511292\n",
      "Weight 38: 0.3158528208732605\n",
      "Weight 39: -0.08556196093559265\n",
      "Weight 40: 0.10712648928165436\n",
      "Weight 41: 0.09112793207168579\n",
      "Weight 42: -0.3026867210865021\n",
      "Weight 43: -0.45085737109184265\n",
      "Weight 44: -0.41424357891082764\n",
      "Weight 45: 0.04763990640640259\n",
      "Weight 46: 0.052029870450496674\n",
      "Weight 47: 0.3561636805534363\n",
      "Weight 48: -0.27031031250953674\n",
      "Weight 49: -0.24902382493019104\n",
      "Weight 50: 0.041463565081357956\n",
      "Weight 51: -0.06165778636932373\n",
      "Weight 52: 0.22187663614749908\n",
      "Weight 53: -0.34530216455459595\n",
      "Weight 54: -0.3934287130832672\n",
      "Weight 55: -0.3673357367515564\n",
      "Weight 56: -0.380124032497406\n",
      "Weight 57: -0.16677065193653107\n",
      "Weight 58: -0.05367952585220337\n",
      "Weight 59: 0.43864357471466064\n",
      "Weight 60: -0.4154655933380127\n",
      "Weight 61: 0.43046849966049194\n",
      "Weight 62: 0.07993248105049133\n",
      "Weight 63: -0.13074202835559845\n",
      "Weight 64: 0.12888436019420624\n",
      "Weight 65: -0.03650669381022453\n",
      "Weight 66: 0.3249489665031433\n",
      "Weight 67: -0.31653085350990295\n",
      "Weight 68: -0.36264944076538086\n",
      "Weight 69: 0.3528420925140381\n",
      "Weight 70: -0.17461952567100525\n",
      "Weight 71: 0.1322093904018402\n",
      "Weight 72: 0.39646652340888977\n",
      "Weight 73: 0.20851893723011017\n",
      "Weight 74: -0.019526392221450806\n",
      "Weight 75: 0.09696000814437866\n",
      "Weight 76: 0.06380371004343033\n",
      "Weight 77: -0.3511887788772583\n",
      "Weight 78: -0.4262489974498749\n",
      "Weight 79: 0.4049634635448456\n",
      "Weight 80: -0.34380581974983215\n",
      "Weight 81: 0.07100102305412292\n",
      "Weight 82: -0.3880469501018524\n",
      "Weight 83: -0.040148407220840454\n",
      "Weight 84: 0.23159922659397125\n",
      "Weight 85: 0.1581384390592575\n",
      "Weight 86: -0.3739052414894104\n",
      "Weight 87: 0.17289972305297852\n",
      "Weight 88: 0.10609062761068344\n",
      "Weight 89: -0.17268554866313934\n",
      "Weight 90: 0.061895668506622314\n",
      "Weight 91: -0.3268076777458191\n",
      "Weight 92: 0.022088762372732162\n",
      "Weight 93: -0.0862785279750824\n",
      "Weight 94: 0.17724725604057312\n",
      "Weight 95: -0.4194738566875458\n",
      "Weight 96: 0.4210028052330017\n",
      "Weight 97: -0.18368589878082275\n",
      "Weight 98: 0.04904298856854439\n",
      "Weight 99: 0.2590389847755432\n",
      "Weight 100: 0.31834426522254944\n",
      "Weight 101: 0.4578152000904083\n",
      "Weight 102: -0.058038562536239624\n",
      "Weight 103: -0.0025197770446538925\n",
      "Weight 104: 0.29601576924324036\n",
      "Weight 105: 0.2961235046386719\n",
      "Weight 106: -0.37464040517807007\n",
      "Weight 107: -0.04758709669113159\n",
      "Weight 108: 0.3065417408943176\n",
      "Weight 109: -0.4504009187221527\n",
      "Weight 110: 0.30664312839508057\n",
      "Weight 111: -0.06207041069865227\n",
      "Weight 112: -0.09798511117696762\n",
      "Weight 113: -0.11341463029384613\n",
      "Weight 114: -0.15143729746341705\n",
      "Weight 115: 0.34454581141471863\n",
      "Weight 116: 0.45595067739486694\n",
      "Weight 117: -0.4810476005077362\n",
      "Weight 118: -0.27885717153549194\n",
      "Weight 119: 0.1851251721382141\n",
      "Weight 120: -0.417547345161438\n",
      "Weight 121: -0.2996281385421753\n",
      "Weight 122: -0.39868295192718506\n",
      "Weight 123: 0.0597379207611084\n",
      "Weight 124: 0.22532357275485992\n",
      "Weight 125: -0.27841341495513916\n",
      "Weight 126: 0.18161724507808685\n",
      "Weight 127: -0.04884728416800499\n",
      "Weight 128: 0.3401470184326172\n",
      "Weight 129: -0.1895979791879654\n",
      "Weight 130: 0.11788840591907501\n",
      "Weight 131: 0.07006188482046127\n",
      "Weight 132: 0.27887359261512756\n",
      "Weight 133: 0.40783604979515076\n",
      "Weight 134: 0.11325693130493164\n",
      "Weight 135: -0.25412100553512573\n",
      "Weight 136: -0.17190633714199066\n",
      "Weight 137: 0.332125186920166\n",
      "Weight 138: -0.029944002628326416\n",
      "Weight 139: -0.08705088496208191\n",
      "Weight 140: 0.1324138045310974\n",
      "Weight 141: -0.4320395588874817\n",
      "Weight 142: 0.31291794776916504\n",
      "Weight 143: 0.12336363643407822\n",
      "Weight 144: -0.11117587238550186\n",
      "Weight 145: 0.07192833721637726\n",
      "Weight 146: 0.009900804609060287\n",
      "Weight 147: 0.03137907013297081\n",
      "Weight 148: -0.495201051235199\n",
      "Weight 149: 0.4535006880760193\n",
      "Weight 150: -0.25720781087875366\n",
      "Weight 151: -0.4048890769481659\n",
      "Weight 152: -0.1664503663778305\n",
      "Weight 153: -0.5343040823936462\n",
      "Weight 154: 0.3939105272293091\n",
      "Weight 155: -0.021711260080337524\n",
      "Weight 156: 0.17656171321868896\n",
      "Weight 157: -0.1516050398349762\n",
      "Weight 158: 0.16841581463813782\n",
      "Weight 159: 0.49074533581733704\n",
      "Weight 160: -0.17471635341644287\n",
      "Weight 161: -0.3432137966156006\n",
      "Weight 162: 0.16525906324386597\n",
      "Weight 163: -0.16544599831104279\n",
      "Weight 164: 0.13862231373786926\n",
      "Weight 165: 0.4036440849304199\n",
      "Weight 166: -0.3793400526046753\n",
      "Weight 167: 0.029631169512867928\n",
      "Weight 168: 0.35496774315834045\n",
      "Weight 169: 0.09592899680137634\n",
      "Weight 170: -0.3725630044937134\n",
      "Weight 171: -0.3506980836391449\n",
      "Weight 172: 0.3303582966327667\n",
      "Weight 173: 0.03721937537193298\n",
      "Weight 174: -0.1458217203617096\n",
      "Weight 175: -0.2525083124637604\n",
      "Weight 176: -0.10763637721538544\n",
      "Weight 177: 0.16154183447360992\n",
      "Weight 178: -0.11295057088136673\n",
      "Weight 179: -0.061643894761800766\n",
      "Weight 180: 0.43975067138671875\n",
      "Weight 181: -0.21803277730941772\n",
      "Weight 182: 0.32701998949050903\n",
      "Weight 183: -0.12130151689052582\n",
      "Weight 184: 0.3496741056442261\n",
      "Weight 185: -0.3412957489490509\n",
      "Weight 186: 0.3207986354827881\n",
      "Weight 187: 0.2906973958015442\n",
      "Weight 188: -0.15151390433311462\n",
      "Weight 189: -0.012985646724700928\n",
      "Weight 190: -0.37638407945632935\n",
      "Weight 191: -0.05244983732700348\n",
      "Weight 192: 0.4458724856376648\n",
      "Weight 193: 0.006146678701043129\n",
      "Weight 194: -0.45774415135383606\n",
      "Weight 195: -0.008519616909325123\n",
      "Weight 196: -0.3079650104045868\n",
      "Weight 197: -0.19297274947166443\n",
      "Weight 198: -0.31642651557922363\n",
      "Weight 199: -0.1932193487882614\n",
      "Weight 200: -0.32917869091033936\n",
      "Weight 201: 0.0819130465388298\n",
      "Weight 202: 0.004797577857971191\n",
      "Weight 203: -0.13665738701820374\n",
      "Weight 204: -0.491110622882843\n",
      "Weight 205: 0.196366548538208\n",
      "Weight 206: -0.255318284034729\n",
      "Weight 207: -0.31688347458839417\n",
      "Weight 208: 0.47196248173713684\n",
      "Weight 209: 0.3235519230365753\n",
      "Weight 210: 0.20902279019355774\n",
      "Weight 211: 0.31121960282325745\n",
      "Weight 212: 0.4359617829322815\n",
      "Weight 213: -0.055640920996665955\n",
      "Weight 214: -0.14077523350715637\n",
      "Weight 215: 0.3076731562614441\n",
      "Weight 216: 0.43513697385787964\n",
      "Weight 217: -0.11375689506530762\n",
      "Weight 218: -0.3169597387313843\n",
      "Weight 219: 0.2934543490409851\n",
      "Weight 220: 0.2646130323410034\n",
      "Weight 221: -0.3659209609031677\n",
      "Weight 222: 0.26633766293525696\n",
      "Weight 223: 0.17245152592658997\n",
      "Weight 224: -0.04122934490442276\n",
      "Weight 225: -0.02789631485939026\n",
      "Weight 226: 0.03839387744665146\n",
      "Weight 227: -0.30208250880241394\n",
      "Weight 228: -0.0541035458445549\n",
      "Weight 229: 0.30983370542526245\n",
      "Weight 230: 0.15963619947433472\n",
      "Weight 231: 0.23726202547550201\n",
      "Weight 232: -0.29712313413619995\n",
      "Weight 233: 0.41423460841178894\n",
      "Weight 234: 0.05283236876130104\n",
      "Weight 235: 0.2281399667263031\n",
      "Weight 236: -0.1296018362045288\n",
      "Weight 237: 0.13803495466709137\n",
      "Weight 238: 0.07180905342102051\n",
      "Weight 239: 0.26340675354003906\n",
      "Weight 240: 0.38117262721061707\n",
      "Weight 241: -0.08164536952972412\n",
      "Weight 242: 0.43203845620155334\n",
      "Weight 243: -0.024661598727107048\n",
      "Weight 244: 0.20000453293323517\n",
      "Weight 245: -0.1864028424024582\n",
      "Weight 246: 0.11821290850639343\n",
      "Weight 247: 0.26020556688308716\n",
      "Weight 248: 0.03204768896102905\n",
      "Weight 249: -0.36509743332862854\n",
      "Weight 250: -0.0538935661315918\n",
      "Weight 251: 0.2965933680534363\n",
      "Weight 252: 0.057075221091508865\n",
      "Weight 253: 0.1943298727273941\n",
      "Weight 254: -0.044790029525756836\n",
      "Weight 255: 0.28470367193222046\n",
      "Weight 256: 0.23291002213954926\n",
      "Weight 257: 0.1963997781276703\n",
      "Weight 258: -0.14109277725219727\n",
      "Weight 259: -0.2156543731689453\n",
      "Weight 260: 0.2728366255760193\n",
      "Weight 261: 0.24233216047286987\n",
      "Weight 262: 0.30853959918022156\n",
      "Weight 263: -0.09090226888656616\n",
      "Weight 264: 0.1904401332139969\n",
      "Weight 265: -0.04312470927834511\n",
      "Weight 266: 0.03452521562576294\n",
      "Weight 267: 0.07380148768424988\n",
      "Weight 268: 0.40742847323417664\n",
      "Weight 269: -0.21997173130512238\n",
      "Weight 270: 0.019312918186187744\n",
      "Weight 271: 0.011853862553834915\n",
      "Weight 272: 0.3856981694698334\n",
      "Weight 273: -0.28840386867523193\n",
      "Weight 274: 0.13556283712387085\n",
      "Weight 275: -0.45393866300582886\n",
      "Weight 276: -0.29995205998420715\n",
      "Weight 277: 0.08828668296337128\n",
      "Weight 278: 0.37306302785873413\n",
      "Weight 279: -0.2746134102344513\n",
      "Weight 280: -0.24900253117084503\n",
      "Weight 281: 0.2354077249765396\n",
      "Weight 282: 0.29394465684890747\n",
      "Weight 283: 0.2770630121231079\n",
      "Weight 284: 0.18948009610176086\n",
      "Weight 285: -0.16643191874027252\n",
      "Weight 286: 0.36473098397254944\n",
      "Weight 287: -0.07669112831354141\n",
      "Weight 288: -0.22162596881389618\n",
      "Weight 289: -0.40824562311172485\n",
      "Weight 290: -0.4186217188835144\n",
      "Weight 291: 0.1764344722032547\n",
      "Weight 292: 0.19666479527950287\n",
      "Weight 293: -0.00022665619326289743\n",
      "Weight 294: -0.40385904908180237\n",
      "Weight 295: 0.15684351325035095\n",
      "Weight 296: -0.06061603128910065\n",
      "Weight 297: 0.24996089935302734\n",
      "Weight 298: -0.1304580569267273\n",
      "Weight 299: 0.34226372838020325\n",
      "Weight 300: 0.2690665125846863\n",
      "Weight 301: 0.18673616647720337\n",
      "Weight 302: -0.46849673986434937\n",
      "Weight 303: 0.11902085691690445\n",
      "Weight 304: 0.01672893762588501\n",
      "Weight 305: -0.0793878436088562\n",
      "Weight 306: 0.28770121932029724\n",
      "Weight 307: 0.19902172684669495\n",
      "Weight 308: 0.2210049331188202\n",
      "Weight 309: -0.0681043267250061\n",
      "Weight 310: 0.12906280159950256\n",
      "Weight 311: 0.13327565789222717\n",
      "Weight 312: 0.08334454894065857\n",
      "Weight 313: -0.2965981960296631\n",
      "Weight 314: 0.01793733239173889\n",
      "Weight 315: -0.2693508267402649\n",
      "Weight 316: 0.03471428155899048\n",
      "Weight 317: 0.289532333612442\n",
      "Weight 318: -0.05818799138069153\n",
      "Weight 319: 0.1119561493396759\n",
      "Weight 320: 0.20734232664108276\n",
      "Weight 321: 0.2196348011493683\n",
      "Weight 322: -0.09718876332044601\n",
      "Weight 323: -0.5834251046180725\n",
      "Weight 324: 0.38531672954559326\n",
      "Weight 325: 0.22808559238910675\n",
      "Weight 326: -0.2612434923648834\n",
      "Weight 327: 0.503044605255127\n",
      "Weight 328: -0.2815397083759308\n",
      "Weight 329: 0.25176888704299927\n",
      "Weight 330: -0.3820316791534424\n",
      "Weight 331: 0.10125502943992615\n",
      "Weight 332: 0.2075466811656952\n",
      "Weight 333: -0.36382973194122314\n",
      "Weight 334: -0.14705686271190643\n",
      "Weight 335: 0.39698371291160583\n",
      "Weight 336: 0.280173122882843\n",
      "Weight 337: -0.0872245728969574\n",
      "Weight 338: -0.34494075179100037\n",
      "Weight 339: 0.02011093683540821\n",
      "Weight 340: 0.19241102039813995\n",
      "Weight 341: 0.20064492523670197\n",
      "Weight 342: -0.2710757553577423\n",
      "Weight 343: 0.031061407178640366\n",
      "Weight 344: 0.2347131371498108\n",
      "Weight 345: 0.4549787938594818\n",
      "Weight 346: 0.5855766534805298\n",
      "Weight 347: -0.00038502630195580423\n",
      "Weight 348: 0.3696044385433197\n",
      "Weight 349: -0.26625075936317444\n",
      "Weight 350: -0.5128847360610962\n",
      "Weight 351: 0.07959349453449249\n",
      "Weight 352: -0.5983781218528748\n",
      "Weight 353: -0.21176780760288239\n",
      "Weight 354: -0.19259585440158844\n",
      "Weight 355: 0.009720276109874249\n",
      "Weight 356: 0.2746065855026245\n",
      "Weight 357: 0.5032299757003784\n",
      "Weight 358: -0.000993419555015862\n",
      "Weight 359: 0.08717135339975357\n",
      "Weight 360: -0.2943992018699646\n",
      "Weight 361: 0.021770792081952095\n",
      "Weight 362: -0.06424297392368317\n",
      "Weight 363: 0.41019412875175476\n",
      "Weight 364: -0.4306294023990631\n",
      "Weight 365: 0.29057759046554565\n",
      "Weight 366: 0.23266059160232544\n",
      "Weight 367: -0.036377109587192535\n",
      "Weight 368: 0.3745597302913666\n",
      "Weight 369: -0.0015815198421478271\n",
      "Weight 370: 0.15391668677330017\n",
      "Weight 371: 0.28881219029426575\n",
      "Weight 372: -0.2981420159339905\n",
      "Weight 373: 0.08297380805015564\n",
      "Weight 374: -0.3104267418384552\n",
      "Weight 375: 0.4193667471408844\n",
      "Weight 376: -0.2668498754501343\n",
      "Weight 377: 0.330093115568161\n",
      "Weight 378: 0.19664278626441956\n",
      "Weight 379: 0.4010538160800934\n",
      "Weight 380: -0.16222316026687622\n",
      "Weight 381: -0.22456161677837372\n",
      "Weight 382: 0.29910263419151306\n",
      "Weight 383: 0.3726092278957367\n",
      "Weight 384: 0.06044527888298035\n",
      "Weight 385: -0.018388479948043823\n",
      "Weight 386: -0.05097925662994385\n",
      "Weight 387: 0.28717467188835144\n",
      "Weight 388: 0.1781797707080841\n",
      "Weight 389: 0.40560582280158997\n",
      "Weight 390: -0.0645921528339386\n",
      "Weight 391: -0.3856985867023468\n",
      "Weight 392: -0.21999384462833405\n",
      "Weight 393: 0.4101276695728302\n",
      "Weight 394: -0.15943756699562073\n",
      "Weight 395: -0.017224788665771484\n",
      "Weight 396: -0.03663915395736694\n",
      "Weight 397: 0.19490239024162292\n",
      "Weight 398: 0.27925464510917664\n",
      "Weight 399: 0.00875738263130188\n",
      "Weight 400: -0.44521719217300415\n",
      "Weight 401: 0.35223856568336487\n",
      "Weight 402: -0.24728579819202423\n",
      "Weight 403: -0.4341001808643341\n",
      "Weight 404: -0.20806770026683807\n",
      "Weight 405: 0.2869536578655243\n",
      "Weight 406: 0.12456914782524109\n",
      "Weight 407: 0.26228538155555725\n",
      "Weight 408: -0.04149093106389046\n",
      "Weight 409: -0.29102712869644165\n",
      "Weight 410: 0.4930824339389801\n",
      "Weight 411: 0.40163329243659973\n",
      "Weight 412: -0.08526727557182312\n",
      "Weight 413: -0.34446966648101807\n",
      "Weight 414: -0.1749509871006012\n",
      "Weight 415: 0.20967398583889008\n",
      "Weight 416: 0.2994530498981476\n",
      "Weight 417: 0.19103333353996277\n",
      "Weight 418: 0.1355896294116974\n",
      "Weight 419: -0.32422834634780884\n",
      "Weight 420: -0.04052308201789856\n",
      "Weight 421: -0.02306509017944336\n",
      "Weight 422: 0.23367777466773987\n",
      "Weight 423: -0.2416035681962967\n",
      "Weight 424: 0.3075079023838043\n",
      "Weight 425: -0.2698327302932739\n",
      "Weight 426: 0.39587095379829407\n",
      "Weight 427: -0.16280561685562134\n",
      "Weight 428: 0.3633934557437897\n",
      "Weight 429: -0.03964722156524658\n",
      "Weight 430: 0.015698730945587158\n",
      "Weight 431: 0.15249595046043396\n",
      "Weight 432: -0.0007044539670459926\n",
      "Weight 433: 0.1524668037891388\n",
      "Weight 434: -0.11847758293151855\n",
      "Weight 435: -0.17380750179290771\n",
      "Weight 436: 0.0753171443939209\n",
      "Weight 437: -0.4822259545326233\n",
      "Weight 438: -0.046445008367300034\n",
      "Weight 439: 0.5983743667602539\n",
      "Weight 440: 0.29791924357414246\n",
      "Weight 441: 0.12247076630592346\n",
      "Weight 442: -0.7154111862182617\n",
      "Weight 443: -0.3622632622718811\n",
      "Weight 444: 0.07144076377153397\n",
      "Weight 445: -0.08112631738185883\n",
      "Weight 446: -0.15290012955665588\n",
      "Weight 447: 0.03427165374159813\n",
      "Weight 448: 0.11259377747774124\n",
      "Weight 449: -0.29861918091773987\n",
      "Weight 450: 0.26767703890800476\n",
      "Weight 451: 0.01625923626124859\n",
      "Weight 452: 0.23289525508880615\n",
      "Weight 453: 0.05101432651281357\n",
      "Weight 454: -0.1515652984380722\n",
      "Weight 455: 0.09247837960720062\n",
      "Weight 456: 0.2487153857946396\n",
      "Weight 457: 0.3307838439941406\n",
      "Weight 458: -0.12299313396215439\n",
      "Weight 459: -0.27578476071357727\n",
      "Weight 460: -0.3279432952404022\n",
      "Weight 461: 0.09664347022771835\n",
      "Weight 462: 0.25459492206573486\n",
      "Weight 463: 0.4040853977203369\n",
      "Weight 464: 0.11511582881212234\n",
      "Weight 465: -0.1349794864654541\n",
      "Weight 466: 0.2988811433315277\n",
      "Weight 467: -0.24022261798381805\n",
      "Weight 468: 0.2365155667066574\n",
      "Weight 469: 0.03858361765742302\n",
      "Weight 470: 0.46229472756385803\n",
      "Weight 471: -0.03618696331977844\n",
      "Weight 472: 0.3677836060523987\n",
      "Weight 473: -0.12789693474769592\n",
      "Weight 474: -0.3124753534793854\n",
      "Weight 475: -0.17041520774364471\n",
      "Weight 476: -0.43047478795051575\n",
      "Weight 477: 0.5295370817184448\n",
      "Weight 478: 0.05540436506271362\n",
      "Weight 479: -0.40865010023117065\n",
      "Weight 480: -0.24134062230587006\n",
      "Weight 481: -0.42159774899482727\n",
      "Weight 482: -0.1402551829814911\n",
      "Weight 483: -0.08175158500671387\n",
      "Weight 484: -0.02770170569419861\n",
      "Weight 485: 0.13719716668128967\n",
      "Weight 486: 0.3206256330013275\n",
      "Weight 487: 0.24583843350410461\n",
      "Weight 488: 0.34144529700279236\n",
      "Weight 489: -0.13875359296798706\n",
      "Weight 490: -0.15082988142967224\n",
      "Weight 491: 0.1590917408466339\n",
      "Weight 492: 0.3604908883571625\n",
      "Weight 493: -0.3719390034675598\n",
      "Weight 494: -0.3968285024166107\n",
      "Weight 495: -0.13827764987945557\n",
      "Weight 496: -0.2322572022676468\n",
      "Weight 497: -0.265712708234787\n",
      "Weight 498: -0.2121269851922989\n",
      "Weight 499: -0.24976249039173126\n",
      "Weight 500: 0.2721579372882843\n",
      "Weight 501: 0.32373538613319397\n",
      "Weight 502: 0.3767799139022827\n",
      "Weight 503: -0.2727126479148865\n",
      "Weight 504: 0.16966453194618225\n",
      "Weight 505: 0.2558039724826813\n",
      "Weight 506: 0.0403793528676033\n",
      "Weight 507: 0.41800567507743835\n",
      "Weight 508: -0.31843215227127075\n",
      "Weight 509: -0.17347818613052368\n",
      "Weight 510: 0.014401227235794067\n",
      "Weight 511: 0.30212029814720154\n",
      "Weight 512: -0.060318537056446075\n",
      "Weight 513: -0.048098206520080566\n",
      "Weight 514: -0.5589321851730347\n",
      "Weight 515: 0.23693057894706726\n",
      "Weight 516: -0.4445580840110779\n",
      "Weight 517: -0.2553709149360657\n",
      "Weight 518: -0.2126106321811676\n",
      "Weight 519: 0.218129962682724\n",
      "Weight 520: 0.03498230502009392\n",
      "Weight 521: 0.3705245554447174\n",
      "Weight 522: -0.15407735109329224\n",
      "Weight 523: 0.13526281714439392\n",
      "Weight 524: -0.1812954843044281\n",
      "Weight 525: 0.044708773493766785\n",
      "Weight 526: 0.06190621852874756\n",
      "Weight 527: -0.14457714557647705\n",
      "Weight 528: -0.3275844156742096\n",
      "Weight 529: -0.13635236024856567\n",
      "Weight 530: -0.2876538038253784\n",
      "Weight 531: 0.37029412388801575\n",
      "Weight 532: 0.13439010083675385\n",
      "Weight 533: 0.41402795910835266\n",
      "Weight 534: -0.05914151296019554\n",
      "Weight 535: 0.2522035539150238\n",
      "Weight 536: -0.2153196483850479\n",
      "Weight 537: -0.07742694020271301\n",
      "Weight 538: -0.4066953957080841\n",
      "Weight 539: -0.02272840030491352\n",
      "Weight 540: -0.015428036451339722\n",
      "Weight 541: -0.22218839824199677\n",
      "Weight 542: -0.24528008699417114\n",
      "Weight 543: -0.33628514409065247\n",
      "Weight 544: 0.3144562542438507\n",
      "Weight 545: -0.36821436882019043\n",
      "Weight 546: -0.2117864340543747\n",
      "Weight 547: 0.3654070794582367\n",
      "Weight 548: -0.165570929646492\n",
      "Weight 549: -0.18278907239437103\n",
      "Weight 550: 0.450545996427536\n",
      "Weight 551: 0.15378794074058533\n",
      "Weight 552: 0.1291545182466507\n",
      "Weight 553: 0.1985178291797638\n",
      "Weight 554: -0.13851356506347656\n",
      "Weight 555: -0.1203848123550415\n",
      "Weight 556: -0.2735634446144104\n",
      "Weight 557: -0.415245920419693\n",
      "Weight 558: -0.2677736282348633\n",
      "Weight 559: -0.37554460763931274\n",
      "Weight 560: 0.2112199068069458\n",
      "Weight 561: 0.22854319214820862\n",
      "Weight 562: -0.2990753948688507\n",
      "Weight 563: -0.5646328330039978\n",
      "Weight 564: -0.3414577543735504\n",
      "Weight 565: -0.5795822739601135\n",
      "Weight 566: -0.27696844935417175\n",
      "Weight 567: 0.07253161072731018\n",
      "Weight 568: 0.49518445134162903\n",
      "Weight 569: 0.09707698225975037\n",
      "Weight 570: 0.040343042463064194\n",
      "Weight 571: -0.3155147433280945\n",
      "Weight 572: -0.11517342925071716\n",
      "Weight 573: -0.19470854103565216\n",
      "Weight 574: -0.21316751837730408\n",
      "Weight 575: -0.2863597273826599\n",
      "Weight 576: -0.5390974283218384\n",
      "Weight 577: 0.3377213776111603\n",
      "Weight 578: -0.3690599501132965\n",
      "Weight 579: 0.2206697165966034\n",
      "Weight 580: -0.021684978157281876\n",
      "Weight 581: 0.1379820704460144\n",
      "Weight 582: -0.16069215536117554\n",
      "Weight 583: -0.16286641359329224\n",
      "Weight 584: -0.5239479541778564\n",
      "Weight 585: -0.004250943660736084\n",
      "Weight 586: 0.03968489170074463\n",
      "Weight 587: 0.3107011914253235\n",
      "Weight 588: 0.09121748805046082\n",
      "Weight 589: 0.12066119909286499\n",
      "Weight 590: -0.11100748181343079\n",
      "Weight 591: -0.29995375871658325\n",
      "Weight 592: -0.1395021378993988\n",
      "Weight 593: -0.2302936166524887\n",
      "Weight 594: -0.25070104002952576\n",
      "Weight 595: -0.16429275274276733\n",
      "Weight 596: -0.0868116244673729\n",
      "Weight 597: 0.3014674186706543\n",
      "Weight 598: 0.031549155712127686\n",
      "Weight 599: -0.33354055881500244\n",
      "Weight 600: 0.17032493650913239\n",
      "Weight 601: 0.2675776779651642\n",
      "Weight 602: -0.20049099624156952\n",
      "Weight 603: -0.0009010786307044327\n",
      "Weight 604: -0.29012709856033325\n",
      "Weight 605: 0.4243972897529602\n",
      "Weight 606: 0.2898690402507782\n",
      "Weight 607: 0.012711018323898315\n",
      "Weight 608: 0.3508076071739197\n",
      "Weight 609: -0.27917343378067017\n",
      "Weight 610: -0.013979963958263397\n",
      "Weight 611: 0.14423829317092896\n",
      "Weight 612: 0.33529528975486755\n",
      "Weight 613: 0.29412809014320374\n",
      "Weight 614: -0.153397798538208\n",
      "Weight 615: -0.1881665140390396\n",
      "Weight 616: 0.13350428640842438\n",
      "Weight 617: -0.35571861267089844\n",
      "Weight 618: 0.02258807048201561\n",
      "Weight 619: -0.32375672459602356\n",
      "Weight 620: -0.4198182225227356\n",
      "Weight 621: 0.2576408088207245\n",
      "Weight 622: -0.3276289105415344\n",
      "Weight 623: -0.36139950156211853\n",
      "Weight 624: -0.0814749151468277\n",
      "Weight 625: -0.07325345277786255\n",
      "Weight 626: 0.3068389594554901\n",
      "Weight 627: -0.31139853596687317\n",
      "Weight 628: -0.7228308320045471\n",
      "Weight 629: -0.7561255693435669\n",
      "Weight 630: -0.7927280068397522\n",
      "Weight 631: 0.23041006922721863\n",
      "Weight 632: 0.002275764476507902\n",
      "Weight 633: -0.32267677783966064\n",
      "Weight 634: 0.13321879506111145\n",
      "Weight 635: -0.025418907403945923\n",
      "Weight 636: -0.09939751029014587\n",
      "Weight 637: 0.15293464064598083\n",
      "Weight 638: -0.27142012119293213\n",
      "Weight 639: 0.4004882276058197\n",
      "Weight 640: 0.20222483575344086\n",
      "Weight 641: -0.023111343383789062\n",
      "Weight 642: 0.08558473736047745\n",
      "Weight 643: -0.35365360975265503\n",
      "Weight 644: 0.31005680561065674\n",
      "Weight 645: -0.38939666748046875\n",
      "Weight 646: 0.301775187253952\n",
      "Weight 647: -0.2761242985725403\n",
      "Weight 648: -0.04557214677333832\n",
      "Weight 649: -0.26040107011795044\n",
      "Weight 650: 0.2727556824684143\n",
      "Weight 651: -0.2117215394973755\n",
      "Weight 652: -0.2956990897655487\n",
      "Weight 653: -0.20331355929374695\n",
      "Weight 654: -0.009608179330825806\n",
      "Weight 655: 0.07422694563865662\n",
      "Weight 656: 0.2465745210647583\n",
      "Weight 657: -0.4003860056400299\n",
      "Weight 658: -0.08890806883573532\n",
      "Weight 659: 0.1053762435913086\n",
      "Weight 660: 0.41662296652793884\n",
      "Weight 661: 0.24220135807991028\n",
      "Weight 662: 0.4558751583099365\n",
      "Weight 663: 0.024240463972091675\n",
      "Weight 664: 0.049872417002916336\n",
      "Weight 665: 0.10172656178474426\n",
      "Weight 666: -0.09309574216604233\n",
      "Weight 667: -0.28023871779441833\n",
      "Weight 668: 0.38266173005104065\n",
      "Weight 669: -0.06960219144821167\n",
      "Weight 670: -0.1500348448753357\n",
      "Weight 671: 0.01926034688949585\n",
      "Weight 672: 0.22387740015983582\n",
      "Weight 673: 0.300114244222641\n",
      "Weight 674: 0.31607505679130554\n",
      "Weight 675: 0.16796046495437622\n",
      "Weight 676: -0.09831714630126953\n",
      "Weight 677: 0.3777059316635132\n",
      "Weight 678: 0.3497869074344635\n",
      "Weight 679: 0.3339979946613312\n",
      "Weight 680: -0.054856929928064346\n",
      "Weight 681: -0.13809451460838318\n",
      "Weight 682: -0.40974247455596924\n",
      "Weight 683: -0.24166974425315857\n",
      "Weight 684: 0.05319032073020935\n",
      "Weight 685: -0.048627547919750214\n",
      "Weight 686: 0.05399352312088013\n",
      "Weight 687: 0.4104105532169342\n",
      "Weight 688: -0.15478117763996124\n",
      "Weight 689: -0.05563583970069885\n",
      "Weight 690: -0.6624042987823486\n",
      "Weight 691: 0.40723472833633423\n",
      "Weight 692: 0.04992310702800751\n",
      "Weight 693: -0.2863345444202423\n",
      "Weight 694: -0.21623963117599487\n",
      "Weight 695: -0.16637775301933289\n",
      "Weight 696: 0.2825472950935364\n",
      "Weight 697: 0.4278477728366852\n",
      "Weight 698: 0.26012998819351196\n",
      "Weight 699: 0.09428390860557556\n",
      "Weight 700: -0.07968279719352722\n",
      "Weight 701: -0.3170556426048279\n",
      "Weight 702: 0.4029068648815155\n",
      "Weight 703: 0.3400409519672394\n",
      "Weight 704: 0.2123124599456787\n",
      "Weight 705: 0.19719573855400085\n",
      "Weight 706: 0.07400686293840408\n",
      "Weight 707: 0.20079463720321655\n",
      "Weight 708: 0.00810740701854229\n",
      "Weight 709: -0.001657861634157598\n",
      "Weight 710: -0.35292214155197144\n",
      "Weight 711: -0.22944147884845734\n",
      "Weight 712: 0.18441025912761688\n",
      "Weight 713: -0.21690019965171814\n",
      "Weight 714: -0.30413874983787537\n",
      "Weight 715: 0.33759236335754395\n",
      "Weight 716: 0.18632814288139343\n",
      "Weight 717: -0.08034641295671463\n",
      "Weight 718: 0.4137643277645111\n",
      "Weight 719: -0.38827788829803467\n",
      "Weight 720: 0.21929538249969482\n",
      "Weight 721: 0.3166705071926117\n",
      "Weight 722: 0.13352055847644806\n",
      "Weight 723: -0.2690570652484894\n",
      "Weight 724: -0.22560721635818481\n",
      "Weight 725: 0.43918749690055847\n",
      "Weight 726: -0.011828887276351452\n",
      "Weight 727: 0.3239445686340332\n",
      "Weight 728: 0.19694602489471436\n",
      "Weight 729: -0.6498859524726868\n",
      "Weight 730: 0.3247772753238678\n",
      "Weight 731: 0.16700462996959686\n",
      "Weight 732: 0.31163647770881653\n",
      "Weight 733: 0.41015711426734924\n",
      "Weight 734: 0.3686004877090454\n",
      "Weight 735: 0.019007567316293716\n",
      "Weight 736: -0.03886166214942932\n",
      "Weight 737: -0.2245347797870636\n",
      "Weight 738: -0.4125072956085205\n",
      "Weight 739: 0.3398781716823578\n",
      "Weight 740: -0.2737811803817749\n",
      "Weight 741: -0.08340084552764893\n",
      "Weight 742: 0.4240143597126007\n",
      "Weight 743: -0.025404244661331177\n",
      "Weight 744: 0.4197002351284027\n",
      "Weight 745: -0.18196922540664673\n",
      "Weight 746: 0.3991558849811554\n",
      "Weight 747: -0.3658052086830139\n",
      "Weight 748: 0.11358943581581116\n",
      "Weight 749: 0.05820605158805847\n",
      "Weight 750: -0.29174184799194336\n",
      "Weight 751: 0.3593588173389435\n",
      "Weight 752: -0.5575593709945679\n",
      "Weight 753: -0.27758169174194336\n",
      "Weight 754: 0.2315976321697235\n",
      "Weight 755: 0.47299450635910034\n",
      "Weight 756: 0.4725334644317627\n",
      "Weight 757: -0.01677432842552662\n",
      "Weight 758: 0.28164830803871155\n",
      "Weight 759: -0.26979920268058777\n",
      "Weight 760: -0.3225286304950714\n",
      "Weight 761: 0.23233294486999512\n",
      "Weight 762: -0.38225674629211426\n",
      "Weight 763: -0.3297237753868103\n",
      "Weight 764: 0.28506436944007874\n",
      "Weight 765: 0.18521477282047272\n",
      "Weight 766: 0.1134343221783638\n",
      "Weight 767: 0.10704725980758667\n",
      "Weight 768: 0.22042496502399445\n",
      "Weight 769: -0.017679443582892418\n",
      "Weight 770: -0.3734009563922882\n",
      "Weight 771: 0.013211998157203197\n",
      "Weight 772: 0.12866149842739105\n",
      "Weight 773: -0.12641912698745728\n",
      "Weight 774: -0.17802807688713074\n",
      "Weight 775: 0.16930469870567322\n",
      "Weight 776: 0.20178471505641937\n",
      "Weight 777: 0.1027572900056839\n",
      "Weight 778: -0.04418322443962097\n",
      "Weight 779: 0.28609567880630493\n",
      "Weight 780: -0.05994081869721413\n",
      "Weight 781: 0.3907203674316406\n",
      "Weight 782: -0.6068621277809143\n",
      "Weight 783: 0.1715555191040039\n",
      "Weight 784: 0.13697251677513123\n",
      "Weight 785: -0.12502911686897278\n",
      "Weight 786: -0.21400736272335052\n",
      "Weight 787: -0.17833732068538666\n",
      "Weight 788: 0.3113757371902466\n",
      "Weight 789: 0.23712502419948578\n",
      "Weight 790: 0.45184868574142456\n",
      "Weight 791: 0.37553197145462036\n",
      "Weight 792: 0.3771797716617584\n",
      "Weight 793: -0.011793756857514381\n",
      "Weight 794: 0.30661699175834656\n",
      "Weight 795: -0.3532490134239197\n",
      "Weight 796: -0.2214089035987854\n",
      "Weight 797: -0.018686464056372643\n",
      "Weight 798: -0.2096128612756729\n",
      "Weight 799: -0.0724601298570633\n",
      "Weight 800: 0.06842855364084244\n",
      "Weight 801: -0.0971999242901802\n",
      "Weight 802: -0.42727130651474\n",
      "Weight 803: -0.1581237018108368\n",
      "Weight 804: 0.2607118487358093\n",
      "Weight 805: 0.22946178913116455\n",
      "Weight 806: 0.34878596663475037\n",
      "Weight 807: 0.06731685996055603\n",
      "Weight 808: 0.3410678803920746\n",
      "Weight 809: 0.37236014008522034\n",
      "Weight 810: -0.03932046517729759\n",
      "Weight 811: -0.2737487554550171\n",
      "Weight 812: 0.20071206986904144\n",
      "Weight 813: 0.3088001012802124\n",
      "Weight 814: -0.3624889552593231\n",
      "Weight 815: 0.1851264089345932\n",
      "Weight 816: 0.1680106520652771\n",
      "Weight 817: 0.20628482103347778\n",
      "Weight 818: 0.17492835223674774\n",
      "Weight 819: 0.4128255546092987\n",
      "Weight 820: 0.09045374393463135\n",
      "Weight 821: 0.27602189779281616\n",
      "Weight 822: 0.4980630874633789\n",
      "Weight 823: -0.29069873690605164\n",
      "Weight 824: 0.30598998069763184\n",
      "Weight 825: -0.1921040117740631\n",
      "Weight 826: 0.3027288019657135\n",
      "Weight 827: -0.4364384412765503\n",
      "Weight 828: 0.3252452611923218\n",
      "Weight 829: 0.04843098297715187\n",
      "Weight 830: 0.08678882569074631\n",
      "Weight 831: -0.4376097619533539\n",
      "Weight 832: -0.14664295315742493\n",
      "Weight 833: 0.0982120931148529\n",
      "Weight 834: 0.11763247847557068\n",
      "Weight 835: -0.04267188906669617\n",
      "Weight 836: 0.35205408930778503\n",
      "Weight 837: 0.39545831084251404\n",
      "Weight 838: 0.01490214467048645\n",
      "Weight 839: 0.11122682690620422\n",
      "Weight 840: -0.022626012563705444\n",
      "Weight 841: -0.3937825560569763\n",
      "Weight 842: 0.43269893527030945\n",
      "Weight 843: -0.03274893760681152\n",
      "Weight 844: -0.1382879614830017\n",
      "Weight 845: 0.024887144565582275\n",
      "Weight 846: 0.2749234139919281\n",
      "Weight 847: -0.1927323341369629\n",
      "Weight 848: -0.3499562740325928\n",
      "Weight 849: 0.0749586969614029\n",
      "Weight 850: -0.4127981662750244\n",
      "Weight 851: 0.1804853081703186\n",
      "Weight 852: 0.23486340045928955\n",
      "Weight 853: -0.31455451250076294\n",
      "Weight 854: 0.21299611032009125\n",
      "Weight 855: 0.14463068544864655\n",
      "Weight 856: -0.09070223569869995\n",
      "Weight 857: 0.17181704938411713\n",
      "Weight 858: 0.2840005159378052\n",
      "Weight 859: 0.3316959738731384\n",
      "Weight 860: 0.1989276260137558\n",
      "Weight 861: 0.27310478687286377\n",
      "Weight 862: 0.42466050386428833\n",
      "Weight 863: 0.23377686738967896\n",
      "Weight 864: 0.3097647726535797\n",
      "Weight 865: 0.09325572848320007\n",
      "Weight 866: -0.13989683985710144\n",
      "Weight 867: 0.14479497075080872\n",
      "Weight 868: -0.1412470042705536\n",
      "Weight 869: 0.03883957862854004\n",
      "Weight 870: -0.3282565176486969\n",
      "Weight 871: 0.05369475483894348\n",
      "Weight 872: 0.10774728655815125\n",
      "Weight 873: 0.39862939715385437\n",
      "Weight 874: 0.22944489121437073\n",
      "Weight 875: 0.28739675879478455\n",
      "Weight 876: 0.12703529000282288\n",
      "Weight 877: -0.3837756812572479\n",
      "Weight 878: -0.012970954179763794\n",
      "Weight 879: -0.26347872614860535\n",
      "Weight 880: 0.41144078969955444\n",
      "Weight 881: 0.04025636985898018\n",
      "Weight 882: 0.2636720836162567\n",
      "Weight 883: -0.41915637254714966\n",
      "Weight 884: -0.2144278734922409\n",
      "Weight 885: -0.03200708329677582\n",
      "Weight 886: -0.2832812964916229\n",
      "Weight 887: -0.04426876828074455\n",
      "Weight 888: 0.06526269763708115\n",
      "Weight 889: 0.08577582240104675\n",
      "Weight 890: 0.32938843965530396\n",
      "Weight 891: -0.15008188784122467\n",
      "Weight 892: -0.2682599425315857\n",
      "Weight 893: 0.16777168214321136\n",
      "Weight 894: 0.0032757585868239403\n",
      "Weight 895: 0.08952760696411133\n",
      "Weight 896: 0.25545892119407654\n",
      "Weight 897: 0.40689972043037415\n",
      "Weight 898: 0.05139738321304321\n",
      "Weight 899: -0.12275153398513794\n",
      "Weight 900: 0.30026230216026306\n",
      "Weight 901: 0.10019931942224503\n",
      "Weight 902: -0.2541772127151489\n",
      "Weight 903: 0.08551011234521866\n",
      "Weight 904: 0.10585769265890121\n",
      "Weight 905: 0.14373308420181274\n",
      "Weight 906: 0.30950966477394104\n",
      "Weight 907: -0.05225187540054321\n",
      "Weight 908: 0.12656141817569733\n",
      "Weight 909: -0.00092755735386163\n",
      "Weight 910: 0.15732142329216003\n",
      "Weight 911: -0.3789449632167816\n",
      "Weight 912: 0.27250906825065613\n",
      "Weight 913: 0.18969061970710754\n",
      "Weight 914: 0.41996511816978455\n",
      "Weight 915: -0.3762357831001282\n",
      "Weight 916: 0.16064950823783875\n",
      "Weight 917: 0.24074026942253113\n",
      "Weight 918: -0.37409254908561707\n",
      "Weight 919: -0.1884719878435135\n",
      "Weight 920: -0.3594549000263214\n",
      "Weight 921: 0.36933669447898865\n",
      "Weight 922: -0.20832976698875427\n",
      "Weight 923: 0.212068110704422\n",
      "Weight 924: 0.1417008340358734\n",
      "Weight 925: 0.14842566847801208\n",
      "Weight 926: -0.2916278839111328\n",
      "Weight 927: 0.2303682267665863\n",
      "Weight 928: 0.09701874852180481\n",
      "Weight 929: -0.637924313545227\n",
      "Weight 930: -0.2878488302230835\n",
      "Weight 931: -0.47223931550979614\n",
      "Weight 932: 0.3123607635498047\n",
      "Weight 933: 0.15811587870121002\n",
      "Weight 934: 0.0765174925327301\n",
      "Weight 935: 0.0858563706278801\n",
      "Weight 936: -0.01935753785073757\n",
      "Weight 937: 0.18523932993412018\n",
      "Weight 938: 0.27952322363853455\n",
      "Weight 939: 0.22034236788749695\n",
      "Weight 940: -0.038289014250040054\n",
      "Weight 941: -0.04680873081088066\n",
      "Weight 942: -0.3526778221130371\n",
      "Weight 943: -0.13798534870147705\n",
      "Weight 944: 0.15397748351097107\n",
      "Weight 945: -0.05450713634490967\n",
      "Weight 946: 0.06257498264312744\n",
      "Weight 947: 0.002395123243331909\n",
      "Weight 948: 0.2661907970905304\n",
      "Weight 949: -0.24889962375164032\n",
      "Weight 950: 0.3902417719364166\n",
      "Weight 951: 0.09198036789894104\n",
      "Weight 952: -0.38782477378845215\n",
      "Weight 953: -0.02698338031768799\n",
      "Weight 954: 0.05998143553733826\n",
      "Weight 955: -0.2502952218055725\n",
      "Weight 956: -0.08071509003639221\n",
      "Weight 957: 0.17653122544288635\n",
      "Weight 958: 0.32727256417274475\n",
      "Weight 959: -0.16374385356903076\n",
      "Weight 960: 0.35100075602531433\n",
      "Weight 961: 0.0990619957447052\n",
      "Weight 962: 0.29266759753227234\n",
      "Weight 963: -0.0891752541065216\n",
      "Weight 964: -0.379865437746048\n",
      "Weight 965: 0.23979678750038147\n",
      "Weight 966: -0.4286803901195526\n",
      "Weight 967: -0.09503990411758423\n",
      "Weight 968: 0.2450980842113495\n",
      "Weight 969: 0.13272610306739807\n",
      "Weight 970: 0.11009517312049866\n",
      "Weight 971: -0.3910648226737976\n",
      "Weight 972: 0.20897457003593445\n",
      "Weight 973: -0.28196415305137634\n",
      "Weight 974: 0.13838163018226624\n",
      "Weight 975: 0.022784382104873657\n",
      "Weight 976: -0.49791839718818665\n",
      "Weight 977: -0.4108036756515503\n",
      "Weight 978: 0.38501477241516113\n",
      "Weight 979: -0.7521299719810486\n",
      "Weight 980: 0.5385086536407471\n",
      "Weight 981: 0.5797131657600403\n",
      "Weight 982: 0.3695199489593506\n",
      "Weight 983: -0.42756104469299316\n",
      "Weight 984: 0.2726542353630066\n",
      "Weight 985: -0.3020811676979065\n",
      "Weight 986: -0.0358794629573822\n",
      "Weight 987: 0.6849765181541443\n",
      "Weight 988: 0.3800610601902008\n",
      "Weight 989: 0.3938862681388855\n",
      "Weight 990: -0.9354832768440247\n",
      "Weight 991: -0.2777751088142395\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'biases' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [93]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeight \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Print all the biases\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, bias \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mbiases\u001b[49m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBias \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbias\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'biases' is not defined"
     ]
    }
   ],
   "source": [
    "for i, weight in enumerate(weights):\n",
    "    print(f\"Weight {i}: {weight}\")\n",
    "\n",
    "# Print all the biases\n",
    "for i, bias in enumerate(biases):\n",
    "    print(f\"Bias {i}: {bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c5932739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "992"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f42a24a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'biases' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [98]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbiases[] = \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_biases \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbiases\u001b[49m:\n\u001b[1;32m     10\u001b[0m     layer_biases \u001b[38;5;241m=\u001b[39m layer_biases\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m bias \u001b[38;5;129;01min\u001b[39;00m layer_biases:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'biases' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"weights_and_biases.txt\", \"w\") as f:\n",
    "    f.write('weights[] = ')\n",
    "    for layer_weights in weights:\n",
    "        layer_weights = layer_weights.flatten()\n",
    "        for weight in layer_weights:\n",
    "            f.write(str(weight) + ',')\n",
    "    f.write('\\n')\n",
    "    f.write('biases[] = ')\n",
    "    for layer_biases in biases:\n",
    "        layer_biases = layer_biases.flatten()\n",
    "        for bias in layer_biases:\n",
    "            f.write(str(bias) + ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4408bfb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'biases' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [99]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     layer_weights \u001b[38;5;241m=\u001b[39m layer_weights\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m      4\u001b[0m     layer_weights\u001b[38;5;241m.\u001b[39mtofile(f)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_biases \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbiases\u001b[49m:\n\u001b[1;32m      6\u001b[0m     layer_biases \u001b[38;5;241m=\u001b[39m layer_biases\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m      7\u001b[0m     layer_biases\u001b[38;5;241m.\u001b[39mtofile(f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'biases' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"weights_and_biases.bin\", \"wb\") as f:\n",
    "    for layer_weights in weights:\n",
    "        layer_weights = layer_weights.flatten()\n",
    "        layer_weights.tofile(f)\n",
    "    for layer_biases in biases:\n",
    "        layer_biases = layer_biases.flatten()\n",
    "        layer_biases.tofile(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318b8a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fa19edf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3824148]]\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"tflite_quant_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Test the model on random input data.\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "interpreter.invoke()\n",
    "\n",
    "# The function `get_tensor()` returns a copy of the tensor data.\n",
    "# Use `tensor()` in order to get a pointer to the tensor.\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c8181084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 1.27029851e-01,  2.45505348e-01,  4.81742322e-01,\n",
       "          5.04379719e-02, -3.36665630e-01, -1.31173074e-01,\n",
       "         -1.36439921e-02, -2.53146291e-02,  1.90617591e-01,\n",
       "          7.00861076e-03, -7.47947931e-01,  3.45697582e-01,\n",
       "          6.26459241e-01,  1.80734813e-01,  4.97436523e-01,\n",
       "          2.79962365e-02],\n",
       "        [ 1.26229599e-01, -2.95625448e-01,  4.19894814e-01,\n",
       "          1.12839296e-01, -7.65909031e-02, -8.95742401e-02,\n",
       "         -3.84261191e-01, -8.77562463e-02,  4.11512285e-01,\n",
       "          1.13839038e-01,  3.91039073e-01,  3.51129800e-01,\n",
       "          1.43260092e-01, -3.72224867e-01, -5.04614592e-01,\n",
       "         -2.65248001e-01],\n",
       "        [-2.61226088e-01, -3.34835440e-01, -1.78556353e-01,\n",
       "         -1.92304134e-01, -1.45662427e-01, -3.11683059e-01,\n",
       "         -5.07401168e-01, -4.18776095e-01, -3.01956922e-01,\n",
       "         -4.47872072e-01, -2.42011055e-01, -1.17778562e-01,\n",
       "          6.26041815e-02, -4.52281833e-02, -8.36840048e-02,\n",
       "         -1.01914011e-01],\n",
       "        [ 3.73750538e-01,  9.94419008e-02,  4.91657443e-02,\n",
       "          2.02814490e-01, -4.02796090e-01,  1.07014901e-04,\n",
       "          1.93409711e-01,  2.50604630e-01, -2.36657903e-01,\n",
       "         -4.63621885e-01,  2.81814009e-01, -3.70264113e-01,\n",
       "          3.87472928e-01,  1.17334247e-01,  1.03144169e-01,\n",
       "         -2.10748576e-02],\n",
       "        [-1.35171175e-01, -2.80588001e-01, -4.46814716e-01,\n",
       "          2.75922835e-01,  2.72374541e-01,  7.56329149e-02,\n",
       "          3.48690748e-01, -3.04241747e-01,  1.73728853e-01,\n",
       "          1.70742989e-01, -1.96540803e-01, -1.68144152e-01,\n",
       "         -2.95600981e-01,  1.77913606e-02, -2.31427804e-01,\n",
       "          1.51874796e-01],\n",
       "        [ 2.48181680e-03,  2.81291932e-01,  8.39392841e-02,\n",
       "         -1.99895948e-01,  3.23463589e-01,  5.35556078e-01,\n",
       "          9.12938938e-02,  2.63442516e-01,  3.40099365e-01,\n",
       "         -1.41648158e-01,  3.76273304e-01,  2.54578978e-01,\n",
       "         -2.12680236e-01, -4.43095952e-01,  3.76928270e-01,\n",
       "          2.65293777e-01],\n",
       "        [-1.25729907e-02,  3.98961902e-01, -3.42552722e-01,\n",
       "         -2.30075434e-01, -3.16535264e-01, -2.88868904e-01,\n",
       "          1.61475465e-02, -8.11131001e-02, -1.91408113e-01,\n",
       "         -7.63348788e-02,  2.70322114e-01,  2.56696284e-01,\n",
       "          2.93052584e-01,  2.04320729e-01, -2.29661882e-01,\n",
       "          2.54990608e-01],\n",
       "        [-2.74003834e-01, -3.26196879e-01,  5.07763743e-01,\n",
       "         -1.46705464e-01, -2.44725347e-01,  6.25644699e-02,\n",
       "          4.08058912e-01, -3.82553458e-01, -1.42282888e-01,\n",
       "          3.64631653e-01,  3.44957024e-01, -3.52603257e-01,\n",
       "         -2.77274489e-01, -8.21644962e-02,  4.09718394e-01,\n",
       "          1.89660758e-01],\n",
       "        [-2.12372318e-01,  1.49757350e-02,  4.25501347e-01,\n",
       "         -2.95061111e-01,  2.99593419e-01,  8.60694051e-02,\n",
       "         -2.88361877e-01,  9.73296165e-02, -2.13473499e-01,\n",
       "          1.86712265e-01,  3.21151987e-02,  3.93119127e-01,\n",
       "         -1.93344533e-01, -2.48645633e-01, -1.25784025e-01,\n",
       "         -4.31973748e-02],\n",
       "        [ 3.44813496e-01,  8.01010951e-02,  4.12267119e-01,\n",
       "         -2.63707906e-01,  4.11341578e-01,  3.09940457e-01,\n",
       "          2.39912093e-01, -2.64430702e-01, -4.57494140e-01,\n",
       "         -1.07858546e-01, -6.76870495e-02, -1.49300337e-01,\n",
       "         -1.90048724e-01, -3.00117314e-01,  1.85947284e-01,\n",
       "          3.74968499e-01],\n",
       "        [ 3.04606050e-01,  2.73271084e-01, -2.45824113e-01,\n",
       "         -7.53797069e-02,  3.12871665e-01, -2.90125191e-01,\n",
       "          4.83509213e-01,  3.78977835e-01,  4.14214849e-01,\n",
       "         -4.33280289e-01, -1.97977908e-02,  1.48836285e-01,\n",
       "         -1.62041962e-01,  8.05356503e-02, -1.13100640e-01,\n",
       "          2.36595944e-01],\n",
       "        [ 7.82522112e-02, -8.38612914e-02,  4.09163088e-01,\n",
       "          2.73081094e-01,  5.85170202e-02,  4.92584974e-01,\n",
       "          1.22696059e-02, -2.12206855e-01, -4.95402142e-02,\n",
       "          4.77121562e-01, -2.84380823e-01,  7.07305297e-02,\n",
       "          2.69610554e-01,  1.02548599e-01,  4.99367476e-01,\n",
       "          8.49791989e-02],\n",
       "        [-2.81015366e-01,  4.24422383e-01,  3.66591930e-01,\n",
       "         -2.57691920e-01, -3.03716123e-01, -2.44996890e-01,\n",
       "         -3.13039236e-02, -2.48336121e-01, -1.84550509e-01,\n",
       "          3.28034729e-01, -3.03362668e-01, -6.69213310e-02,\n",
       "         -1.02207348e-01,  8.25152993e-02, -4.23628718e-01,\n",
       "          1.11985274e-01]], dtype=float32),\n",
       " array([ 0.03207278, -0.01262236, -0.04897233, -0.0380093 ,  0.05061208,\n",
       "         0.09343849, -0.05249299,  0.        , -0.0153411 ,  0.03151024,\n",
       "         0.01508466,  0.00914021, -0.03053932,  0.        , -0.09502531,\n",
       "        -0.05841714], dtype=float32),\n",
       " array([[ 0.05036739,  0.27147144,  0.14424475, -0.3504259 , -0.35088956,\n",
       "         -0.291749  ,  0.19209969,  0.24611887, -0.08317805,  0.2871938 ,\n",
       "         -0.13279228,  0.01384995,  0.29560027,  0.425641  ,  0.03031263,\n",
       "         -0.24155974],\n",
       "        [-0.34474388,  0.2715455 ,  0.2876473 , -0.69825244, -0.05869723,\n",
       "          0.31338108, -0.11258519,  0.27770963,  0.32941303, -0.12709886,\n",
       "          0.22523007,  0.28646517, -0.04615577,  0.04104562,  0.21150073,\n",
       "         -0.1613506 ],\n",
       "        [-0.3165401 , -0.29850218,  0.34241092,  0.20430088,  0.231312  ,\n",
       "          0.4047479 ,  0.36107782, -0.13532111, -0.04531683,  0.30538124,\n",
       "         -0.4478914 , -0.21974988, -0.24123596,  0.17742932, -0.24924299,\n",
       "          0.28013504],\n",
       "        [ 0.08838239,  0.13310757,  0.4442512 ,  0.32742566,  0.17529823,\n",
       "          0.32191372,  0.20179197,  0.43295446,  0.24239469,  0.36175117,\n",
       "         -0.08952963, -0.48390132, -0.3685352 , -0.31986916, -0.2607425 ,\n",
       "         -0.2611396 ],\n",
       "        [ 0.2326329 , -0.26784578,  0.21707521, -0.04185307, -0.46198538,\n",
       "         -0.30273375, -0.45165125,  0.00559881, -0.04184304,  0.63120437,\n",
       "          0.20464443,  0.40006414,  0.0915335 , -0.5788727 ,  0.05826366,\n",
       "         -0.6300037 ],\n",
       "        [ 0.41408798,  0.4612322 , -0.6313794 ,  0.20155357, -0.27663842,\n",
       "         -0.71005464, -0.02430936, -0.22512622,  0.11273461,  0.421303  ,\n",
       "          0.0255163 , -0.27046666,  0.38152638, -0.07965446, -0.2182745 ,\n",
       "          0.1388688 ],\n",
       "        [-0.1480287 ,  0.34989303, -0.2931908 , -0.12469447, -0.08743852,\n",
       "         -0.04053458,  0.08153639, -0.3536046 , -0.0629887 , -0.23525962,\n",
       "          0.1811326 ,  0.32994547, -0.73385596,  0.3994773 , -0.37374938,\n",
       "         -0.2135634 ],\n",
       "        [-0.11550897,  0.31350818, -0.21268405,  0.0785763 ,  0.40547523,\n",
       "          0.07236454,  0.40952536,  0.4302998 ,  0.0122838 , -0.2964008 ,\n",
       "         -0.35822946, -0.05871812,  0.12166443,  0.29132983,  0.4328436 ,\n",
       "          0.14740804],\n",
       "        [-0.21416253, -0.01605702, -0.2424536 , -0.00901088, -0.5331072 ,\n",
       "          0.19611712,  0.5526784 , -0.19049588,  0.17949526, -0.5803558 ,\n",
       "          0.20413056, -0.01496147, -0.28339872,  0.36273685, -0.16164523,\n",
       "          0.44923785],\n",
       "        [ 0.2415953 , -0.05365774, -0.37838006, -0.07550462, -0.55512816,\n",
       "         -0.8132363 ,  0.11615902,  0.17252448,  0.8824875 ,  0.11241087,\n",
       "         -0.16794278,  0.68308   , -0.1051604 , -0.29364157, -0.15410572,\n",
       "          0.50108707],\n",
       "        [-0.22740346, -0.3553469 , -0.0025746 ,  0.01273974,  0.31886363,\n",
       "          0.02974217, -0.46253344, -0.12411946,  0.14822616,  0.14656103,\n",
       "          0.02510446, -0.15203021, -0.03651529, -0.10441773, -0.07433084,\n",
       "         -0.34856865],\n",
       "        [ 0.18871883, -0.04244082,  0.02540225,  0.05929858, -0.18155561,\n",
       "          0.27483684,  0.29284737,  0.21850762,  0.205074  , -0.37692523,\n",
       "          0.17299652, -0.25053087,  0.10725386,  0.23072642, -0.26189712,\n",
       "          0.00106586],\n",
       "        [-0.41925928, -0.24547248, -0.3831078 , -0.16546483,  0.3795869 ,\n",
       "         -0.37127432,  0.03011034, -0.2199735 ,  0.00401656, -0.97292185,\n",
       "         -0.24478006, -0.21413732, -0.05893128,  0.3728136 ,  0.31990752,\n",
       "         -0.41604862],\n",
       "        [ 0.1496742 , -0.31546283, -0.10992905, -0.04425731, -0.4012024 ,\n",
       "         -0.39680466,  0.41213873, -0.3540096 , -0.3670774 , -0.30551362,\n",
       "         -0.2239014 ,  0.22170636,  0.15848354,  0.414516  , -0.07408893,\n",
       "          0.24689391],\n",
       "        [-0.2407689 , -0.39418963, -0.01934803,  0.3106969 ,  0.52138025,\n",
       "         -0.00568076,  0.16400102, -0.26011813, -0.8566191 , -0.2941104 ,\n",
       "          0.19952372, -0.28793222,  0.23987618,  0.6720418 , -0.24324052,\n",
       "         -0.709556  ],\n",
       "        [ 0.00973743, -0.06793562,  0.42269638, -0.40247357, -0.04019399,\n",
       "          0.07591623,  0.32708934, -0.3169478 ,  0.04947993, -0.16435824,\n",
       "          0.30565313,  0.2313882 ,  0.27134994, -0.28559408, -0.11228421,\n",
       "          0.25138107]], dtype=float32),\n",
       " array([ 0.        ,  0.09087443, -0.00894577, -0.00527815, -0.03352903,\n",
       "        -0.02475152, -0.03203318,  0.        ,  0.01053201, -0.02737895,\n",
       "        -0.00595696, -0.00343854,  0.084959  , -0.00857461,  0.        ,\n",
       "        -0.02119412], dtype=float32),\n",
       " array([[ 0.26922116,  0.41252086,  0.16740951,  0.22785142,  0.10005185,\n",
       "          0.34257165,  0.03031343, -0.27814198,  0.27290985, -0.00435418,\n",
       "         -0.21388905,  0.38354906, -0.13417673,  0.13530675, -0.04794717,\n",
       "         -0.23484425],\n",
       "        [-0.2594829 ,  0.6313282 ,  0.03280072,  0.25119272,  0.34672758,\n",
       "         -0.39260224, -0.01345699, -0.20643649,  0.654526  , -0.3098382 ,\n",
       "          0.7890141 ,  0.22034197,  0.2196239 ,  0.0173122 , -0.03335884,\n",
       "          0.06357985],\n",
       "        [-0.23669206, -0.09076017,  0.11366034, -0.05673633,  0.18461803,\n",
       "         -0.16896915,  0.24328554, -0.41703585,  0.25918326,  0.40901887,\n",
       "          0.30466798, -0.38470632, -0.19918425, -0.0510148 ,  0.01045172,\n",
       "         -0.05169089],\n",
       "        [ 0.11609474,  0.24365756,  0.39698094, -0.27747026, -0.13991031,\n",
       "         -0.29523057, -0.32720733, -0.10154557, -0.5551015 , -0.29332358,\n",
       "         -0.5273829 ,  0.5171859 ,  0.00521675, -0.2519405 ,  0.10299616,\n",
       "         -0.3834057 ],\n",
       "        [-0.4550442 , -0.67300934,  0.27591157, -0.11562579,  0.36672884,\n",
       "         -0.20020708, -0.45277455, -0.21700582, -0.14506501,  0.18431443,\n",
       "         -0.08218054,  0.12899727,  0.12183139,  0.02647653,  0.27811077,\n",
       "         -0.34303614],\n",
       "        [-0.14486018, -0.34697312,  0.09500189,  0.32452044, -0.2286574 ,\n",
       "         -0.4105272 , -0.30898163,  0.03649348,  0.2911105 ,  0.20216928,\n",
       "          0.3427086 ,  0.11504329, -0.17857625,  0.37598908, -0.17527546,\n",
       "          0.09866451],\n",
       "        [-0.08021122,  0.21330373,  0.20940833,  0.29085597,  0.34055462,\n",
       "         -0.08217001,  0.43999228, -0.07950038, -0.09875926,  0.31169388,\n",
       "         -0.33594233, -0.15716557, -0.34921694, -0.2958147 , -0.06189433,\n",
       "          0.11468734],\n",
       "        [ 0.21291265,  0.14861265, -0.27842999, -0.4114228 ,  0.16536036,\n",
       "          0.14603135, -0.28354836, -0.01477566, -0.24140288,  0.33541235,\n",
       "          0.01363921, -0.11832654,  0.07413056, -0.4146337 , -0.16516253,\n",
       "          0.38217363],\n",
       "        [ 0.58320796,  0.25559098, -0.1121136 , -0.1605173 , -0.2037751 ,\n",
       "         -0.24068537,  0.4119308 , -0.36008   ,  0.02470509, -0.16821568,\n",
       "          0.45073748,  0.19229999,  0.13139263, -0.10444726,  0.31124353,\n",
       "          0.4112335 ],\n",
       "        [ 0.03402914,  0.43421367,  0.42995834, -0.21641761, -0.3006861 ,\n",
       "         -0.41523528, -0.11425874, -0.34821382,  0.03810212, -0.3267967 ,\n",
       "         -0.15221353,  0.7424789 ,  0.14269945, -0.53309774, -0.12752868,\n",
       "          0.23717104],\n",
       "        [-0.08087239,  0.393166  ,  0.309606  , -0.24543332, -0.4105276 ,\n",
       "          0.02235159,  0.52189505,  0.02730858, -0.40641084, -0.09021745,\n",
       "         -0.4615444 ,  0.14193954,  0.1811011 ,  0.6757378 ,  0.3186826 ,\n",
       "          0.30944902],\n",
       "        [-0.76645976,  0.32626507,  0.11102686, -0.45319396,  0.10421974,\n",
       "          0.4267635 ,  0.2091666 , -0.42175445, -0.28539944, -0.16220695,\n",
       "         -0.01166828, -0.18689139,  0.02773236, -0.02075723,  0.448579  ,\n",
       "          0.4375435 ],\n",
       "        [-0.14804287,  0.5521518 ,  0.19752087, -0.8345185 , -0.44032222,\n",
       "         -0.14671975,  0.42988056, -0.18591881, -0.25783804, -0.6054113 ,\n",
       "          0.53223515, -0.4872396 , -0.24440914, -0.1632129 ,  0.41198406,\n",
       "          0.01573527],\n",
       "        [ 0.21015553, -0.17472267,  0.42991763, -0.05004979,  0.32672018,\n",
       "          0.15148678, -0.20763868, -0.26826215,  0.16620976,  0.15822503,\n",
       "         -0.54229033,  0.18057014, -0.39826238, -0.3885686 ,  0.01924704,\n",
       "         -0.4658302 ],\n",
       "        [-0.04076549,  0.21128663, -0.2692675 , -0.18139306,  0.2558103 ,\n",
       "          0.1828483 ,  0.08938894,  0.33875635, -0.12009656,  0.17005524,\n",
       "         -0.41430655,  0.01715934, -0.16482618, -0.0860303 ,  0.03504062,\n",
       "         -0.36364153],\n",
       "        [ 0.3570312 ,  0.16796926,  0.25222698,  0.3328152 , -0.35136408,\n",
       "          0.19153252, -0.07865159, -0.22971588, -0.02541613,  0.275356  ,\n",
       "         -0.22566369,  0.48943323, -0.11901518,  0.32377243,  0.00607678,\n",
       "          0.4100571 ]], dtype=float32),\n",
       " array([ 0.04353456,  0.08565651,  0.01276425, -0.02569238, -0.01176088,\n",
       "         0.        , -0.01389938,  0.        , -0.00409375, -0.00161908,\n",
       "         0.0274261 , -0.01011752, -0.03035641, -0.06359777,  0.0508352 ,\n",
       "        -0.00010304], dtype=float32),\n",
       " array([[ 0.18232778, -0.11758199, -0.37802985,  0.42538437,  0.13645849,\n",
       "         -0.65545285, -0.82645327,  0.01262562,  0.10023026, -0.24493258,\n",
       "         -0.46085593, -0.4901879 ,  0.14824137,  0.06328198, -0.22504005,\n",
       "         -0.51914936],\n",
       "        [ 0.40941694,  0.22168377,  0.11893236, -0.24815582, -0.11639887,\n",
       "         -0.4710141 , -0.85186124, -0.42299682,  0.0687943 , -0.7953726 ,\n",
       "         -0.24517293, -0.29486308, -0.17517878,  0.40323478, -0.38416043,\n",
       "         -0.05315796],\n",
       "        [-0.17483957, -0.27824706, -0.1753062 ,  0.14309019, -0.05747244,\n",
       "         -0.27895117, -0.13807634,  0.08744512,  0.34812787, -0.02774223,\n",
       "          0.4371515 ,  0.22731572, -0.25490355,  0.09085872,  0.21120498,\n",
       "          0.26112896],\n",
       "        [ 0.27783972, -0.07066464, -0.0630412 ,  0.45283347, -0.30746132,\n",
       "          0.14927271,  0.45553562, -0.25522017,  0.09819611, -0.14624587,\n",
       "         -0.01050393,  0.20662029, -0.42864645,  0.33307534, -0.19627252,\n",
       "          0.23196219],\n",
       "        [ 0.26180542, -0.22773083, -0.12242159,  0.16859712, -0.04583213,\n",
       "          0.40500236,  0.21809986, -0.08594595, -0.05340602,  0.24175803,\n",
       "          0.36543798,  0.14486967, -0.29815167,  0.27606001, -0.15494467,\n",
       "          0.32049513],\n",
       "        [ 0.34043524, -0.20595963,  0.1253641 ,  0.28944483, -0.33976138,\n",
       "          0.27708802,  0.34289995,  0.36685815, -0.29582193,  0.3556762 ,\n",
       "          0.3882564 , -0.4251196 ,  0.30588487,  0.34256747,  0.22306332,\n",
       "          0.42983267],\n",
       "        [ 0.06059904,  0.08289394,  0.08770084,  0.45410213, -0.3971822 ,\n",
       "         -0.34774956, -0.34342143,  0.36873516, -0.2425006 , -0.24805169,\n",
       "          0.13422109,  0.31580666, -0.3337091 ,  0.30946198, -0.0140696 ,\n",
       "          0.3103392 ],\n",
       "        [ 0.18896773, -0.17411578,  0.24159357, -0.20283347,  0.39634112,\n",
       "          0.24676386,  0.23925188,  0.3348684 ,  0.36899033, -0.12581277,\n",
       "          0.06758782, -0.04329017,  0.13773361,  0.27215478, -0.03177643,\n",
       "         -0.2756409 ],\n",
       "        [ 0.4111483 ,  0.30354074, -0.0812033 ,  0.00455744,  0.01325828,\n",
       "          0.00178322,  0.425471  , -0.3846273 ,  0.27991062, -0.09172085,\n",
       "         -0.08400612,  0.18141235, -0.30707538, -0.45890543,  0.06042185,\n",
       "         -0.28822035],\n",
       "        [-0.00210235, -0.07135716,  0.10472821,  0.12432852, -0.166787  ,\n",
       "          0.11534263,  0.32723424, -0.35482565, -0.15710734,  0.41715744,\n",
       "          0.29342422,  0.08297866,  0.35987937, -0.09007611, -0.01177206,\n",
       "          0.03428398],\n",
       "        [ 0.34241846, -0.19881575,  0.2864413 ,  0.41746086,  0.00926018,\n",
       "         -0.21354434,  0.39195323, -0.25009698,  0.30611977,  0.4228399 ,\n",
       "          0.14409696, -0.32677156,  0.16493149,  0.18167293, -0.02442178,\n",
       "          0.1633391 ],\n",
       "        [-0.10460383, -0.24212822, -0.7199722 , -0.4821299 ,  0.04882324,\n",
       "          0.3495239 , -0.8091414 ,  0.16754705,  0.15364563,  0.02450196,\n",
       "         -0.6107419 ,  0.23854731, -0.3306598 , -0.39985436,  0.096071  ,\n",
       "         -0.19833091],\n",
       "        [-0.3224495 ,  0.03063884, -0.09089322,  0.19204314,  0.3968949 ,\n",
       "         -0.0267764 , -0.08563471, -0.30384138, -0.42645323,  0.12163964,\n",
       "         -0.20189163,  0.00497636, -0.3385594 ,  0.07681009,  0.15423682,\n",
       "          0.4218169 ],\n",
       "        [-0.49120373,  0.05962083,  0.01931936,  0.00131121,  0.13865176,\n",
       "          0.1392358 , -0.23441371, -0.04525831, -0.42193383,  0.12143654,\n",
       "         -0.26280788,  0.23759612, -0.07677428, -0.48504138, -0.4241964 ,\n",
       "          0.02816955],\n",
       "        [ 0.25351322, -0.40174264,  0.35833797,  0.16302295,  0.38559595,\n",
       "          0.43131483, -0.553778  ,  0.03250712, -0.0202041 , -0.19514091,\n",
       "         -0.42384174,  0.07916273,  0.38506454, -0.24540739,  0.0957932 ,\n",
       "          0.4771191 ],\n",
       "        [-0.19547068, -0.2771933 ,  0.32275084, -0.39821234, -0.417503  ,\n",
       "          0.06732705, -0.44084847, -0.4591294 ,  0.03311022, -0.41726616,\n",
       "         -0.4106282 , -0.01709341, -0.29955983, -0.35370448, -0.3459945 ,\n",
       "          0.03047987]], dtype=float32),\n",
       " array([-0.02774075,  0.        , -0.03839049, -0.00099504,  0.        ,\n",
       "        -0.07301546,  0.00178815, -0.1133147 , -0.0043457 ,  0.0201737 ,\n",
       "         0.02117492,  0.02429946, -0.02271502, -0.07964794,  0.00342021,\n",
       "        -0.00600121], dtype=float32),\n",
       " array([[-1.65631473e-01, -1.40059546e-01,  2.16548279e-01,\n",
       "         -4.21092324e-02, -8.74162093e-02,  1.06898323e-01,\n",
       "          3.71856898e-01, -2.15753578e-02,  2.24841490e-01,\n",
       "          2.49929711e-01,  2.02592224e-01,  2.63228178e-01,\n",
       "         -3.25511962e-01, -3.63515228e-01, -1.66109592e-01,\n",
       "          3.91018331e-01],\n",
       "        [ 2.10023820e-02, -2.99198568e-01,  6.33150041e-02,\n",
       "          3.03390533e-01,  2.89851576e-01, -3.63137215e-01,\n",
       "         -4.18210804e-01, -2.17349604e-01, -1.20239854e-01,\n",
       "          9.06277001e-02,  4.10500139e-01,  3.87136489e-01,\n",
       "         -1.52747840e-01,  3.63818496e-01, -3.76076877e-01,\n",
       "          1.59721404e-01],\n",
       "        [ 3.49837452e-01, -4.23084021e-01, -3.05944949e-01,\n",
       "         -3.57438445e-01,  4.50531989e-01,  1.07991077e-01,\n",
       "          3.28375131e-01, -5.53591192e-01,  6.48106486e-02,\n",
       "          2.94264466e-01, -3.32486302e-01,  4.06841099e-01,\n",
       "         -2.11230181e-02,  2.89365590e-01,  1.94317728e-01,\n",
       "         -5.03037930e-01],\n",
       "        [-1.47329137e-01, -2.78117657e-01, -2.51065701e-01,\n",
       "          4.74251658e-01, -1.64303675e-01,  3.53012890e-01,\n",
       "         -2.13371530e-01, -3.32044996e-02, -2.15376794e-01,\n",
       "          2.31604546e-01, -3.79809767e-01,  2.43011132e-01,\n",
       "          4.21049565e-01, -1.56869084e-01,  3.53048444e-01,\n",
       "         -3.56461674e-01],\n",
       "        [-3.09235990e-01, -4.08668995e-01,  4.16778028e-02,\n",
       "         -1.30845934e-01, -4.14186269e-01, -4.06085670e-01,\n",
       "          1.37120157e-01, -3.53084087e-01,  1.78300649e-01,\n",
       "          3.51419479e-01,  1.21198624e-01,  7.78429806e-02,\n",
       "          9.74116027e-02, -1.49668962e-01,  1.39548212e-01,\n",
       "          1.83390170e-01],\n",
       "        [-1.26652718e-01, -1.13549918e-01, -9.41173807e-02,\n",
       "          1.51764482e-01, -2.12593779e-01, -5.06995738e-01,\n",
       "         -2.60223839e-02, -2.62293845e-01, -4.68317866e-01,\n",
       "          2.11814553e-01,  1.40521526e-01,  1.17216259e-01,\n",
       "         -5.43335676e-01,  3.67301553e-02, -2.47514412e-01,\n",
       "          2.18450099e-01],\n",
       "        [-7.89328814e-02, -1.57831192e-01,  2.29234815e-01,\n",
       "         -5.73155880e-01,  1.55406594e-01,  1.27360761e-01,\n",
       "          5.07381618e-01,  4.91522312e-01, -2.92400807e-01,\n",
       "         -1.85594752e-01, -3.25875521e-01, -1.91679120e-01,\n",
       "          2.22119004e-01, -3.75710815e-01,  3.41744095e-01,\n",
       "          2.43655801e-01],\n",
       "        [ 4.19257253e-01, -2.50106938e-02,  1.19842261e-01,\n",
       "          8.01764205e-02,  1.68407068e-01,  7.83427879e-02,\n",
       "         -3.71738151e-02, -7.43572563e-02,  3.49867731e-01,\n",
       "         -2.23021004e-02,  3.91214699e-01,  3.41033429e-01,\n",
       "         -4.84883003e-02, -8.05444047e-02, -3.02859336e-01,\n",
       "          1.86900705e-01],\n",
       "        [-2.26471260e-01,  3.63300025e-01, -2.88234651e-01,\n",
       "         -6.76016808e-02, -2.92986762e-02, -3.58090214e-02,\n",
       "          1.87604174e-01,  2.85505652e-01,  1.78253591e-01,\n",
       "          3.98479402e-01,  2.76504904e-01, -3.70965824e-02,\n",
       "         -9.36458539e-03,  1.33859785e-02,  6.43356889e-03,\n",
       "          8.53256434e-02],\n",
       "        [ 2.84032971e-01, -4.28151220e-01,  2.53367335e-01,\n",
       "         -3.54152918e-01, -6.71876192e-01,  3.85894984e-01,\n",
       "          4.61397767e-01,  3.18590283e-01, -3.34010601e-01,\n",
       "         -2.90772796e-01,  3.33410710e-01,  2.42231544e-02,\n",
       "         -3.64450395e-01, -1.85545877e-01,  2.70361513e-01,\n",
       "         -3.40379179e-01],\n",
       "        [-1.22949243e-01, -2.02730328e-01,  3.30686241e-01,\n",
       "         -3.23936827e-02, -2.31433645e-01,  1.65919676e-01,\n",
       "         -1.23748168e-01,  3.55257511e-01,  2.85935476e-02,\n",
       "         -8.68163288e-01, -6.97895586e-02, -1.30578965e-01,\n",
       "          2.89284170e-01,  3.13822091e-01, -1.59766749e-01,\n",
       "          8.60472471e-02],\n",
       "        [-1.20805018e-01, -4.22161281e-01, -2.87281185e-01,\n",
       "         -1.44338071e-01,  2.91662604e-01, -3.91999245e-01,\n",
       "          3.17573607e-01, -1.67236533e-02,  2.50711471e-01,\n",
       "         -2.98705310e-01, -7.83768743e-02, -1.56694148e-02,\n",
       "          3.18937063e-01, -2.52284467e-01,  2.48280659e-01,\n",
       "         -4.69423175e-01],\n",
       "        [-1.02974296e-01,  4.21891063e-01, -5.77365160e-02,\n",
       "         -2.09883302e-01,  5.79217911e-01, -8.70632157e-02,\n",
       "         -4.66318816e-01,  2.88023740e-01,  2.59344757e-01,\n",
       "          4.04586568e-02, -3.20962846e-01, -1.70680135e-01,\n",
       "         -7.51401589e-04,  3.06130797e-01, -3.60522449e-01,\n",
       "         -1.05449736e-01],\n",
       "        [-3.07234108e-01,  3.91714036e-01, -4.09541011e-01,\n",
       "          5.60687542e-01, -2.94957459e-02,  7.79463470e-01,\n",
       "          4.30288106e-01,  2.39031747e-01,  5.51787131e-02,\n",
       "          1.18468991e-02, -3.74983937e-01, -7.78954208e-01,\n",
       "         -7.57376254e-02,  1.72956176e-02,  2.92298868e-02,\n",
       "         -4.17004555e-01],\n",
       "        [-2.57188439e-01,  3.39642853e-01,  4.12046283e-01,\n",
       "          2.93934524e-01, -2.20576972e-02, -3.83859783e-01,\n",
       "          3.03616345e-01, -1.68688253e-01,  2.15549782e-01,\n",
       "         -3.73355061e-01,  2.37900674e-01, -5.71065485e-01,\n",
       "         -2.56708860e-01, -1.00807451e-01,  2.92932689e-01,\n",
       "         -4.31660801e-01],\n",
       "        [-2.30899692e-01,  1.46647040e-02, -3.47671777e-01,\n",
       "         -1.12015456e-01,  3.91428441e-01, -1.68573409e-01,\n",
       "         -1.46623001e-01, -5.11047728e-02, -7.39873573e-03,\n",
       "         -1.75166816e-01, -1.34547263e-01,  2.71275658e-02,\n",
       "          1.82029635e-01,  1.25909418e-01, -5.93925975e-02,\n",
       "          1.70321479e-01]], dtype=float32),\n",
       " array([-3.1386137e-02, -7.9475706e-03, -2.6422618e-02, -1.1440533e-02,\n",
       "         2.7305709e-02, -7.6657903e-05,  1.4301173e-02,  1.7675219e-02,\n",
       "        -2.8763711e-02, -3.8212969e-03,  1.2324598e-02,  5.1946159e-02,\n",
       "         2.2043785e-02, -4.4193979e-02, -3.8617697e-02,  1.2372019e-02],\n",
       "       dtype=float32),\n",
       " array([[-3.36544931e-01, -2.09163725e-01, -1.49651706e-01,\n",
       "         -2.20165312e-01,  2.05941945e-01,  2.14230865e-01,\n",
       "         -4.07549053e-01,  3.94743115e-01,  7.37924278e-02,\n",
       "         -1.19041242e-01, -1.62860945e-01,  1.32563502e-01,\n",
       "          1.75843805e-01, -2.36572772e-01, -5.58275841e-02,\n",
       "          4.97942343e-02],\n",
       "        [ 4.18546438e-01,  1.38418287e-01, -2.53895313e-01,\n",
       "          5.23320556e-01, -3.18655759e-01,  3.22585166e-01,\n",
       "         -8.74413326e-02,  2.50707924e-01, -2.67245859e-01,\n",
       "          3.21051478e-01, -2.88444757e-01, -3.34967449e-02,\n",
       "         -1.00063577e-01,  2.29780316e-01, -1.86627582e-01,\n",
       "          2.31722459e-01],\n",
       "        [ 1.54673204e-01,  2.77096301e-01,  4.53052819e-02,\n",
       "         -2.29836538e-01,  4.01008576e-01, -4.12524223e-01,\n",
       "          9.63207427e-03,  4.27607894e-02, -4.27370787e-01,\n",
       "         -3.34530741e-01,  3.21192533e-01,  3.08279961e-01,\n",
       "         -3.00064504e-01, -2.31108695e-01, -2.22396716e-01,\n",
       "          1.05326980e-01],\n",
       "        [-3.34247917e-01, -3.85625549e-02, -8.43409896e-01,\n",
       "          3.51064980e-01, -5.71040869e-01, -6.61804199e-01,\n",
       "          6.25090003e-01, -8.40944111e-01,  9.07741010e-01,\n",
       "         -2.33655900e-01, -3.81848633e-01, -5.30081213e-01,\n",
       "         -5.79487324e-01, -1.92997344e-02,  5.55974126e-01,\n",
       "         -5.71241260e-01],\n",
       "        [-2.85569131e-01, -2.79376000e-01, -3.06108415e-01,\n",
       "         -1.64801583e-01, -4.68680076e-02, -3.85920614e-01,\n",
       "          6.14373572e-02,  4.25392210e-01,  1.30737543e-01,\n",
       "         -1.71069965e-01,  3.04936439e-01, -4.41071540e-01,\n",
       "          4.43056494e-01,  4.55322385e-01,  2.58021593e-01,\n",
       "          2.43015200e-01],\n",
       "        [-5.82562760e-02,  6.89882115e-02, -3.68060201e-01,\n",
       "          1.03300191e-01, -6.42899215e-01,  4.89817530e-01,\n",
       "          4.62300241e-01, -4.37906384e-01, -2.31996894e-01,\n",
       "         -1.03898957e-01,  1.06592633e-01,  2.30656248e-02,\n",
       "         -2.92385578e-01, -9.02948603e-02,  5.66975586e-02,\n",
       "          5.15712984e-02],\n",
       "        [-8.91859755e-02, -9.74006206e-03,  1.85434863e-01,\n",
       "          4.43767905e-01, -8.33766609e-02, -4.31936860e-01,\n",
       "         -2.51252860e-01, -4.38251674e-01, -1.70517772e-01,\n",
       "          2.46662706e-01, -1.86537862e-01,  2.77170744e-02,\n",
       "         -5.92218004e-02,  2.81213433e-01, -1.06012315e-01,\n",
       "         -2.17980191e-01],\n",
       "        [ 2.54337192e-01,  8.42696801e-02,  1.30015984e-01,\n",
       "          9.82949883e-02,  5.16741769e-04, -1.41452134e-01,\n",
       "          1.97740212e-01, -2.66924024e-01,  1.78456217e-01,\n",
       "          3.34078789e-01, -3.70062500e-01,  1.40149102e-01,\n",
       "          1.21259935e-01,  1.80409506e-01,  1.62535921e-01,\n",
       "         -2.08232865e-01],\n",
       "        [ 1.21017387e-02,  4.02675599e-01, -1.68394297e-01,\n",
       "          1.75697461e-01,  4.24805075e-01, -5.21901667e-01,\n",
       "         -4.11831379e-01, -2.08791971e-01, -3.88332874e-01,\n",
       "         -3.65746468e-01,  2.09693089e-01,  2.79447943e-01,\n",
       "          3.71876597e-01,  3.77431549e-02,  4.40290034e-01,\n",
       "         -3.97218615e-01],\n",
       "        [ 2.10879996e-01, -9.17922258e-02,  3.46704066e-01,\n",
       "          3.96550387e-01, -1.32348448e-01,  1.48863778e-01,\n",
       "          4.41890359e-01, -6.49559200e-01,  7.94726729e-01,\n",
       "          9.57723334e-02, -1.59060210e-01,  2.44949147e-01,\n",
       "         -2.32017294e-01,  6.64298356e-01, -3.07330489e-01,\n",
       "          1.15795732e-02],\n",
       "        [ 1.09627713e-02,  1.49602935e-01, -2.68420756e-01,\n",
       "         -9.25992131e-02,  1.15171269e-01, -1.57427341e-01,\n",
       "          1.30445972e-01,  2.45834425e-01, -2.24295631e-01,\n",
       "          1.11079738e-01, -3.61449718e-01, -1.84806108e-01,\n",
       "          5.38029253e-01, -5.50293811e-02, -4.23280686e-01,\n",
       "          2.11953625e-01],\n",
       "        [ 1.18168145e-01, -2.17905536e-01, -2.80515421e-02,\n",
       "         -1.57406777e-01, -2.47372046e-01,  1.88027710e-01,\n",
       "          1.43654749e-01,  5.51794648e-01, -3.50182533e-01,\n",
       "          8.59412849e-02, -1.32078826e-01,  1.03988536e-01,\n",
       "         -1.95278496e-01, -4.25576448e-01,  3.97977263e-01,\n",
       "          1.64860785e-01],\n",
       "        [ 4.19460803e-01,  2.61213541e-01, -5.97996712e-01,\n",
       "          4.33944613e-01,  2.11643755e-01, -1.99512422e-01,\n",
       "          3.67258370e-01,  2.33733013e-01,  3.45441312e-01,\n",
       "          3.81039858e-01, -1.80130169e-01, -8.85976925e-02,\n",
       "          3.16809297e-01,  3.51743251e-01, -3.09601158e-01,\n",
       "          2.73966312e-01],\n",
       "        [-3.57518345e-02,  1.49756804e-01, -2.78501868e-01,\n",
       "          3.22370142e-01, -1.18101791e-01,  6.67709112e-02,\n",
       "          3.29991519e-01,  2.43880004e-02, -4.09569852e-02,\n",
       "         -1.79478765e-01,  9.13003609e-02, -2.52440035e-01,\n",
       "          2.64674008e-01, -6.61032423e-02,  3.46892923e-01,\n",
       "          1.79057479e-01],\n",
       "        [ 4.04364944e-01,  4.07560259e-01, -2.08909407e-01,\n",
       "         -3.43328595e-01, -4.08045620e-01, -1.11133650e-01,\n",
       "         -1.88100245e-02, -3.03648096e-02, -1.47994068e-02,\n",
       "          1.03948474e-01, -1.31234685e-02, -3.75201762e-01,\n",
       "          2.68468291e-01,  2.88474023e-01, -6.62783161e-02,\n",
       "         -1.65739059e-01],\n",
       "        [-3.05921853e-01, -3.68793130e-01, -3.57697219e-01,\n",
       "          1.54126197e-01, -5.20326555e-01, -2.48198569e-01,\n",
       "         -1.49629787e-02,  1.31835371e-01,  1.15302823e-01,\n",
       "          4.27256286e-01, -2.89270133e-01,  7.00170919e-02,\n",
       "          5.41927479e-02, -6.00058623e-02, -2.29348049e-01,\n",
       "         -1.54478535e-01]], dtype=float32),\n",
       " array([ 0.02912612,  0.00196455,  0.05492549,  0.02322644,  0.05478466,\n",
       "         0.04949676,  0.00998761,  0.0524247 ,  0.01423305, -0.00356554,\n",
       "        -0.02973431, -0.00051273, -0.01532427,  0.01886722,  0.01047421,\n",
       "         0.01194209], dtype=float32),\n",
       " array([[ 4.04743791e-01, -3.38568807e-01,  3.88725698e-01,\n",
       "         -3.77568215e-01, -1.99711253e-03,  2.33464435e-01,\n",
       "          2.09451780e-01, -6.62193075e-02,  3.70936662e-01,\n",
       "          2.46915579e-01,  2.17780814e-01,  1.21717975e-01,\n",
       "          1.30311660e-02, -2.77531087e-01,  2.80539602e-01,\n",
       "          2.54340380e-01],\n",
       "        [ 3.86829972e-01,  2.03262970e-01,  8.74879882e-02,\n",
       "          4.11952883e-01, -3.91367376e-02, -3.86234462e-01,\n",
       "          3.39106590e-01,  4.00147706e-01, -9.49835479e-02,\n",
       "          1.31402448e-01,  7.65483221e-03,  3.08961898e-01,\n",
       "          2.16194868e-01, -5.84845841e-02, -4.19107944e-01,\n",
       "          8.30555186e-02],\n",
       "        [-3.61678421e-01, -3.98548394e-01,  5.27081609e-01,\n",
       "          2.46806234e-01,  2.55075961e-01, -6.96164787e-01,\n",
       "         -5.01420856e-01,  9.09013450e-02, -2.05952227e-02,\n",
       "          7.03580603e-02, -2.54834920e-01,  9.82595533e-02,\n",
       "          2.90190335e-02,  9.95705426e-02, -6.29956484e-01,\n",
       "         -3.19741249e-01],\n",
       "        [ 4.32149023e-01, -3.00439000e-01, -1.44923525e-02,\n",
       "          2.63662130e-01,  1.44599766e-01,  2.81428128e-01,\n",
       "          9.05711353e-02,  4.29049224e-01, -2.96668887e-01,\n",
       "          1.66466966e-01,  2.30805293e-01, -2.39876196e-01,\n",
       "          1.61287814e-01, -9.88101661e-02,  2.05248266e-01,\n",
       "          1.58193827e-01],\n",
       "        [-1.68652326e-01, -9.33884084e-02,  5.40232837e-01,\n",
       "         -2.13530272e-01, -3.55139256e-01, -2.94574440e-01,\n",
       "         -4.91181701e-01, -5.78170836e-01, -8.81938636e-02,\n",
       "         -2.06349373e-01,  2.61548646e-02,  3.95588696e-01,\n",
       "          8.54466781e-02,  2.24839300e-01, -1.40399098e-01,\n",
       "          2.68826872e-01],\n",
       "        [-5.50212443e-01, -3.11759591e-01,  8.94571990e-02,\n",
       "          1.71117648e-01, -5.15095890e-02, -4.00007039e-01,\n",
       "         -1.87703103e-01, -1.93966642e-01, -8.18105340e-02,\n",
       "          4.36002195e-01, -4.58465487e-01,  2.49083545e-02,\n",
       "          3.60188425e-01, -3.04082453e-01, -3.59655976e-01,\n",
       "         -4.71235439e-02],\n",
       "        [ 1.28074318e-01,  2.48617474e-02,  1.55906647e-01,\n",
       "          1.99809670e-01, -1.19422123e-01, -3.50919515e-01,\n",
       "          1.04952812e-01, -1.44429505e-01, -4.32754904e-01,\n",
       "         -2.39224568e-01,  3.03037077e-01, -2.18728364e-01,\n",
       "         -2.25300729e-01, -1.54084355e-01,  4.47491497e-01,\n",
       "         -2.12730542e-01],\n",
       "        [-3.94945890e-01, -5.16337454e-01,  1.95532013e-02,\n",
       "         -6.25335157e-01,  1.79203883e-01, -5.45981169e-01,\n",
       "         -2.53414631e-01, -7.10884854e-02, -3.08957577e-01,\n",
       "          4.51883823e-02, -4.73229349e-01,  3.41133475e-01,\n",
       "         -3.48537534e-01,  2.11626083e-01, -1.15802810e-01,\n",
       "         -2.49921635e-01],\n",
       "        [-2.06777882e-02, -1.70554429e-01,  1.93307593e-01,\n",
       "          3.62934828e-01, -3.31147790e-01, -8.04084092e-02,\n",
       "         -1.61227286e-01, -9.67839137e-02,  1.28636330e-01,\n",
       "          2.43359387e-01,  4.12164271e-01, -2.87542611e-01,\n",
       "          3.50569069e-01, -3.84252727e-01, -1.84533820e-01,\n",
       "         -3.78075600e-01],\n",
       "        [ 1.17742248e-01, -3.53168041e-01, -1.98646620e-01,\n",
       "          1.72952279e-01, -2.78650820e-01, -1.10692233e-01,\n",
       "          2.86996812e-01, -1.37841767e-02, -3.74317497e-01,\n",
       "         -4.60919052e-01, -1.74618348e-01,  2.51527488e-01,\n",
       "         -3.04199159e-01, -3.39030445e-01,  2.54188359e-01,\n",
       "         -1.85045958e-01],\n",
       "        [ 2.15911314e-01, -2.39083573e-01, -3.91228020e-01,\n",
       "         -1.98046729e-01,  2.74179786e-01, -1.02409936e-01,\n",
       "          3.21930259e-01, -3.97437841e-01, -2.46922821e-01,\n",
       "          3.45189542e-01,  4.08508070e-03, -2.10696846e-01,\n",
       "         -3.94198179e-01,  2.14375585e-01, -3.64575326e-01,\n",
       "          2.74814337e-01],\n",
       "        [-5.22435427e-01,  2.77015358e-01,  4.78682603e-04,\n",
       "         -5.61861932e-01,  2.85952240e-01, -2.99830228e-01,\n",
       "          2.07550123e-01,  1.93569422e-01, -2.10867479e-01,\n",
       "         -4.28039990e-02,  2.37959370e-01,  4.23625946e-01,\n",
       "         -2.77572811e-01, -6.69187307e-02, -1.67148128e-01,\n",
       "         -1.90518454e-01],\n",
       "        [-1.73663706e-01,  8.07768703e-02,  4.11190540e-01,\n",
       "         -2.07358360e-01, -1.21716052e-01,  1.88449189e-01,\n",
       "         -6.67974427e-02,  2.97909379e-01, -6.51631653e-02,\n",
       "         -3.35300297e-01,  3.53109948e-02,  2.11022809e-01,\n",
       "          2.59511530e-01, -3.04546505e-01,  1.42447110e-02,\n",
       "          5.79773895e-02],\n",
       "        [ 4.33136940e-01,  3.61586034e-01,  1.90138817e-01,\n",
       "         -4.16556001e-01, -3.49352926e-01,  4.27131474e-01,\n",
       "         -1.70407221e-01, -1.78450048e-01,  8.42796862e-02,\n",
       "          1.57986179e-01,  4.20931727e-01, -3.11514080e-01,\n",
       "         -2.60421902e-01, -6.83446527e-02,  3.88334453e-01,\n",
       "          3.17830652e-01],\n",
       "        [ 4.06714618e-01,  1.85244992e-01,  3.32189560e-01,\n",
       "          5.57214975e-01,  2.05022275e-01,  3.91180843e-01,\n",
       "          3.65571856e-01,  8.81776214e-02,  3.28770250e-01,\n",
       "          2.11446881e-01, -3.31584573e-01,  8.97674337e-02,\n",
       "         -3.70057225e-01, -4.84398007e-02, -2.39850596e-01,\n",
       "         -3.73647362e-01],\n",
       "        [-3.31521392e-01,  2.12916359e-01,  3.42555135e-01,\n",
       "         -5.24735034e-01,  3.22641313e-01,  1.25555530e-01,\n",
       "         -2.98853785e-01, -1.49085939e-01, -9.22583640e-02,\n",
       "         -1.31873144e-02, -3.81694436e-01,  1.98501825e-01,\n",
       "          3.57414186e-01, -3.17447066e-01, -5.83723448e-02,\n",
       "          1.44628193e-02]], dtype=float32),\n",
       " array([ 0.02864726, -0.0317681 , -0.0384302 , -0.08654466, -0.05283582,\n",
       "         0.035637  ,  0.03301324,  0.02882487,  0.        , -0.07379717,\n",
       "         0.04160906, -0.0019617 ,  0.02149412,  0.        ,  0.02924043,\n",
       "        -0.09302489], dtype=float32),\n",
       " array([[ 0.26735282],\n",
       "        [-0.31807795],\n",
       "        [-0.1877455 ],\n",
       "        [-0.79111767],\n",
       "        [-0.48324877],\n",
       "        [ 0.33183363],\n",
       "        [ 0.43726265],\n",
       "        [ 0.518975  ],\n",
       "        [ 0.35560536],\n",
       "        [-0.2695194 ],\n",
       "        [ 0.6054551 ],\n",
       "        [-0.48178443],\n",
       "        [ 0.32916272],\n",
       "        [-0.31289908],\n",
       "        [ 0.31391767],\n",
       "        [ 0.00336359]], dtype=float32),\n",
       " array([0.0330198], dtype=float32)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3b48723c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 13 and the array at index 2 has size 16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [111]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(biases_compressed)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights and biases saved to\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\n\u001b[0;32m---> 19\u001b[0m \u001b[43msave_to_bin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweights_and_biases.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [111]\u001b[0m, in \u001b[0;36msave_to_bin\u001b[0;34m(weights, biases, filename)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_to_bin\u001b[39m(weights, biases, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_and_biases.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# Concatenate the weights and biases into one array\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     weights \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     biases \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(biases)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Compress the arrays into binary format\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 13 and the array at index 2 has size 16"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def save_to_bin(weights, biases, filename=\"weights_and_biases.bin\"):\n",
    "    # Concatenate the weights and biases into one array\n",
    "    weights = np.concatenate(weights)\n",
    "    biases = np.concatenate(biases)\n",
    "\n",
    "    # Compress the arrays into binary format\n",
    "    weights_compressed = weights.tobytes()\n",
    "    biases_compressed = biases.tobytes()\n",
    "\n",
    "    # Write the binary data to the file\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(weights_compressed)\n",
    "        f.write(biases_compressed)\n",
    "\n",
    "    print(\"Weights and biases saved to\", filename)\n",
    "\n",
    "save_to_bin(weights, biases, filename=\"weights_and_biases.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a436bbec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights = best_model.get_weights()\n",
    "\n",
    "with open(\"weights.bin\", \"wb\") as f:\n",
    "    for weight in weights:\n",
    "        weight.tofile(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5d71236c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'get_biases'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [102]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_biases\u001b[49m()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbiases.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m bias \u001b[38;5;129;01min\u001b[39;00m biases:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'get_biases'"
     ]
    }
   ],
   "source": [
    "weights = best_model.get_biases()\n",
    "\n",
    "with open(\"biases.txt\", \"wb\") as f:\n",
    "    for bias in biases:\n",
    "        bias.tofile(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f7b1a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89674d7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedce09a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d11db9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruning_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "#     boundaries=[1000], values=[0.5, 0.0]\n",
    "# )\n",
    "\n",
    "# # Apply pruning to the model\n",
    "# pruning_params = {\n",
    "#   'pruning_schedule': pruning_schedule\n",
    "# }\n",
    "# pruned_model = tf.keras.models.model_from_tflite(tflite_quant_model, custom_objects={'PruningEarlyStoppingCallback':tf.keras.callbacks.PruningEarlyStoppingCallback})\n",
    "\n",
    "# # Prune the model\n",
    "# pruning_layer = tf.keras.layers.experimental.pruning.prune_low_magnitude(pruned_model.layers, **pruning_params)\n",
    "\n",
    "# # Save the pruned model\n",
    "# pruned_model.save(\"model_quant_pruned.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb2f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _, pre_pruning_acc = model.evaluate(test_data, test_labels, verbose=0)\n",
    "\n",
    "# # Set the threshold for pruning, below this value weights and biases will be set to zero\n",
    "# pruning_threshold = 0.1\n",
    "\n",
    "# # Iterate over all layers of the model\n",
    "# for layer in model.layers:\n",
    "#   if isinstance(layer, tf.keras.layers.Dense):\n",
    "#     # Get the weights and biases of the layer\n",
    "#     weights = layer.get_weights()[0]\n",
    "#     biases = layer.get_weights()[1]\n",
    "\n",
    "#     # Prune the weights\n",
    "#     mask = tf.abs(weights) > pruning_threshold\n",
    "#     weights = tf.where(mask, weights, 0.0)\n",
    "\n",
    "#     # Prune the biases\n",
    "#     mask = tf.abs(biases) > pruning_threshold\n",
    "#     biases = tf.where(mask, biases, 0.0)\n",
    "\n",
    "#     # Set the pruned weights and biases back to the layer\n",
    "#     layer.set_weights([weights, biases])\n",
    "\n",
    "# # Evaluate the model after pruning\n",
    "# _, post_pruning_acc = model.evaluate(test_data, test_labels, verbose=0)\n",
    "\n",
    "# # Print the accuracy before and after pruning\n",
    "# print(\"Accuracy before pruning:\", pre_pruning_acc)\n",
    "# print(\"Accuracy after pruning:\", post_pruning_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "39807a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightss_to_cpp(model, filename=\"weights_and_biases.txt\"):\n",
    "    model.summary()\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for l in range(len(model.layers)):\n",
    "        W, B = model.layers[l].get_weights()\n",
    "        weights.append(np.ravel(W))\n",
    "        biases.append(np.ravel(B))\n",
    "\n",
    "    z = np.concatenate(weights)\n",
    "    b = np.concatenate(biases)\n",
    "    with open(filename, \"w\") as f:\n",
    "        f.write(\"weights: {\")\n",
    "        for i in range(len(z)):\n",
    "            if (i < len(z)-1):\n",
    "                f.write(str(z[i])+\", \")\n",
    "            else:\n",
    "                f.write(str(z[i]))\n",
    "        f.write(\"}\\n\\n\")\n",
    "\n",
    "        f.write(\"biases: {\")\n",
    "        for i in range(len(b)):\n",
    "            if (i < len(b)-1):\n",
    "                f.write(str(b[i])+\", \")\n",
    "            else:\n",
    "                f.write(str(b[i]))\n",
    "        f.write(\"}\\n\\n\")\n",
    "\n",
    "        arch = []\n",
    "        arch.append(model.layers[0].input_shape[1])\n",
    "        for i in range(1, len(model.layers)):\n",
    "            arch.append(model.layers[i].input_shape[1])\n",
    "        arch.append(model.layers[len(model.layers)-1].output_shape[1])\n",
    "        f.write(\"Architecture: {\")\n",
    "        for i in range(len(arch)):\n",
    "            if (i < len(arch)-1):\n",
    "                f.write(str(arch[i])+\", \")\n",
    "            else:\n",
    "                f.write(str(arch[i]))\n",
    "        f.write(\"}\")\n",
    "        print(\"Architecture (alpha):\", arch)\n",
    "        print(\"Layers: \", len(arch))\n",
    "    print(\"Weights: \", z)\n",
    "    print(\"Biases: \", b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c3685daa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [104]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mweightss_to_cpp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtflite_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweights_and_biases.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [103]\u001b[0m, in \u001b[0;36mweightss_to_cpp\u001b[0;34m(model, filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweightss_to_cpp\u001b[39m(model, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_and_biases.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m()\n\u001b[1;32m      3\u001b[0m     weights \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m     biases \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "weightss_to_cpp(tflite_model, filename=\"weights_and_biases.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5dd0bfc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mweightss_to_cpp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtflite_quant_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweights_and_biases.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36mweightss_to_cpp\u001b[0;34m(model, filename)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweightss_to_cpp\u001b[39m(model, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_and_biases.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m()\n\u001b[1;32m      3\u001b[0m     weights \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m     biases \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "weightss_to_cpp(tflite_quant_model, filename=\"weights_and_biases.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b101293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e834309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def weights_to_bin(model, filename=\"weights_and_biases.bin\"):\n",
    "    model.summary()\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for l in range(len(model.layers)):\n",
    "        W, B = model.layers[l].get_weights()\n",
    "        weights.append(np.ravel(W))\n",
    "        biases.append(np.ravel(B))\n",
    "\n",
    "    z = np.concatenate(weights)\n",
    "    b = np.concatenate(biases)\n",
    "\n",
    "    # Compress the weights and biases into binary format\n",
    "    z_compressed = z.tobytes()\n",
    "    b_compressed = b.tobytes()\n",
    "\n",
    "    # Write the binary data to the file\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(z_compressed)\n",
    "        f.write(b_compressed)\n",
    "\n",
    "    print(\"Weights and biases saved to\", filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "67376804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpwnmcz8x6/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpwnmcz8x6/assets\n",
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
      "WARNING:absl:Optimization option OPTIMIZE_FOR_SIZE is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
      "2023-02-06 16:34:00.640030: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-02-06 16:34:00.640068: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-02-06 16:34:00.640256: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpwnmcz8x6\n",
      "2023-02-06 16:34:00.644847: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-02-06 16:34:00.644876: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpwnmcz8x6\n",
      "2023-02-06 16:34:00.665113: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-02-06 16:34:00.840496: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpwnmcz8x6\n",
      "2023-02-06 16:34:00.878061: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 237805 microseconds.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'layers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(b_compressed)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeights and biases saved to\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mweights_to_bin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtflite_quant_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweights_and_biases.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36mweights_to_bin\u001b[0;34m(model, filename)\u001b[0m\n\u001b[1;32m     10\u001b[0m weights \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m biases \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m)):\n\u001b[1;32m     13\u001b[0m     W, B \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[l]\u001b[38;5;241m.\u001b[39mget_weights()\n\u001b[1;32m     14\u001b[0m     weights\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mravel(W))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'layers'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Convert the best_model to a TFLite model with quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "def weights_to_bin(model, filename=\"weights_and_biases.bin\"):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for l in range(len(model.layers)):\n",
    "        W, B = model.layers[l].get_weights()\n",
    "        weights.append(np.ravel(W))\n",
    "        biases.append(np.ravel(B))\n",
    "\n",
    "    z = np.concatenate(weights)\n",
    "    b = np.concatenate(biases)\n",
    "\n",
    "    # Compress the weights and biases into binary format\n",
    "    z_compressed = z.tobytes()\n",
    "    b_compressed = b.tobytes()\n",
    "\n",
    "    # Write the binary data to the file\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(z_compressed)\n",
    "        f.write(b_compressed)\n",
    "\n",
    "    print(\"Weights and biases saved to\", filename)\n",
    "\n",
    "weights_to_bin(tflite_quant_model, filename=\"weights_and_biases.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24d03e9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mweights_to_bin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtflite_quant_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweights_and_biases.bin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36mweights_to_bin\u001b[0;34m(model, filename)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweights_to_bin\u001b[39m(model, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights_and_biases.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m()\n\u001b[1;32m      5\u001b[0m     weights \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m     biases \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "weights_to_bin(tflite_quant_model, filename=\"weights_and_biases.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1608af5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4627272721276049\n",
      "10.328490198528636\n",
      "188.3899693598225\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(y_test, y_pred_test))\n",
    "print(mean_absolute_error(y_test,y_pred_test))\n",
    "print(mean_squared_error(y_test,y_pred_test))\n",
    "\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5dd9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651dac91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
